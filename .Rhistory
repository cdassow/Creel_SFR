f=NA)
set.seed(10)
for(i in 1:nrow(pars.ma)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
meanlog = pars.ma[i,1],
sdlog = pars.ma[i,2])
pval.mayAug$alpha[i]=pars.ma[i,1]
pval.mayAug$beta[i]=pars.ma[i,2]
pval.mayAug$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.mayAug$sd[i]=sd(log(tempdat))
pval.mayAug$med[i]=median(log(tempdat))
pval.mayAug$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="mayAug"], rescale.p = T)
pval.mayAug$x2[i]=ct$statistic
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="mayAug"]))
pval.mayAug$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.mayAug$cvExceed=0
pval.mayAug$sdExceed=0
pval.mayAug$medExceed=0
pval.mayAug$kurtExceed=0
pval.mayAug$x2StatExceed=0
pval.mayAug$fStatExceed=0
mayAug.cv=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))/mean(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.sd=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.med=median(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.x2=chisq.test(modDat$exp.rate[modDat$treat=="mayAug"],p=modDat$exp.rate[modDat$treat=="mayAug"],rescale.p = T)
mayAug.f=var.test(log(modDat$exp.rate[modDat$treat=="mayAug"]),log(modDat$exp.rate[modDat$treat=="mayAug"]))
pval.mayAug$cvExceed[pval.mayAug$cv>mayAug.cv]=1
pval.mayAug$sdExceed[pval.mayAug$sd>mayAug.sd]=1
pval.mayAug$medExceed[pval.mayAug$med>mayAug.med]=1
pval.mayAug$kurtExceed[pval.mayAug$kurt>mayAug.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.mayAug$x2StatExceed[pval.mayAug$x2>mayAug.x2$statistic]=1 # looking at how often the test statistic is bigger than the mayAug
pval.mayAug$fStatExceed[pval.mayAug$f>mayAug.f$statistic]=1 # how often the f statistic is bigger than the mayAug
#### Pb WD25 ####
pval.wd25=data.frame(alpha=rep(NA,nrow(pars.wd25)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.wd25)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
meanlog = pars.wd25[i,1],
sdlog = pars.wd25[i,2])
pval.wd25$alpha[i]=pars.wd25[i,1]
pval.wd25$beta[i]=pars.wd25[i,2]
pval.wd25$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.wd25$sd[i]=sd(log(tempdat))
pval.wd25$med[i]=median(log(tempdat))
pval.wd25$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="wd25"], rescale.p = T)
pval.wd25$x2[i]=ct$statistic
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="wd25"]))
pval.wd25$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.wd25$cvExceed=0
pval.wd25$sdExceed=0
pval.wd25$medExceed=0
pval.wd25$kurtExceed=0
pval.wd25$x2StatExceed=0
pval.wd25$fStatExceed=0
wd25.cv=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))/mean(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.sd=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.med=median(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.x2=chisq.test(modDat$exp.rate[modDat$treat=="wd25"],p=modDat$exp.rate[modDat$treat=="wd25"],rescale.p = T)
wd25.f=var.test(log(modDat$exp.rate[modDat$treat=="wd25"]),log(modDat$exp.rate[modDat$treat=="wd25"]))
pval.wd25$cvExceed[pval.wd25$cv>wd25.cv]=1
pval.wd25$sdExceed[pval.wd25$sd>wd25.sd]=1
pval.wd25$medExceed[pval.wd25$med>wd25.med]=1
pval.wd25$kurtExceed[pval.wd25$kurt>wd25.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.wd25$x2StatExceed[pval.wd25$x2>wd25.x2$statistic]=1 # looking at how often the test statistic is bigger than the wd25
pval.wd25$fStatExceed[pval.wd25$f>wd25.f$statistic]=1 # how often the f statistic is bigger than the wd25
#### Pb WD50 ####
pval.wd50=data.frame(alpha=rep(NA,nrow(pars.wd50)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.wd50)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
meanlog = pars.wd50[i,1],
sdlog = pars.wd50[i,2])
pval.wd50$alpha[i]=pars.wd50[i,1]
pval.wd50$beta[i]=pars.wd50[i,2]
pval.wd50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.wd50$sd[i]=sd(log(tempdat))
pval.wd50$med[i]=median(log(tempdat))
pval.wd50$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="wd50"], rescale.p = T)
pval.wd50$x2[i]=ct$statistic
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="wd50"]))
pval.wd50$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.wd50$cvExceed=0
pval.wd50$sdExceed=0
pval.wd50$medExceed=0
pval.wd50$kurtExceed=0
pval.wd50$x2StatExceed=0
pval.wd50$fStatExceed=0
wd50.cv=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))/mean(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.sd=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.med=median(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.x2=chisq.test(modDat$exp.rate[modDat$treat=="wd50"],p=modDat$exp.rate[modDat$treat=="wd50"],rescale.p = T)
wd50.f=var.test(log(modDat$exp.rate[modDat$treat=="wd50"]),log(modDat$exp.rate[modDat$treat=="wd50"]))
pval.wd50$cvExceed[pval.wd50$cv>wd50.cv]=1
pval.wd50$sdExceed[pval.wd50$sd>wd50.sd]=1
pval.wd50$medExceed[pval.wd50$med>wd50.med]=1
pval.wd50$kurtExceed[pval.wd50$kurt>wd50.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.wd50$x2StatExceed[pval.wd50$x2>wd50.x2$statistic]=1 # looking at how often the test statistic is bigger than the wd50
pval.wd50$fStatExceed[pval.wd50$f>wd50.f$statistic]=1 # how often the f statistic is bigger than the wd50
#### Pb WE50 ####
pval.we50=data.frame(alpha=rep(NA,nrow(pars.we50)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.we50)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
meanlog = pars.we50[i,1],
sdlog = pars.we50[i,2])
pval.we50$alpha[i]=pars.we50[i,1]
pval.we50$beta[i]=pars.we50[i,2]
pval.we50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.we50$sd[i]=sd(log(tempdat))
pval.we50$med[i]=median(log(tempdat))
pval.we50$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="we50"], rescale.p = T)
pval.we50$x2[i]=ct$statistic
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="we50"]))
pval.we50$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.we50$cvExceed=0
pval.we50$sdExceed=0
pval.we50$medExceed=0
pval.we50$kurtExceed=0
pval.we50$x2StatExceed=0
pval.we50$fStatExceed=0
we50.cv=sd(log(modDat$exp.rate[modDat$treat=="we50"]))/mean(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.sd=sd(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.med=median(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.x2=chisq.test(modDat$exp.rate[modDat$treat=="we50"],p=modDat$exp.rate[modDat$treat=="we50"],rescale.p = T)
we50.f=var.test(log(modDat$exp.rate[modDat$treat=="we50"]),log(modDat$exp.rate[modDat$treat=="we50"]))
pval.we50$cvExceed[pval.we50$cv>we50.cv]=1
pval.we50$sdExceed[pval.we50$sd>we50.sd]=1
pval.we50$medExceed[pval.we50$med>we50.med]=1
pval.we50$kurtExceed[pval.we50$kurt>we50.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.we50$x2StatExceed[pval.we50$x2>we50.x2$statistic]=1 # looking at how often the test statistic is bigger than the we50
pval.we50$fStatExceed[pval.we50$f>we50.f$statistic]=1 # how often the f statistic is bigger than the we50
# making a list object with all the pval objects
self.pvals=list(pval.actual, pval.noWinter, pval.mayAug, pval.wd25, pval.wd50, pval.we50)
all.self.pvals=data.frame(dataSet=c(rep("actual",6),rep("noWinter",6),rep("mayAug",6),rep("wd25",6),rep("wd50",6),rep("we50",6)),
measure=rep(c("cv","sd","med","kurt","x2Sig","f"),6),
pval=NA)
for(i in 1:length(self.pvals)){
tp=self.pvals[[i]]
datSet=unique(all.self.pvals$dataSet)[i]
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="cv"]=sum(tp$cvExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="sd"]=sum(tp$sdExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="med"]=sum(tp$medExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="kurt"]=sum(tp$kurtExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="x2Sig"]=sum(tp$x2StatExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="f"]=sum(tp$fStatExceed==1)/nrow(tp)
}
ggplot(all.self.pvals)+theme_classic()+
geom_point(aes(x=measure, y=pval), size=2)+facet_wrap(~dataSet)+
geom_hline(yintercept = c(0.1,0.9), linetype=2)+
geom_hline(yintercept = 0.5, linetype=4)
knitr::opts_chunk$set(echo = F)
rm(list=ls())
# load packages
library(ggplot2)
library(tidyr)
library(ggpubr)
library(dplyr)
library(MASS)
# load data
nabYr=read.csv("C:/Users/dassocju/OneDrive - State of Wisconsin/NotreDame/NDstuff/Dissertation/2/scripts_data/nabYr.csv", header = T, stringsAsFactors = F)
i.obs=read.csv("C:/Users/dassocju/OneDrive - State of Wisconsin/NotreDame/NDstuff/Dissertation/2/scripts_data/i.obs.csv", header = T, stringsAsFactors = F)
# function for log likelihood estimation
cahill_Likelihood=function(parms,nabYr,i.obs){ # attempt at restructuring musky model in light of Cahill work and Brett's comments
# parms to estimate
alpha=10^(parms[1])           # ricker parameter
beta=10^(parms[2])            # ricker parameter
m1=exp(parms[3])/(1+exp(parms[3]))  #slope in exponential age-dependent mortality
m2=exp(parms[4])/(1+exp(parms[4]))  #intercept in exponential age-dependent mortality
BetaL0=parms[5]     # effective density effect on Linfinity, intercept
BetaL1=parms[6]/1e6     # effective density effect on Linfinity, slope
BetaO0=parms[7]     # effective density effect on omega, intercept
BetaO1=parms[8]/1e6     # effective density effect on omega, slope
psi=parms[9]     # length-dependent angling vulnerability parameter
# pulling in necessary data
exploitation=nabYr$urec
time=nabYr$year
abundObs=nabYr[nabYr$year%in%time,c(1:4,6)] # adult pe's (fish >76.2cm)
fishObs=i.obs[i.obs$year%in%time,] # length-at-age observations with associated years
# matrices to hold model results
M=numeric(length(time))
fm=numeric(length(time))
N=matrix(NA, nrow=length(time), ncol=30)
vf=matrix(NA,nrow=length(time),ncol=30)
effectiveDens=numeric(length(time))
Linf=numeric(length(time))
omega=numeric(length(time))
L=matrix(NA,nrow=length(time), ncol=30)
RL=matrix(NA,nrow=length(time),nco=30)
# constants
kObs=0.15 #vonB from Escanaba obs
t0Obs=-1.76 #vonB from Escanaba obs
LinfObs=109.76 #vonB from Escanaba obs
initialL=LinfObs*(1-exp(-kObs*(1:30-t0Obs))) # using vonB parms from calculations using all Escanaba Observations
#initialN=c(50,5,5,5,rep(1,26)) #rough approximation of PE during the era the model is started in (mean of the adult PE in the first 10 years of the data set. Likely subadults aged 1:4 are estimates just to get things started)
initialN=c(25,2,2,2,rep(c(0,1),13)) #SEJ reduced these because the observed abundance at beginning of time series is much lower; could probably look at sensitivity to initial conditions eventually...
# initializing the things that need it
effectiveDens[1]=sum(initialN*(initialL^2)) # initial effective density to get the model started
N[1,]=initialN
L[1,]=initialL
RL[1,]=(LinfObs/LinfObs)*(1-exp(-(kObs*1:30))) # initial relative lengths, using kObs here bu twill use omega/Linf in the loop
vf[1,]=RL[1,]^psi
# model equations
# loop through time
for(t in 2:length(time)){
# calculate effective density and vonB parameters for time t
effectiveDens[t]=sum((L[t-1,]^2)*N[t-1,]) # sum of squared fish lengths from the year prior
Linf[t]=exp(BetaL0+BetaL1*effectiveDens[t-1]) # density dependent Linf based on effective density instead of actual density
omega[t]=exp(BetaO0+BetaO1*effectiveDens[t-1]) # density dependent omega based on effective density instead of actual density
# loop through ages for time t
for(a in 1:30){
if(a==1){
# age 1 equations
# AGE 1 LENGTH
L[t,1]=Linf[t]*(1-exp(-(omega[t]/Linf[t])*(1-t0Obs))) # calculation of added length for each age year year based on previous length and density dependent Linf and omega
RL[t,1]=(Linf[t]/LinfObs)*(1-exp(-(omega[t]/Linf[t])*1)) # this should matter since age 1 fish should never be vulnerable but including the RL calc here for completeness
vf[t,1]=RL[t,1]^psi
# ABUNDANCE
# fraction of age 4, 5, and 6 fish mature in a given year
# forcing fish younger than 4 to be immature and older than 6 to be mature
A=sum(N[t-1,4:6]*(1/(1+exp(-2*(4:6-5)))))+sum(N[t-1,7:30])# age based logistic function to calculate what percentage of the age-class is mature. Age 5 is 50% mature
N[t,a]=alpha*A*exp(-beta*A) # Ricker recruitment equation
}
else{
# all other ages
# LENGTH and RELATIVE LENGTH
L[t,a]=L[t-1,a-1]+omega[t]/Linf[t]*(Linf[t]-L[t-1,a-1]) # using this simple dynamic equation (basis of vonB curve) dL/da=k*(Linf-L)
# from Cahill with multiple systems, Colin used LinfObs to "relativize", but Brett suggested just using 1 for the first term
#   (Linf[t]/LinfObs)*(1-exp(-(omega[t]/Linf[t])*a)) # calculating relative length for this year to use for vf
RL[t,a]=(1-exp(-(omega[t]/Linf[t])*a)) # calculating relative length for this year to use for vf
vf[t,a]=RL[t,a]^psi # vulnerability to angling based on relative length
# ABUNDANCE
###----------
### this seems messed up too... we have an inverse logit to force parameters between 0 and 1 and exponentiate here and then below too...
###    sticking for this for now, but might consider removing some of these transformations...
M[a]=exp(-m1*a+m2) # age-specific natural mortality
fm[t]=exploitation[t] # fishing mortality rate taken directly from Escanaba Lake comprehensive creel
N[t,a]=N[t-1,a-1]*exp(-M[a]-(vf[t,a]*fm[t])) # change in abundance
}
}
}
simAbund=N%*%(1/(1+exp(-2*(1:30-5)))) # of mature fish in the population
# lining up model length to compare to observed length in the likelihood
fishObs$modL=NA
rownames(L)=time
colnames(L)=1:30
for(i in 1:nrow(fishObs)){
fishObs$modL[i]=ifelse(fishObs$year[i]%in%row.names(L),L[rownames(L)==fishObs$year[i],colnames(L)==fishObs$age[i]],NA)
}
# calculate log-likelihoods
VonBresid=fishObs$length_cm-fishObs$modL
logAbundresid=log(abundObs$pe)-log(simAbund)
# penalizing simulations where we get simulations that fail
VonBresid[is.na(VonBresid)]=1e3
logAbundresid[is.na(logAbundresid)]=1e3
VonBresid=VonBresid[!is.na(fishObs$age)] # missing some data or ability to match model with data
logAbundresid=logAbundresid[!is.na(abundObs$pe)] # missing some data
### brasswell et al. method for likelihood variance
sigmaVonB=sqrt((1/length(VonBresid))*sum(VonBresid^2))
sigmaAbund=sqrt((1/length(logAbundresid))*sum(logAbundresid^2))
VonBsumll=sum(dnorm(VonBresid,mean=0,sd=sigmaVonB,log=TRUE),na.rm=TRUE)
Abundsumll=sum(dnorm(logAbundresid,mean=0,sd=sigmaAbund,log=TRUE),na.rm=TRUE)
#### ADDING A WEIGHTING FACTOR TO ABUNDANCE DATA TO ALLOW ABUNDANCE TO MATTER FOR PARAMETER ESTIMATION
####     using 23.42553 because N=1101 for lengths and N=47 for abundance; 1101/47=23.42553
sumll=VonBsumll+Abundsumll*23.42553
return(sumll)
}
cahillSim=function(parms){
# parms to estimate
alpha=10^(parms[1])           # ricker parameter
beta=10^(parms[2])            # ricker parameter
m1=exp(parms[3])/(1+exp(parms[3]))  #slope in exponential age-dependent mortality
m2=exp(parms[4])/(1+exp(parms[4]))  #intercept in exponential age-dependent mortality
BetaL0=parms[5]     # effective density effect on Linfinity, intercept
BetaL1=parms[6]/1e6     # effective density effect on Linfinity, slope
BetaO0=parms[7]     # effective density effect on omega, intercept
BetaO1=parms[8]/1e6     # effective density effect on omega, slope
psi=parms[9]     # length-dependent angling vulnerablity parameter
# pulling in necessary data
exploitation=nabYr$urec
time=nabYr$year
abundObs=nabYr[nabYr$year%in%time,c(1:4,6)] # adult pe's (fish >76.2cm)
fishObs=i.obs[i.obs$year%in%time,] # length-at-age observations with associated years
# matrices to hold model results
M=numeric(length(time))
fm=numeric(length(time))
N=matrix(NA, nrow=length(time), ncol=30)
vf=matrix(NA,nrow=length(time),ncol=30)
effectiveDens=numeric(length(time))
Linf=numeric(length(time))
omega=numeric(length(time))
L=matrix(NA,nrow=length(time), ncol=30)
RL=matrix(NA,nrow=length(time),nco=30)
# constants
kObs=0.15 #vonB from Escanaba obs
t0Obs=-1.76 #vonB from Escanaba obs
LinfObs=109.76 #vonB from Escanaba obs
initialL=LinfObs*(1-exp(-kObs*(1:30-t0Obs))) # using vonB parms from calculations using all Escanaba Observations
#initialN=c(50,5,5,5,rep(1,26)) #rough approximation of PE during the era the model is started in (mean of the adult PE in the first 10 years of the data set. Likely subadults aged 1:4 are estimates just to get things started)
initialN=c(25,2,2,2,rep(c(0,1),13)) #SEJ reduced these because the observed abundance at beginning of time series is much lower; could probably look at sensitivity to initial conditions eventually...
# initializing the things that need it
effectiveDens[1]=sum(initialN*(initialL^2)) # initial effective density to get the model started
N[1,]=initialN
L[1,]=initialL
RL[1,]=(LinfObs/LinfObs)*(1-exp(-(kObs*1:30))) # initial relative lengths, using kObs here bu twill use omega/Linf in the loop
vf[1,]=RL[1,]^psi
# model equations
for(t in 2:length(time)){
# calculate effective density and vonB paramters for time t
effectiveDens[t]=sum((L[t-1,]^2)*N[t-1,]) # sum of squared fish lengths from the year prior
Linf[t]=exp(BetaL0+BetaL1*effectiveDens[t-1]) # density dependent Linf based on effective density instead of actual density
omega[t]=exp(BetaO0+BetaO1*effectiveDens[t-1]) # density dependent omega based on effective density instead of actual density
# loop through ages for time t
for(a in 1:30){
if(a==1){
# age 1 equations
# AGE 1 LENGTH
L[t,1]=Linf[t]*(1-exp(-(omega[t]/Linf[t])*(1-t0Obs))) # calculation of added length for each age year year based on previous length and density dependent Linf and omega
RL[t,1]=(Linf[t]/LinfObs)*(1-exp(-(omega[t]/Linf[t])*1)) # this should matter since age 1 fish should never be vulnerable but including the RL calc here for completeness
vf[t,1]=RL[t,1]^psi
# ABUNDANCE
# fraction of age 4, 5, and 6 fish mature in a given year
# forcing fish younger than 4 to be immature and older than 6 to be mature
A=sum(N[t-1,4:6]*(1/(1+exp(-2*(4:6-5)))))+sum(N[t-1,7:30])# age based logistic function to calculate what percentage of the age-class is mature. Age 5 is 50% mature
N[t,a]=alpha*A*exp(-beta*A) # Ricker recruitment equation
}
else{
# all other ages
# LENGTH and RELATIVE LENGTH
L[t,a]=L[t-1,a-1]+omega[t]/Linf[t]*(Linf[t]-L[t-1,a-1]) # using this simple dynamic equation (basis of vonB curve) dL/da=k*(Linf-L)
# from Cahill with multiple systems, Colin used LinfObs to "relativize", but Brett suggested just using 1 for the first term
#   (Linf[t]/LinfObs)*(1-exp(-(omega[t]/Linf[t])*a)) # calculating relative length for this year to use for vf
RL[t,a]=(1-exp(-(omega[t]/Linf[t])*a)) # calculating relative length for this year to use for vf
vf[t,a]=RL[t,a]^psi # vulnerability to angling based on relative length
# ABUNDANCE
###----------
### this seems messed up too... we have an inverse logit to force parameters between 0 and 1 and exponentiate here and then below too...
###    sticking for this for now, but might consider removing some of these transformations...
M[a]=exp(-m1*a+m2) # age-specific natural mortality
fm[t]=exploitation[t] # fishing mortality rate taken directly from Escanaba Lake comprehensive creel
N[t,a]=N[t-1,a-1]*exp(-M[a]-(vf[t,a]*fm[t])) # change in abundance
}
}
}
simAbund=N%*%(1/(1+exp(-2*(1:30-5)))) # of mature fish in the population
# lining up model length to compare to observed length in the likelihood
fishObs$modL=NA
rownames(L)=time
colnames(L)=1:30
for(i in 1:nrow(fishObs)){
fishObs$modL[i]=ifelse(fishObs$year[i]%in%row.names(L),L[rownames(L)==fishObs$year[i],colnames(L)==fishObs$age[i]],NA)
}
return(list(abund=simAbund,
lengths=L,
predvObs=fishObs,
relativeL=RL,
effectiveDens=effectiveDens,
vfs=vf))
}
######### Metropolis algorithm
unif_proposalFunction<-function(p,lowEnd, highEnd){
# names for parameters  vector
#alpha, beta, m1, m2, BetaL0, BetaL1, BetaO0, BetaO1, psi
while(TRUE){
proposal=p+runif(length(p),-0.5,0.5)*(highEnd-lowEnd)/5  # came from Sasha's code... not sure of the basis for this...
if(all((proposal>lowEnd & proposal<highEnd))){
break
}
}
return(proposal)
}
mvnorm_proposalFunction<-function(p,sigma,lowEnd,highEnd){
# names for parameters  vector
#alpha, beta, m1, m2, BetaL0, BetaL1, BetaO0, BetaO1, psi
while(TRUE){
proposal=p+matrix(mvrnorm(n=1,matrix(0,1,length(min)),as.matrix(sigma)*matrix(1,length(min),length(min)),tol=1e-6),1,length(min))  # came from Sasha's code... not sure of the basis for this...
if(all((proposal>lowEnd & proposal<highEnd))){
break
}
}
return(proposal)
}
startvalueFunction<-function(lowEnd,highEnd){
return(runif(9,lowEnd,highEnd))
}
View(cahill_Likelihood)
1194+250+28+132
25/3
75*7
70*7
250/7
200/19
10.5/3
dat=data.frame(catchRates=c(rlnorm(100, meanlog = log(5), sdlog = log(1)),
rlnorm(100, meanlog = log(2), sdlog = log(3)),
rlnorm(100, meanlog = log(10), sdlog = log(1))),
regulation=c(rep("3line",100),rep("1line",100),rep("3line")),
lakeSize=c(rep(1000,100), rep(1200,100),rep(3000,100)),
lakeName=c(rep("lakeA",100),rep("lakeB",100),rep("lakeC",100)))
dat=data.frame(catchRates=c(rlnorm(100, meanlog = log(5), sdlog = log(1)),
rlnorm(100, meanlog = log(2), sdlog = log(3)),
rlnorm(100, meanlog = log(10), sdlog = log(1))),
regulation=c(rep("3line",100),rep("1line",100),rep("3line",100)),
lakeSize=c(rep(1000,100), rep(1200,100),rep(3000,100)),
lakeName=c(rep("lakeA",100),rep("lakeB",100),rep("lakeC",100)))
# a plot to look at the data
# here I'm using the ggplot package, just a personal preference thing
library(ggplot2)
ggplot(dat)+theme_classic()+
geom_density(aes(x=catchRates, fill=lakeName))
fit1=lm(catchRates~regulation, data = dat)
summary(fit1)
ggplot(dat)+theme_classic()+
geom_boxplot(aes(x=regulation, y=catchRate, fill=lakeName))
ggplot(dat)+theme_classic()+
geom_boxplot(aes(x=regulation, y=catchRates, fill=lakeName))
# now throwing in lake size and lake name as additional explanatory variables, looking at just single effects
fit2=lm(catchRates~regulation+lakeSize+lakeName, data = dat)
summary(fit2)
View(dat)
dat=data.frame(catchRates=c(rlnorm(100, meanlog = log(5), sdlog = log(5)),
rlnorm(100, meanlog = log(2), sdlog = log(3)),
rlnorm(100, meanlog = log(10), sdlog = log(2))),
regulation=c(rep("3line",100),rep("1line",100),rep("3line",100)),
lakeSize=c(rep(1000,100), rep(1200,100),rep(3000,100)),
lakeName=c(rep("lakeA",100),rep("lakeB",100),rep("lakeC",100)))
ggplot(dat)+theme_classic()+
geom_density(aes(x=catchRates, fill=lakeName))
ggplot(dat)+theme_classic()+
geom_boxplot(aes(x=regulation, y=catchRates, fill=lakeName))
dat=data.frame(catchRates=c(rlnorm(100, meanlog = log(5), sdlog = log(1)),
rlnorm(100, meanlog = log(2), sdlog = log(3)),
rlnorm(100, meanlog = log(10), sdlog = log(2))),
regulation=c(rep("3line",100),rep("1line",100),rep("3line",100)),
lakeSize=c(rep(1000,100), rep(1200,100),rep(3000,100)),
lakeName=c(rep("lakeA",100),rep("lakeB",100),rep("lakeC",100)))
ggplot(dat)+theme_classic()+
geom_density(aes(x=catchRates, fill=lakeName))
ggplot(dat)+theme_classic()+
geom_boxplot(aes(x=regulation, y=catchRates, fill=lakeName))
fit1=lm(catchRates~regulation, data = dat)
summary(fit1)
# now throwing in lake size and lake name as additional explanatory variables, looking at just single effects
fit2=lm(catchRates~regulation+lakeSize+lakeName, data = dat)
summary(fit2)
# modeling interactions between the 3 explanatory variables
fit3=lm(catchRates~., data = dat)
summary(fit3)
# modeling interactions between the 3 explanatory variables
fit3=lm(catchRates~regulation:lakeSize, data = dat)
summary(fit3)
summary(fit2) #
dat=data.frame(catchRates=c(rlnorm(100, meanlog = log(5), sdlog = log(2)),
rlnorm(100, meanlog = log(2), sdlog = log(3)),
rlnorm(100, meanlog = log(10), sdlog = log(2))),
regulation=c(rep("3line",100),rep("1line",100),rep("3line",100)),
lakeSize=c(rep(1000,100), rep(1200,100),rep(3000,100)),
lakeName=c(rep("lakeA",100),rep("lakeB",100),rep("lakeC",100)))
ggplot(dat)+theme_classic()+
geom_density(aes(x=catchRates, fill=lakeName))
ggplot(dat)+theme_classic()+
geom_boxplot(aes(x=regulation, y=catchRates, fill=lakeName))
fit1=lm(catchRates~regulation, data = dat)
summary(fit1) # reg is a significant explainer
# now throwing in lake size and lake name as additional explanatory variables, looking at just single effects
fit2=lm(catchRates~regulation+lakeSize+lakeName, data = dat)
summary(fit2) # rega and lake size are significant, the problem with lake ID
# modeling interactions between lake size and regulation
fit3=lm(catchRates~regulation:lakeSize, data = dat)
summary(fit3)
# modeling interactions between lake size and regulation
fit3=lm(catchRates~regulation*lakeSize, data = dat)
summary(fit3) #
# modeling interactions between lake size and regulation
fit3=lm(catchRates~regulation:lakeSize, data = dat)
summary(fit3) #
