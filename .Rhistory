treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="we50"])),rep("pred",length(modDat$exp.rate[modDat$treat=="we50"]))))
a.p.beta=ggplot(aComp.beta)+theme_classic()+
geom_density(aes(x=u,fill=treat),alpha=0.2)+
scale_fill_viridis_d()+labs(x="Exploitation Rate", y="Density", title = "Actual", fill=element_blank())
nw.p.beta=ggplot(nwComp.beta)+theme_classic()+
geom_density(aes(x=u,fill=treat),alpha=0.2)+
scale_fill_viridis_d()+labs(x="Exploitation Rate", y="Density", title = "No Winter Data \n(April - October)", fill=element_blank())
ma.p.beta=ggplot(maComp.beta)+theme_classic()+
geom_density(aes(x=u,fill=treat),alpha=0.2)+
scale_fill_viridis_d()+labs(x="Exploitation Rate", y="Density", title = "May - August \nData Only", fill=element_blank())
wd25.p.beta=ggplot(wd25Comp.beta)+theme_classic()+
geom_density(aes(x=u,fill=treat),alpha=0.2)+
scale_fill_viridis_d()+labs(x="Exploitation Rate", y="Density", title = "25% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
wd50.p.beta=ggplot(wd25Comp.beta)+theme_classic()+
geom_density(aes(x=u,fill=treat),alpha=0.2)+
scale_fill_viridis_d()+labs(x="Exploitation Rate", y="Density", title = "50% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
we50.p.beta=ggplot(we50Comp.beta)+theme_classic()+
geom_density(aes(x=u,fill=treat),alpha=0.2)+
scale_fill_viridis_d()+labs(x="Exploitation Rate", y="Density", title = "50% of Weekdend \nCreel Visits/Month \nRemoved", fill=element_blank())
ggarrange(a.p.beta,nw.p.beta,ma.p.beta,wd25.p.beta,wd50.p.beta,we50.p.beta, common.legend = T)
# bayesian p-value for each model
#### Pb ACTUAL ####
pval.actual=data.frame(alpha=rep(NA,nrow(pars.a.beta)),
beta=NA,
cv=NA,
sd=NA)
set.seed(10)
for(i in 1:nrow(pars.a.beta)){
tempdat=rbeta(n=length(modDat$exp.rate[modDat$treat=="actual"]),
shape1 = pars.a.beta[i,1],
shape2 = pars.a.beta[i,2])
pval.actual$alpha[i]=pars.a.beta[i,1]
pval.actual$beta[i]=pars.a.beta[i,2]
pval.actual$cv[i]=sd(tempdat)/mean(tempdat)
pval.actual$sd[i]=sd(tempdat)
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.actual$cvExceed=0
pval.actual$sdExceed=0
actual.cv=sd(modDat$exp.rate[modDat$treat=="actual"])/mean(modDat$exp.rate[modDat$treat=="actual"])
actual.sd=sd(modDat$exp.rate[modDat$treat=="actual"])
pval.actual$cvExceed[pval.actual$cv>actual.cv]=1
pval.actual$sdExceed[pval.actual$sd>actual.sd]=1
# sum(pval.actual$cvExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
# sum(pval.actual$sdExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
#### Pb NO WINTER ####
pval.noWinter=data.frame(alpha=rep(NA,nrow(pars.nw.beta)),
beta=NA,
cv=NA,
sd=NA)
set.seed(10)
for(i in 1:nrow(pars.nw.beta)){
tempdat=rbeta(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
shape1 = pars.nw.beta[i,1],
shape2 = pars.nw.beta[i,2])
pval.noWinter$alpha[i]=pars.nw.beta[i,1]
pval.noWinter$beta[i]=pars.nw.beta[i,2]
pval.noWinter$cv[i]=sd(tempdat)/mean(tempdat)
pval.noWinter$sd[i]=sd(tempdat)
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.noWinter$cvExceed=0
pval.noWinter$sdExceed=0
noWinter.cv=sd(modDat$exp.rate[modDat$treat=="noWinter"])/mean(modDat$exp.rate[modDat$treat=="noWinter"])
noWinter.sd=sd(modDat$exp.rate[modDat$treat=="noWinter"])
pval.noWinter$cvExceed[pval.noWinter$cv>noWinter.cv]=1
pval.noWinter$sdExceed[pval.noWinter$sd>noWinter.sd]=1
# sum(pval.noWinter$cvExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
# sum(pval.noWinter$sdExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
#### Pb MAYAUGUST ####
pval.mayAug=data.frame(alpha=rep(NA,nrow(pars.ma.beta)),
beta=NA,
cv=NA,
sd=NA)
set.seed(10)
for(i in 1:nrow(pars.ma.beta)){
tempdat=rbeta(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
shape1 = pars.ma.beta[i,1],
shape2 = pars.ma.beta[i,2])
pval.mayAug$alpha[i]=pars.ma.beta[i,1]
pval.mayAug$beta[i]=pars.ma.beta[i,2]
pval.mayAug$cv[i]=sd(tempdat)/mean(tempdat)
pval.mayAug$sd[i]=sd(tempdat)
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.mayAug$cvExceed=0
pval.mayAug$sdExceed=0
mayAug.cv=sd(modDat$exp.rate[modDat$treat=="mayAug"])/mean(modDat$exp.rate[modDat$treat=="mayAug"])
mayAug.sd=sd(modDat$exp.rate[modDat$treat=="mayAug"])
pval.mayAug$cvExceed[pval.mayAug$cv>mayAug.cv]=1
pval.mayAug$sdExceed[pval.mayAug$sd>mayAug.sd]=1
# sum(pval.mayAug$cvExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
# sum(pval.mayAug$sdExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
#### Pb WD25 ####
pval.wd25=data.frame(alpha=rep(NA,nrow(pars.wd25.beta)),
beta=NA,
cv=NA,
sd=NA)
set.seed(10)
for(i in 1:nrow(pars.wd25.beta)){
tempdat=rbeta(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
shape1 = pars.wd25.beta[i,1],
shape2 = pars.wd25.beta[i,2])
pval.wd25$alpha[i]=pars.wd25.beta[i,1]
pval.wd25$beta[i]=pars.wd25.beta[i,2]
pval.wd25$cv[i]=sd(tempdat)/mean(tempdat)
pval.wd25$sd[i]=sd(tempdat)
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.wd25$cvExceed=0
pval.wd25$sdExceed=0
wd25.cv=sd(modDat$exp.rate[modDat$treat=="wd25"])/mean(modDat$exp.rate[modDat$treat=="wd25"])
wd25.sd=sd(modDat$exp.rate[modDat$treat=="wd25"])
pval.wd25$cvExceed[pval.wd25$cv>wd25.cv]=1
pval.wd25$sdExceed[pval.wd25$sd>wd25.sd]=1
# sum(pval.wd25$cvExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
# sum(pval.wd25$sdExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
#### Pb WD50 ####
pval.wd50=data.frame(alpha=rep(NA,nrow(pars.wd50.beta)),
beta=NA,
cv=NA,
sd=NA)
set.seed(10)
for(i in 1:nrow(pars.wd50.beta)){
tempdat=rbeta(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
shape1 = pars.wd50.beta[i,1],
shape2 = pars.wd50.beta[i,2])
pval.wd50$alpha[i]=pars.wd50.beta[i,1]
pval.wd50$beta[i]=pars.wd50.beta[i,2]
pval.wd50$cv[i]=sd(tempdat)/mean(tempdat)
pval.wd50$sd[i]=sd(tempdat)
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.wd50$cvExceed=0
pval.wd50$sdExceed=0
wd50.cv=sd(modDat$exp.rate[modDat$treat=="wd50"])/mean(modDat$exp.rate[modDat$treat=="wd50"])
wd50.sd=sd(modDat$exp.rate[modDat$treat=="wd50"])
pval.wd50$cvExceed[pval.wd50$cv>wd50.cv]=1
pval.wd50$sdExceed[pval.wd50$sd>wd50.sd]=1
# sum(pval.wd50$cvExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
# sum(pval.wd50$sdExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
#### Pb WE50 ####
pval.we50=data.frame(alpha=rep(NA,nrow(pars.we50.beta)),
beta=NA,
cv=NA,
sd=NA)
set.seed(10)
for(i in 1:nrow(pars.we50.beta)){
tempdat=rbeta(n=length(modDat$exp.rate[modDat$treat=="we50"]),
shape1 = pars.we50.beta[i,1],
shape2 = pars.we50.beta[i,2])
pval.we50$alpha[i]=pars.we50.beta[i,1]
pval.we50$beta[i]=pars.we50.beta[i,2]
pval.we50$cv[i]=sd(tempdat)/mean(tempdat)
pval.we50$sd[i]=sd(tempdat)
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.we50$cvExceed=0
pval.we50$sdExceed=0
we50.cv=sd(modDat$exp.rate[modDat$treat=="we50"])/mean(modDat$exp.rate[modDat$treat=="we50"])
we50.sd=sd(modDat$exp.rate[modDat$treat=="we50"])
pval.we50$cvExceed[pval.we50$cv>we50.cv]=1
pval.we50$sdExceed[pval.we50$sd>we50.sd]=1
# sum(pval.we50$cvExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
# sum(pval.we50$sdExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
## p-value tests comparing the scenarios to actual to show that some scenarios approximate the actual quite well.
bpval.comp=data.frame(scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
coef.var.pval=NA,
sd.pval=NA)
pval.actual$cvComp=0
pval.actual$sdComp=0
pval.actual$cvComp[pval.actual$cv>actual.cv]=1
pval.actual$sdComp[pval.actual$sd>actual.sd]=1
pval.noWinter$cvComp=0
pval.noWinter$sdComp=0
pval.noWinter$cvComp[pval.noWinter$cv>actual.cv]=1
pval.noWinter$sdComp[pval.noWinter$sd>actual.sd]=1
pval.mayAug$cvComp=0
pval.mayAug$sdComp=0
pval.mayAug$cvComp[pval.mayAug$cv>actual.cv]=1
pval.mayAug$sdComp[pval.mayAug$sd>actual.sd]=1
pval.wd25$cvComp=0
pval.wd25$sdComp=0
pval.wd25$cvComp[pval.wd25$cv>actual.cv]=1
pval.wd25$sdComp[pval.wd25$sd>actual.sd]=1
pval.wd50$cvComp=0
pval.wd50$sdComp=0
pval.wd50$cvComp[pval.wd50$cv>actual.cv]=1
pval.wd50$sdComp[pval.wd50$sd>actual.sd]=1
pval.we50$cvComp=0
pval.we50$sdComp=0
pval.we50$cvComp[pval.we50$cv>actual.cv]=1
pval.we50$sdComp[pval.we50$sd>actual.sd]=1
bpval.comp$coef.var.pval=c(sum(pval.actual$cvComp)/nrow(pval.actual),
sum(pval.noWinter$cvComp)/nrow(pval.noWinter),
sum(pval.mayAug$cvComp)/nrow(pval.mayAug),
sum(pval.wd25$cvComp)/nrow(pval.wd25),
sum(pval.wd50$cvComp)/nrow(pval.wd50),
sum(pval.we50$cvComp)/nrow(pval.we50))
bpval.comp$sd.pval=c(sum(pval.actual$sdComp)/nrow(pval.actual),
sum(pval.noWinter$sdComp)/nrow(pval.noWinter),
sum(pval.mayAug$sdComp)/nrow(pval.mayAug),
sum(pval.wd25$sdComp)/nrow(pval.wd25),
sum(pval.wd50$sdComp)/nrow(pval.wd50),
sum(pval.we50$sdComp)/nrow(pval.we50))
kable(bpval.comp, digits = 3, col.names = c("Scenario", "CV p-value","SD p-value"), align="lcc",caption = "Bayesian p-values for a beta distributed model. Each p-value describes whether or not there is a significant difference between data generated by the fitted model and the actual data for that scenario. Two metrics are assessed here, coefficient of variation (CV) and standard devidation (SD).")
tab=data.frame(statistic=c("CV","SD","Median","Kurtosis","X2","F"),
desription=c("Ratio of the standard deviation to the mean.",
"Square root of the variance.",
"Middle of the data when all values are ordered from smallest to largest. Similar to a mean but less sensitive to outlier values.",
"Measure of the width, or 'tailedness' of a distribution. In other words, do the tails of the distribution contain more or fewer outliers (i.e. are they thicker or thinner)?",
"Chi square test statistic for a 'goodness of fit' test is calculated. Here I want to know if frequency of values in the model generated data come from the same distribution as the observed data",
"The ratio of the variance between the model simulated data and the observed data."))
# bayesian p-value for each model
pars.a=getSample(expR.actual)
pars.nw=getSample(expR.nw)
pars.ma=getSample(expR.ma)
pars.wd25=getSample(expR.25wd)
pars.wd50=getSample(expR.50wd)
pars.we50=getSample(expR.50we)
#### Pb ACTUAL ####
pval.actual=data.frame(alpha=rep(NA,nrow(pars.a)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.a)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
meanlog = pars.a[i,1],
sdlog = pars.a[i,2])
pval.actual$alpha[i]=pars.a[i,1]
pval.actual$beta[i]=pars.a[i,2]
pval.actual$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.actual$sd[i]=sd(log(tempdat))
pval.actual$med[i]=median(log(tempdat))
pval.actual$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="actual"], rescale.p = T)
pval.actual$x2[i]=ct$p.value
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="actual"]))
pval.actual$f[i]=f.test$statistic
}
# now calculate the number of times the test statistic exceeds that of the real data
pval.actual$cvExceed=0
pval.actual$sdExceed=0
pval.actual$medExceed=0
pval.actual$kurtExceed=0
pval.actual$x2SigDiff=0
pval.actual$fStatExceed=0
actual.cv=sd(log(modDat$exp.rate[modDat$treat=="actual"]))/mean(log(modDat$exp.rate[modDat$treat=="actual"]))
actual.sd=sd(log(modDat$exp.rate[modDat$treat=="actual"]))
actual.med=median(log(modDat$exp.rate[modDat$treat=="actual"]))
actual.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="actual"]))
actual.x2=chisq.test(modDat$exp.rate[modDat$treat=="actual"],p=modDat$exp.rate[modDat$treat=="actual"],rescale.p = T)
actual.f=var.test(log(modDat$exp.rate[modDat$treat=="actual"]),log(modDat$exp.rate[modDat$treat=="actual"]))
pval.actual$cvExceed[pval.actual$cv>actual.cv]=1
pval.actual$sdExceed[pval.actual$sd>actual.sd]=1
pval.actual$medExceed[pval.actual$med>actual.med]=1
pval.actual$kurtExceed[pval.actual$kurt>actual.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.actual$x2SigDiff[pval.actual$x2<0.05]=1 # looking at how often we would say the simmed data did not come from the probabilit distribution of the actual data
pval.actual$fStatExceed[pval.actual$f>actual.f$statistic]=1 # how often the f statistic is bigger than the actual
#### Pb NoWinter ####
pval.noWinter=data.frame(alpha=rep(NA,nrow(pars.nw)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.nw)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
meanlog = pars.nw[i,1],
sdlog = pars.nw[i,2])
pval.noWinter$alpha[i]=pars.nw[i,1]
pval.noWinter$beta[i]=pars.nw[i,2]
pval.noWinter$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.noWinter$sd[i]=sd(log(tempdat))
pval.noWinter$med[i]=median(log(tempdat))
pval.noWinter$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="noWinter"], rescale.p = T)
pval.noWinter$x2[i]=ct$p.value
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="noWinter"]))
pval.noWinter$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.noWinter$cvExceed=0
pval.noWinter$sdExceed=0
pval.noWinter$medExceed=0
pval.noWinter$kurtExceed=0
pval.noWinter$x2SigDiff=0
pval.noWinter$fStatExceed=0
noWinter.cv=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))/mean(log(modDat$exp.rate[modDat$treat=="noWinter"]))
noWinter.sd=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))
noWinter.med=median(log(modDat$exp.rate[modDat$treat=="noWinter"]))
noWinter.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="noWinter"]))
noWinter.x2=chisq.test(modDat$exp.rate[modDat$treat=="noWinter"],p=modDat$exp.rate[modDat$treat=="noWinter"],rescale.p = T)
noWinter.f=var.test(log(modDat$exp.rate[modDat$treat=="noWinter"]),log(modDat$exp.rate[modDat$treat=="noWinter"]))
pval.noWinter$cvExceed[pval.noWinter$cv>noWinter.cv]=1
pval.noWinter$sdExceed[pval.noWinter$sd>noWinter.sd]=1
pval.noWinter$medExceed[pval.noWinter$med>noWinter.med]=1
pval.noWinter$kurtExceed[pval.noWinter$kurt>noWinter.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.noWinter$x2SigDiff[pval.noWinter$x2<0.05]=1 # looking at how often we would say the simmed data did not come from the probabilit distribution of the noWinter data
pval.noWinter$fStatExceed[pval.noWinter$f>noWinter.f$statistic]=1 # how often the f statistic is bigger than the noWinter
#### Pb MayAug ####
pval.mayAug=data.frame(alpha=rep(NA,nrow(pars.ma)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.ma)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
meanlog = pars.ma[i,1],
sdlog = pars.ma[i,2])
pval.mayAug$alpha[i]=pars.ma[i,1]
pval.mayAug$beta[i]=pars.ma[i,2]
pval.mayAug$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.mayAug$sd[i]=sd(log(tempdat))
pval.mayAug$med[i]=median(log(tempdat))
pval.mayAug$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="mayAug"], rescale.p = T)
pval.mayAug$x2[i]=ct$p.value
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="mayAug"]))
pval.mayAug$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.mayAug$cvExceed=0
pval.mayAug$sdExceed=0
pval.mayAug$medExceed=0
pval.mayAug$kurtExceed=0
pval.mayAug$x2SigDiff=0
pval.mayAug$fStatExceed=0
mayAug.cv=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))/mean(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.sd=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.med=median(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.x2=chisq.test(modDat$exp.rate[modDat$treat=="mayAug"],p=modDat$exp.rate[modDat$treat=="mayAug"],rescale.p = T)
mayAug.f=var.test(log(modDat$exp.rate[modDat$treat=="mayAug"]),log(modDat$exp.rate[modDat$treat=="mayAug"]))
pval.mayAug$cvExceed[pval.mayAug$cv>mayAug.cv]=1
pval.mayAug$sdExceed[pval.mayAug$sd>mayAug.sd]=1
pval.mayAug$medExceed[pval.mayAug$med>mayAug.med]=1
pval.mayAug$kurtExceed[pval.mayAug$kurt>mayAug.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.mayAug$x2SigDiff[pval.mayAug$x2<0.05]=1 # looking at how often we would say the simmed data did not come from the probabilit distribution of the mayAug data
pval.mayAug$fStatExceed[pval.mayAug$f>mayAug.f$statistic]=1 # how often the f statistic is bigger than the mayAug
#### Pb WD25 ####
pval.wd25=data.frame(alpha=rep(NA,nrow(pars.wd25)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.wd25)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
meanlog = pars.wd25[i,1],
sdlog = pars.wd25[i,2])
pval.wd25$alpha[i]=pars.wd25[i,1]
pval.wd25$beta[i]=pars.wd25[i,2]
pval.wd25$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.wd25$sd[i]=sd(log(tempdat))
pval.wd25$med[i]=median(log(tempdat))
pval.wd25$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="wd25"], rescale.p = T)
pval.wd25$x2[i]=ct$p.value
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="wd25"]))
pval.wd25$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.wd25$cvExceed=0
pval.wd25$sdExceed=0
pval.wd25$medExceed=0
pval.wd25$kurtExceed=0
pval.wd25$x2SigDiff=0
pval.wd25$fStatExceed=0
wd25.cv=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))/mean(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.sd=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.med=median(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.x2=chisq.test(modDat$exp.rate[modDat$treat=="wd25"],p=modDat$exp.rate[modDat$treat=="wd25"],rescale.p = T)
wd25.f=var.test(log(modDat$exp.rate[modDat$treat=="wd25"]),log(modDat$exp.rate[modDat$treat=="wd25"]))
pval.wd25$cvExceed[pval.wd25$cv>wd25.cv]=1
pval.wd25$sdExceed[pval.wd25$sd>wd25.sd]=1
pval.wd25$medExceed[pval.wd25$med>wd25.med]=1
pval.wd25$kurtExceed[pval.wd25$kurt>wd25.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.wd25$x2SigDiff[pval.wd25$x2<0.05]=1 # looking at how often we would say the simmed data did not come from the probabilit distribution of the wd25 data
pval.wd25$fStatExceed[pval.wd25$f>wd25.f$statistic]=1 # how often the f statistic is bigger than the wd25
#### Pb WD50 ####
pval.wd50=data.frame(alpha=rep(NA,nrow(pars.wd50)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.wd50)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
meanlog = pars.wd50[i,1],
sdlog = pars.wd50[i,2])
pval.wd50$alpha[i]=pars.wd50[i,1]
pval.wd50$beta[i]=pars.wd50[i,2]
pval.wd50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.wd50$sd[i]=sd(log(tempdat))
pval.wd50$med[i]=median(log(tempdat))
pval.wd50$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="wd50"], rescale.p = T)
pval.wd50$x2[i]=ct$p.value
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="wd50"]))
pval.wd50$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.wd50$cvExceed=0
pval.wd50$sdExceed=0
pval.wd50$medExceed=0
pval.wd50$kurtExceed=0
pval.wd50$x2SigDiff=0
pval.wd50$fStatExceed=0
wd50.cv=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))/mean(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.sd=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.med=median(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.x2=chisq.test(modDat$exp.rate[modDat$treat=="wd50"],p=modDat$exp.rate[modDat$treat=="wd50"],rescale.p = T)
wd50.f=var.test(log(modDat$exp.rate[modDat$treat=="wd50"]),log(modDat$exp.rate[modDat$treat=="wd50"]))
pval.wd50$cvExceed[pval.wd50$cv>wd50.cv]=1
pval.wd50$sdExceed[pval.wd50$sd>wd50.sd]=1
pval.wd50$medExceed[pval.wd50$med>wd50.med]=1
pval.wd50$kurtExceed[pval.wd50$kurt>wd50.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.wd50$x2SigDiff[pval.wd50$x2<0.05]=1 # looking at how often we would say the simmed data did not come from the probabilit distribution of the wd50 data
pval.wd50$fStatExceed[pval.wd50$f>wd50.f$statistic]=1 # how often the f statistic is bigger than the wd50
#### Pb WE50 ####
pval.we50=data.frame(alpha=rep(NA,nrow(pars.we50)),
beta=NA,
cv=NA,
sd=NA,
med=NA,
kurt=NA,
x2=NA,
f=NA)
set.seed(10)
for(i in 1:nrow(pars.we50)){
tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
meanlog = pars.we50[i,1],
sdlog = pars.we50[i,2])
pval.we50$alpha[i]=pars.we50[i,1]
pval.we50$beta[i]=pars.we50[i,2]
pval.we50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
pval.we50$sd[i]=sd(log(tempdat))
pval.we50$med[i]=median(log(tempdat))
pval.we50$kurt[i]=kurtosis(log(tempdat))
ct=chisq.test(tempdat, p=modDat$exp.rate[modDat$treat=="we50"], rescale.p = T)
pval.we50$x2[i]=ct$p.value
f.test=var.test(log(tempdat),log(modDat$exp.rate[modDat$treat=="we50"]))
pval.we50$f[i]=f.test$statistic
}
# now calculate the number of times the cv or sd exceeds that of the real data
pval.we50$cvExceed=0
pval.we50$sdExceed=0
pval.we50$medExceed=0
pval.we50$kurtExceed=0
pval.we50$x2SigDiff=0
pval.we50$fStatExceed=0
we50.cv=sd(log(modDat$exp.rate[modDat$treat=="we50"]))/mean(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.sd=sd(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.med=median(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.kurt=kurtosis(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.x2=chisq.test(modDat$exp.rate[modDat$treat=="we50"],p=modDat$exp.rate[modDat$treat=="we50"],rescale.p = T)
we50.f=var.test(log(modDat$exp.rate[modDat$treat=="we50"]),log(modDat$exp.rate[modDat$treat=="we50"]))
pval.we50$cvExceed[pval.we50$cv>we50.cv]=1
pval.we50$sdExceed[pval.we50$sd>we50.sd]=1
pval.we50$medExceed[pval.we50$med>we50.med]=1
pval.we50$kurtExceed[pval.we50$kurt>we50.kurt]=1 # kurt value great that observed data means there are more outliers, or thicker tails than the observed data
pval.we50$x2SigDiff[pval.we50$x2<0.05]=1 # looking at how often we would say the simmed data did not come from the probabilit distribution of the we50 data
pval.we50$fStatExceed[pval.we50$f>we50.f$statistic]=1 # how often the f statistic is bigger than the we50
### summarizing pvalue output
# reminder this is all comparison to self
# making a list object with all the pval objects
self.pvals=list(pval.actual, pval.noWinter, pval.mayAug, pval.wd25, pval.wd50, pval.we50)
all.self.pvals=data.frame(dataSet=c(rep("actual",6),rep("noWinter",6),rep("mayAug",6),rep("wd25",6),rep("wd50",6),rep("we50",6)),
measure=rep(c("cv","sd","med","kurt","x2Sig","f"),6),
pval=NA)
for(i in 1:length(self.pvals)){
tp=self.pvals[[i]]
datSet=unique(all.self.pvals$dataSet)[i]
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="cv"]=sum(tp$cvExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="sd"]=sum(tp$sdExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="med"]=sum(tp$medExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="kurt"]=sum(tp$kurtExceed==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="x2Sig"]=sum(tp$x2SigDiff==1)/nrow(tp)
all.self.pvals$pval[all.self.pvals$dataSet==datSet & all.self.pvals$measure=="f"]=sum(tp$fStatExceed==1)/nrow(tp)
}
# not including x2 for this plot since I'm using the calculation differently.
ggplot(all.self.pvals[all.self.pvals$measure!="x2Sig",])+theme_classic()+
geom_point(aes(x=measure, y=pval), size=2)+facet_wrap(~dataSet)+
geom_hline(yintercept = c(0.1,0.9), linetype=2)+
geom_hline(yintercept = 0.5, linetype=4)+
labs(y="Bayesian p-value", x="Metric")
ggplot(all.self.pvals[all.self.pvals$measure!="x2Sig",])+theme_classic()+
geom_point(aes(x=measure, y=pval), size=2)+facet_wrap(~dataSet)+
geom_hline(yintercept = c(0.1,0.9), linetype=2)+
geom_hline(yintercept = 0.5, linetype=4)+
labs(y="Bayesian p-value", x="Metric")
