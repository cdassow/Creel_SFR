---
title: "Exploitation Rate Creel Titration"
author: "Colin Dassow"
date: "`r Sys.Date()`"
output: bookdown::pdf_document2

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F, cache = T, tidy = TRUE, tidy.opts = list(width.cutoff = 60))

rm(list=ls())
library(wdnr.fmdb)
library(tinytex)
library(tidyverse)
library(ggpubr)
library(lubridate)
library(knitr)
library(BayesianTools)

setwd("C:/Users/dassocju/Documents/OAS_git_repos/Creel_SFR")

#reading in objects that take a long time to produce to improve rendering speed of this document
call=readRDS("creelDataSet_all.RData")
# unlist, etc. to get back to individual dfs
cserv=call[[1]]
cvis=call[[2]]
ccou=call[[3]]
cint=call[[4]]
cfish=call[[5]]
cfish.i=call[[6]]

```

# WDNR Creel Titration

## **Purpose**

The term 'titration' is used here to describe the main question this analysis seeks to answer:

> **How much less creel data could be collected, on an individual survey basis while still maintaining the stratified random sampling design, and still produce exploitation rate ($u$) estimates that are NOT significantly different from exploitation rates obtained using the current creel survey design?**

The data informing these analyses has all been pulled from the Fisheries Management Information System (FMIS) using the `wdnr.fmdb` `R` package built to interface with this database.
All analyses were conducted in `R` using `r version$version.string`.

## Current Creel Data as Baseline

DNR has amassed a large amount of inland creel information over many decades (1984-2023) across many waterbodies (n=314).
Using this data and functionality of `wdnr.fmdb` it is possible to calculate effort, catch, harvest, and harvest rate for multiple species for each unique creel survey conducted.
This information can be further grouped by month, season, day type, etc. to provide insight into important temporal patterns in the fisheries metrics of interest.
These calculations are accomplished fairly easily using the following code:

```{r creel-data-retreival, eval=FALSE, echo=T}

# reading in DNR creel data

cserv=get_creel_surveys()
cvis=get_creel_visits()
ccou=get_creel_counts()
cint=get_creel_int_party()
cfish=get_creel_fish_data()

```

```{r baseline-fisheries-metrics, echo=T}

ceff=calc_creel_effort(creel_count_data=ccou, creel_int_data=cint) 

charv=calc_creel_harvest(creel_count_data = ccou, creel_int_data = cint,creel_fish_data = cfish) 

charvR=calc_creel_harvest_rates(creel_fish_data=cfish)

```

## Walleye Exploitation Rate *u*

Here I'm dealing specifically with walleye exploitation rate estimates for ceded territory lakes only.
Exploitation rate is a key metric under the Voigt Decision and any proposed changes to creel surveys would need to ensure that the proper exploitation rate information can still be gathered.

What I've done here is calculate exploitation rates based on the full data sets as well as for 5 scenarios:

1.  all winter creel information removed

2.  only creel information from 'summer' May to August is used

3.  25% of weekday creel visits per month are removed

4.  50% of weekday creel visits per month are removed

5.  50% of weekend creel visits per month are removed.
    Creel visits are removed on a month by month basis by randomly removing creel visits to the lake that day

This random removal of days per month and day type should maintain the statistical integrity of the stratified random design that is a part of the creel survey.
For context, this was not how data was removed in a similar analysis done by Deroba et al. (2007) where they removed entire weeks (randomly selected) per month instead of randomly selecting days.
Their analysis also employed t-tests to analyze the resulting exploitation rates in each of their reduced data sets which relies on the assumption of normality and has been discussed above.

Exploitation rate ($u$) was calculated as:

$$ u=\frac{R}{M} $$ $$ R=(\frac{C}{T})H$$

Where $R$ is the estimated total number of marked fish harvested for a given creel strata and $M$ is the total number of walleye marked that spring during fyke net surveys.
The estimate of the number of marked fish harvested per strata ($R$) is obtained by applying the proportion of marked fish observed in the creel to the total harvest estimate for that strata ($H$).
$C$ is the actual number of marked fish observed in a given creel strata and $T$ is the total number of fish examined.

```{r u-rate-calcs}

lchar=get_fmdb_lakechar()
ctwiWBIC=lchar[lchar$trtystat==1 & !is.na(lchar$trtystat),]
ctwiWBIC.creel=ctwiWBIC$wbic[ctwiWBIC$wbic%in%cserv$wbic] # wbics in CTWI that have been creeled

tagdat=read.csv("tags_and_marks.csv") # marking data from fmdb since the package function URLs didn't seem to be working
tagdat$County=standardize_county_names(tagdat$County)
tagdat$Gear=standardize_string(tagdat$Gear)
tagdat$Waterbody.Name=standardize_waterbody_names(tagdat$Waterbody.Name)

fndat=tagdat%>% # this is just in case we care what type of mark is there, but I'm going to overwrite this for now and just get total number marked.
  filter(WBIC%in%ctwiWBIC.creel & Species.Code=="X22" & Gear=="fyke_net", !is.na(Mark.Given) & Mark.Given!="")%>%
  group_by(WBIC, Waterbody.Name,Survey.Year,Survey.Begin.Date,Survey.End.Date,Survey.Seq.No,Mark.Given)%>%
  summarise(nFish=sum(Number.of.Fish))

# now summing across all types of marks
fndat=tagdat%>% 
  filter(WBIC%in%ctwiWBIC.creel & Species.Code=="X22" & Gear=="fyke_net", !is.na(Mark.Given) & Mark.Given!="")%>%
  group_by(WBIC, Waterbody.Name,Survey.Year,Survey.Begin.Date,Survey.End.Date,Survey.Seq.No)%>%
  summarise(nFish=sum(Number.of.Fish))%>%
  mutate(Survey.Begin.Date=lubridate::mdy(Survey.Begin.Date),
         Survey.End.Date=lubridate::mdy(Survey.End.Date),
         begin.md=format(Survey.Begin.Date, "%m-%d"),
         end.md=format(Survey.End.Date, "%m-%d"))%>%
  filter(begin.md<"06-01") # getting rid of any marking surveys that may have taken place well after fishing season opened.

fdat=fndat%>% # now that I have only surveys I want, pooling across start and end dates within a year to create an annual total number marked for comparison to creel data
  group_by(WBIC, Waterbody.Name, Survey.Year)%>%
  summarise(nFN.marked=sum(nFish))

# table of marks found during creel surveys
crRecap=cfish.i%>%
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  mutate(month=month(sample.date),
         daytype=ifelse(wday(sample.date)%in%c(1,7),"weekend","weekday"))%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel
crRecap=crRecap%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# I think spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates=charv%>%
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst=crRecap%>%
  left_join(harvestEstimates)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp=markHarvEst%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked) # checked against the Nebagamon example provided by Tom and I'm really close with these estimates. It looks like my # of clipped fish counts are different from what Tom has in his exploitation DB. Not sure why that is but that's what appears to be causing the slight different in our exploitation rate estimates.

naExps=ang.exp[is.na(ang.exp$exp.rate),] # looking at the lake-years without data to see if there's a problem with my code or just missing data
# looks like it's either cases were no fish were marked in spring fyking for that creel survey or if for a specific strata no walleye were harvested or no marked walleye were harvested, either of those will throw an NA in the exploitation rate calculation

# there are 6 observation where the number of marks returned was higher than what was marked, I'm throwing these out.
ang.exp=ang.exp[!is.na(ang.exp$exp.rate) & ang.exp$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp=ang.exp%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

# now I have 'actual' exploitation rates

# next I want to make a few scenarios with 'reduced' data, a 'noWinter' scenario, 'May-August' scenario, and 3 % removals (0.25 weekdays, 0.5 weekdays, 0.5 weekend)

ttrExp=as.data.frame(matrix(NA,ncol=ncol(cfish.i)))
colnames(ttrExp)=colnames(cfish.i)

ifish=cfish.i%>% # making a data frame to add my groupings too so I can leave cfish.i alone
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  mutate(daytype=ifelse(wday(sample.date)%in%c(1,7),"weekday","weekend"),
         month=month(sample.date),
         season=ifelse(month%in%4:10,"openwater","winter"))

iharv=charv%>% # making a separate data frame to add groupings to so I can leave charv alone
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  mutate(season=ifelse(month%in%4:10,"openwater","winter"))

# now making a reduced data frame for each scenario

###### NO WINTER ####

ifish.nw=ifish%>%
  filter(season=="openwater")%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

ifish.nw=ifish.nw%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.nw=iharv%>%
  filter(season=="openwater")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.nw=ifish.nw%>%
  left_join(harvestEstimates.nw)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.nw=markHarvEst.nw%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.nw=ang.exp.nw[!is.na(ang.exp.nw$exp.rate) & ang.exp.nw$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp.nw=ang.exp.nw%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

###### SUMMER ONLY ####

ifish.ma=ifish%>%
  filter(month%in%c(5:8))%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

ifish.ma=ifish.ma%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.ma=iharv%>%
  filter(month%in%c(5:8))%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.ma=ifish.ma%>%
  left_join(harvestEstimates.ma)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.ma=markHarvEst.ma%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.ma=ang.exp.ma[!is.na(ang.exp.ma$exp.rate) & ang.exp.ma$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp.ma=ang.exp.ma%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

# for each of these % reductions I'm removing data based on visit fish seq no to simulate what would happen if that creel clear visit to the lake were removed.
###### 25% WEEKDAYS ####

rm.fish.seq.no=NA
rm.visit.fish.seq.no=NA
ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$daytype=="weekday",]
  ms=unique(full$month)
  if(nrow(full)>0){
    for(m in 1:length(ms)){ # month loop
      visits=unique(full$visit.fish.seq.no[full$month==ms[m]])
      visit.rm=visits[c(round(runif(round(0.25*length(visits)),min=1,max = length(visits))))]
      t.rm=full$fish.data.seq.no[full$visit.fish.seq.no%in%visit.rm]# fish seq nos to remove that associated with the visit seq nos selected for removal
      
      rm.visit.fish.seq.no=c(rm.visit.fish.seq.no,visit.rm) # visit fish seq nos to use to reduce the interview, count, and fish data needed to get harvest estimates
      rm.fish.seq.no=c(rm.fish.seq.no,t.rm) # vector of fish to remove from individual fish data
      
    }
  }
}

ifish.25wd=ifish[!(ifish$fish.data.seq.no%in%rm.fish.seq.no),] # reduced individual fish data
iint.25wd=cint%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced interview data
icou.25wd=ccou%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced count data
ifishAg.25wd=cfish%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced aggregate fish data

iharv.25wd=calc_creel_harvest(creel_count_data = icou.25wd,
                              creel_int_data = iint.25wd,
                              creel_fish_data = ifishAg.25wd) # harvest estimates from the reduced data

wd25=ifish.25wd%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

wd25=wd25%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.25wd=iharv.25wd%>%
  filter(species.code=="X22")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.25wd=wd25%>%
  left_join(harvestEstimates.25wd)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.25wd=markHarvEst.25wd%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.25wd=ang.exp.25wd[!is.na(ang.exp.25wd$exp.rate) & ang.exp.25wd$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp.25wd=ang.exp.25wd%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))


###### 50% WEEKDAYS ####

rm.fish.seq.no=NA
rm.visit.fish.seq.no=NA
ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$daytype=="weekday",]
  ms=unique(full$month)
  if(nrow(full)>0){
    for(m in 1:length(ms)){ # month loop
      visits=unique(full$visit.fish.seq.no[full$month==ms[m]])
      visit.rm=visits[c(round(runif(round(0.50*length(visits)),min=1,max = length(visits))))]
      t.rm=full$fish.data.seq.no[full$visit.fish.seq.no%in%visit.rm]# fish seq nos to remove that associated with the vist seq nos selected for removal
      
      rm.visit.fish.seq.no=c(rm.visit.fish.seq.no,visit.rm) # visit fish seq nos to use to reduce the interview, count, and fish data needed to get harvest estimates
      rm.fish.seq.no=c(rm.fish.seq.no,t.rm) # vector of fish to remove from individual fish data
      
    }
  }
}

ifish.50wd=ifish[!(ifish$fish.data.seq.no%in%rm.fish.seq.no),] # reduced individual fish data
iint.50wd=cint%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced interview data
icou.50wd=ccou%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced count data
ifishAg.50wd=cfish%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced aggregate fish data

iharv.50wd=calc_creel_harvest(creel_count_data = icou.50wd,
                              creel_int_data = iint.50wd,
                              creel_fish_data = ifishAg.50wd) # harvest estimates from the reduced data

wd50=ifish.50wd%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

wd50=wd50%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.50wd=iharv.50wd%>%
  filter(species.code=="X22")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.50wd=wd50%>%
  left_join(harvestEstimates.50wd)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.50wd=markHarvEst.50wd%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.50wd=ang.exp.50wd[!is.na(ang.exp.50wd$exp.rate) & ang.exp.50wd$exp.rate<1.0,]

# creating survey-level exploitation rates
ang.exp.50wd=ang.exp.50wd%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

###### 50% WEEKENDS ####

rm.fish.seq.no=NA
rm.visit.fish.seq.no=NA
ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$daytype=="weekdend",]
  ms=unique(full$month)
  if(nrow(full)>0){
    for(m in 1:length(ms)){ # month loop
      visits=unique(full$visit.fish.seq.no[full$month==ms[m]])
      visit.rm=visits[c(round(runif(round(0.50*length(visits)),min=1,max = length(visits))))]
      t.rm=full$fish.data.seq.no[full$visit.fish.seq.no%in%visit.rm]# fish seq nos to remove that associated with the vist seq nos selected for removal
      
      rm.visit.fish.seq.no=c(rm.visit.fish.seq.no,visit.rm) # visit fish seq nos to use to reduce the interview, count, and fish data needed to get harvest estimates
      rm.fish.seq.no=c(rm.fish.seq.no,t.rm) # vector of fish to remove from individual fish data
      
    }
  }
}

ifish.50we=ifish[!(ifish$fish.data.seq.no%in%rm.fish.seq.no),] # reduced individual fish data
iint.50we=cint%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced interview data
icou.50we=ccou%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced count data
ifishAg.50we=cfish%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced aggregate fish data

iharv.50we=calc_creel_harvest(creel_count_data = icou.50we,
                              creel_int_data = iint.50we,
                              creel_fish_data = ifishAg.50we) # harvest estimates from the reduced data

we50=ifish.50we%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

we50=we50%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.50we=iharv.50we%>%
  filter(species.code=="X22")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.50we=we50%>%
  left_join(harvestEstimates.50we)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.50we=markHarvEst.50we%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.50we=ang.exp.50we[!is.na(ang.exp.50we$exp.rate) & ang.exp.50we$exp.rate<1.0,]

# creating survey-level exploitation rates
ang.exp.50we=ang.exp.50we%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

```

When calculating $u$ for each data set I first calculated $u$ by each month and day type then calculated an average $u$ for that creel survey based on `survey.seq.no` to produce one exploitation rate for that survey.
This was done to match the exploitation rate database which provides exploitation rates on a survey-level and not the strata-level.

> **Another important note, all exploitation rates = 0 were dropped before modeling for two reasons, 1) because the data is lognormally distributed (\@ref(fig:monthly-rates), \@ref(fig:u-rate-plot)) taking the log of 0 isn't possible and the typical approach would be to add a small number to make it non-zero but this leads to skewed data in this case that would require modeling with a different data distribution, likely gamma or beta. This could be done but on short notice here I stuck with lognormal and dropped 0s. 2) If a creel survey produces a 0 now, chances are that as more data is removed from that survey it won't suddenly become non-zero, so the 0s are not likely to change under future creel designs. If anything more lakes may produce 0 exploitation rates if fewer samples are taken and the scenarios here show just that.**

```{r, monthly-rates,fig.width=8,fig.cap="Distribution of exploitation rates by month (panel a) for all years of CTWI creel data. Horizontal red line is overall mean exploitation rate. Black points are outliers. Panel B is the same information presented on a log-scale. In both plots exploitation rates of 0 are omitted."}


ang.exp=markHarvEst%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked) # checked against the Nebagamon example provided by Tom and I'm really close with these estimates. It looks like my # of clipped fish counts are different from what Tom has in his exploitation DB. Not sure why that is but that's what appears to be causing the slight different in our exploitation rate estimates.

# there are observation where the number of marks returned was higher than what was marked, I'm throwing these out.
ang.exp=ang.exp[!is.na(ang.exp$exp.rate) & ang.exp$exp.rate<1.0,]

# plot of exploitation rate by month
ang.exp$plot.month=month(ang.exp$month,label = T)
p1=ggplot(ang.exp)+theme_classic()+
  geom_boxplot(aes(y=exp.rate, x=plot.month), fill="grey")+
  geom_hline(yintercept = mean(ang.exp$exp.rate, na.rm = T), color="red")+
  labs(y="Exploitation Rate", x="Month")

# plot of exploitation rate by month - logged
p2=ggplot(ang.exp)+theme_classic()+
  geom_boxplot(aes(y=log(exp.rate), x=plot.month), fill="grey")+
  geom_hline(yintercept = -4.449, color="red")+ # setting this manually since the 0 exp rates mess up the logging here
  labs(y="Log(Exploitation Rate)", x="Month")

ggarrange(p1,p2,nrow = 1,ncol=2,labels = 'auto')
```

```{r u-rate-plot, fig.cap="Distribution, on a log scale, of exploitation rates for the full data set plus 5 scenarios with reduced data. Actual = full data set, mayAug = May to August creel data only, noWinter = winter creel data removed, wd25 = 25% of weekday creel visits per month removed, wd50 = 50% of weekday creel visits per month removed, we50 = 50% of weekend creel visits per month removed."}

# creating survey-level exploitation rates for the actual data
ang.exp=ang.exp%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

# combining actual and reduced dataframes into one big one for plotting

ttrExp=rbind(cbind(ang.exp,treat=rep("actual",nrow(ang.exp))),
             cbind(ang.exp.nw, treat=rep("noWinter",nrow(ang.exp.nw))),
             cbind(ang.exp.ma, treat=rep("mayAug",nrow(ang.exp.ma))),
             cbind(ang.exp.25wd, treat=rep("wd25",nrow(ang.exp.25wd))),
             cbind(ang.exp.50wd, treat=rep("wd50",nrow(ang.exp.50wd))),
             cbind(ang.exp.50we, treat=rep("we50",nrow(ang.exp.50we))))

colnames(ttrExp)=c("survey.seq.no","exp.rate","exp.rate.sd","treat")
ggplot(ttrExp)+theme_classic()+
  geom_density(aes(x=log(exp.rate), fill=treat),alpha=0.2)+
  labs(x="Log(Exploitation Rate)", y="Density", fill="Scenario")
# data is non normally distributed, exp.rate is continuous and greater than 0, consider gamma or lognormal distribution for modeling
```

### Bayesian Model Fitting

In order to estimate parameters for a model of the exploitation rate distribution that produced what is represented in the creel data a Bayesian modeling approach was used.
This is because the exploitation rate data are not normally distributed and thus don't meet the assumptions of typical analyses based on normality.
Here I've chosen to model the data with a lognormal distribution, other potential options for the type of data are the gamma and beta distributions.
A Bayesian approach also allows for the inclusion of prior information about the system and does not rely on p-values (though some Bayesian p-values are presented later on) which are arbitrary in nature and can be manipulated via high sample sizes.
The Bayesian approach can be adapted to accommodate and prior distribution we feel is necessary be it lognormal, beta, or gamma.

Individual likelihoods were constructed for each reduced data set and fit using the functions in the `BayesianTools` package in `R`.
These model fits were checked for convergence using visual inspection of trace plots, posterior distributions, and Gelman-Rubin Diagnostic tests to ensure that models had converged.
All models were fit using Differential Evolution Markov-Chain-Monte-Carlo algorithms run for 10,000 iterations with the first 5,000 discarded as burn-in.

```{r u-bt-model-fits, echo=T}
# one likelihood to estimate parms for using the 6 different treatments
# creating data frame to model with u rates that are NA removed, these are 6% of the data and are from years where creel happened but no fish were FN marked concurrently.
modDat=ttrExp[!is.na(ttrExp$exp.rate),]

#removing 0s since they can't be logged and if I were to make them a small number they would throw off the data and make it bimodal which would probably mean switching to a gamma or beta distribution to model the data. Could work, but I don't have time to mess with that right now. I'm going to operate under the assumption that if a survey give a 0 exploitation rate with the full survey, then us collecting less data isn't going to change that number. 
zeros.actual=sum(modDat$exp.rate[modDat$treat=="actual"]==0)
zeros.nw=sum(modDat$exp.rate[modDat$treat=="noWinter"]==0)
zeros.ma=sum(modDat$exp.rate[modDat$treat=="mayAug"]==0)
zeros.wd25=sum(modDat$exp.rate[modDat$treat=="wd25"]==0)
zeros.wd50=sum(modDat$exp.rate[modDat$treat=="wd50"]==0)
zeros.we50=sum(modDat$exp.rate[modDat$treat=="we50"]==0)

Ztab=data.frame(Scenario=c("Actual","No Winter","May-August","25% Weekday Reduction","50% Weekday Reduction","50% Weekend Reduction"),
                Zeros=c(zeros.actual,zeros.nw,zeros.ma,zeros.wd25,zeros.wd50,zeros.we50))
kable(Ztab,
      format='latex',
      caption="Table of the number of surveys with exploitation rate estimates equal to 0 for each data scenario.")

# removing 0s here as described in the text.
modDat=modDat[modDat$exp.rate!=0,]

#### ACTUAL ####
uLL.a=function(param){
  alpha=param[1]
  beta=param[2]

  us=rlnorm(nrow(modDat[modDat$treat=="actual",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="actual"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="actual"])),sd(log(modDat$exp.rate[modDat$treat=="actual"]))),
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.actual=createBayesianSetup(uLL.a, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.actual=runMCMC(bayesianSetup = setup.actual, sampler = "DEzs", settings = settings)


#### NW ####
uLL.nw=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="noWinter",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="noWinter"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="noWinter"])),sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.nw=createBayesianSetup(uLL.nw, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.nw=runMCMC(bayesianSetup = setup.nw, sampler = "DEzs", settings = settings)

#### MA ####

uLL.ma=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="mayAug",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="mayAug"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="mayAug"])),sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.ma=createBayesianSetup(uLL.ma, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.ma=runMCMC(bayesianSetup = setup.ma, sampler = "DEzs", settings = settings)

#### WD25 ####

uLL.wd25=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="wd25",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="wd25"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd25"])),sd(log(modDat$exp.rate[modDat$treat=="wd25"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.wd25=createBayesianSetup(uLL.wd25, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.25wd=runMCMC(bayesianSetup = setup.wd25, sampler = "DEzs", settings = settings)

#### WD50 ####
uLL.wd50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="wd50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="wd50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd50"])),sd(log(modDat$exp.rate[modDat$treat=="wd50"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.wd50=createBayesianSetup(uLL.wd50, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.50wd=runMCMC(bayesianSetup = setup.wd50, sampler = "DEzs", settings = settings)

#### WE50 ####
uLL.we50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="we50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="we50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="we50"])),sd(log(modDat$exp.rate[modDat$treat=="we50"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.we50=createBayesianSetup(uLL.we50, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.50we=runMCMC(bayesianSetup = setup.we50, sampler = "DEzs", settings = settings)

```

### Model Checking

Here model fit is being checked in several ways.
Initially a visual inspection comparing data simulated using median parameter values from the posterior compared to the observed data for that scenario.
A good model fit here is evident when the distribution of the observed and predicted data overlap nearly entirely (\@ref(fig:u-data-reproduction)).

Gelman-Rubin diagnostics were also used to assess convergence, while this doesn't assess model fit it does describe whether or not the MCMC algorithm converged on a solution or whether the parameter space has not been fully explored yet.
For this test values below 1.1 are considered 'converged' with a value of 1 being the lowest possible score.
All models had Gelmin-Rubin test values < 1.03.

Lastly, a Bayesian p-value was calculated for each model to further describe model fit to the data (\@ref(tab:u-b-p-value)).
Bayesian p-values are based on the same idea as a frequentist p-value : *What is the probability of observing a more extreme test statistic than the one calculated from the observed data*.
To calculate a Bayesian p-value each set of parameter values in the MCMC chain is used to parameterize a log-normal distribution from which a set of 'new' data are drawn.
A test statistic is calculated for this new data and determined to either be more or less extreme than the test statistic is for observed data.
The proportion of these test statistics that are more extreme than the observed is then calculated.
If the model has done a good job of fitting the data then the parameter values should generally generate data that looks like the observed data and the test statistic should be equally likely to be more or less extreme than the observed value.
Thus, with Bayesian p-values a value of 0.5 is ideal as it means the model can generate data that matches the distribution of the observed data really well (\@ref(tab:u-b-p-value), b-p-value-comparison).
If this p-value was very low (\<.10) or very high (\>0.9) that would indicate that the model is not fitting the data well because it is unable to reproduce the data.

In this analysis two test statistics have been chosen to provide alternate, but not unrelated, measures of model fit.
These are the coefficient of variation and standard deviation.
Any test statistic of choice could be chosen as long as it can be justified for the objectives of the analysis at hand.

```{r u-data-reproduction, fig.width=8, fig.height=6 ,fig.cap="Distributions of the observed and model predicted data for each scenario. Model predicted data comes from lognormal distribution paramterized using the median parmater estimate of the model posterior."}

# looking to see if parm estimates produce data that visually at least looks like the observed data for that scenario
pars.a=getSample(expR.actual)
pars.nw=getSample(expR.nw)
pars.ma=getSample(expR.ma)
pars.wd25=getSample(expR.25wd)
pars.wd50=getSample(expR.50wd)
pars.we50=getSample(expR.50we)

aComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                         meanlog = median(pars.a[,1]),
                                                                         sdlog = median(pars.a[,2]))),
                    treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="actual"])),rep("pred",length(modDat$exp.rate[modDat$treat=="actual"]))))

nwComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="noWinter"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
                                                                    meanlog = median(pars.nw[,1]),
                                                                    sdlog = median(pars.nw[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="noWinter"])),rep("pred",length(modDat$exp.rate[modDat$treat=="noWinter"]))))

maComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="mayAug"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
                                                                    meanlog = median(pars.ma[,1]),
                                                                    sdlog = median(pars.ma[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="mayAug"])),rep("pred",length(modDat$exp.rate[modDat$treat=="mayAug"]))))

wd25Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="wd25"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
                                                                    meanlog = median(pars.wd25[,1]),
                                                                    sdlog = median(pars.wd25[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="wd25"])),rep("pred",length(modDat$exp.rate[modDat$treat=="wd25"]))))

wd50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="wd50"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
                                                                    meanlog = median(pars.wd50[,1]),
                                                                    sdlog = median(pars.wd50[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="wd50"])),rep("pred",length(modDat$exp.rate[modDat$treat=="wd50"]))))

we50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="we50"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
                                                                    meanlog = median(pars.we50[,1]),
                                                                    sdlog = median(pars.we50[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="we50"])),rep("pred",length(modDat$exp.rate[modDat$treat=="we50"]))))

a.p=ggplot(aComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "Actual", fill=element_blank())
nw.p=ggplot(nwComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "No Winter Data \n(April - October)", fill=element_blank())
ma.p=ggplot(maComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "May - August \nData Only", fill=element_blank())
wd25.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "25% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
wd50.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
we50.p=ggplot(we50Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekdend \nCreel Visits/Month \nRemoved", fill=element_blank())

ggarrange(a.p,nw.p,ma.p,wd25.p,wd50.p,we50.p, common.legend = T)
```

```{r u-gelman-rubin-diagnostics, echo=T, eval=FALSE}

gelmanDiagnostics(expR.actual) # converged
gelmanDiagnostics(expR.nw) # converged
gelmanDiagnostics(expR.ma) # converged
gelmanDiagnostics(expR.25wd) # converged
gelmanDiagnostics(expR.50wd) # converged
gelmanDiagnostics(expR.50we) # converged

```

```{r u-b-p-value}

# dataframe to hold output
bpValues=data.frame(scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                    coef.var.pval=NA,
                    sd.pval=NA)
#### Pb ACTUAL ####
pval.actual=data.frame(alpha=rep(NA,nrow(pars.a)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.a)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                 meanlog = pars.a[i,1],
                 sdlog = pars.a[i,2])
  pval.actual$alpha[i]=pars.a[i,1]
  pval.actual$beta[i]=pars.a[i,2]
  pval.actual$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.actual$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.actual$cvExceed=0
pval.actual$sdExceed=0

actual.cv=sd(log(modDat$exp.rate[modDat$treat=="actual"]))/mean(log(modDat$exp.rate[modDat$treat=="actual"]))
actual.sd=sd(log(modDat$exp.rate[modDat$treat=="actual"]))

pval.actual$cvExceed[pval.actual$cv>actual.cv]=1
pval.actual$sdExceed[pval.actual$sd>actual.sd]=1

bpValues$coef.var.pval[1]=sum(pval.actual$cvExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[1]=sum(pval.actual$sdExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb NO WINTER ####
pval.noWinter=data.frame(alpha=rep(NA,nrow(pars.nw)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.nw)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
                 meanlog = pars.nw[i,1],
                 sdlog = pars.nw[i,2])
  pval.noWinter$alpha[i]=pars.nw[i,1]
  pval.noWinter$beta[i]=pars.nw[i,2]
  pval.noWinter$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.noWinter$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.noWinter$cvExceed=0
pval.noWinter$sdExceed=0

noWinter.cv=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))/mean(log(modDat$exp.rate[modDat$treat=="noWinter"]))
noWinter.sd=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))

pval.noWinter$cvExceed[pval.noWinter$cv>noWinter.cv]=1
pval.noWinter$sdExceed[pval.noWinter$sd>noWinter.sd]=1

bpValues$coef.var.pval[2]=sum(pval.noWinter$cvExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[2]=sum(pval.noWinter$sdExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb MAYAUGUST ####
pval.mayAug=data.frame(alpha=rep(NA,nrow(pars.ma)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.ma)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
                 meanlog = pars.ma[i,1],
                 sdlog = pars.ma[i,2])
  pval.mayAug$alpha[i]=pars.ma[i,1]
  pval.mayAug$beta[i]=pars.ma[i,2]
  pval.mayAug$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.mayAug$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.mayAug$cvExceed=0
pval.mayAug$sdExceed=0

mayAug.cv=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))/mean(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.sd=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))

pval.mayAug$cvExceed[pval.mayAug$cv>mayAug.cv]=1
pval.mayAug$sdExceed[pval.mayAug$sd>mayAug.sd]=1

bpValues$coef.var.pval[3]=sum(pval.mayAug$cvExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[3]=sum(pval.mayAug$sdExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WD25 ####
pval.wd25=data.frame(alpha=rep(NA,nrow(pars.wd25)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.wd25)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
                 meanlog = pars.wd25[i,1],
                 sdlog = pars.wd25[i,2])
  pval.wd25$alpha[i]=pars.wd25[i,1]
  pval.wd25$beta[i]=pars.wd25[i,2]
  pval.wd25$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.wd25$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.wd25$cvExceed=0
pval.wd25$sdExceed=0

wd25.cv=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))/mean(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.sd=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))

pval.wd25$cvExceed[pval.wd25$cv>wd25.cv]=1
pval.wd25$sdExceed[pval.wd25$sd>wd25.sd]=1

bpValues$coef.var.pval[4]=sum(pval.wd25$cvExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[4]=sum(pval.wd25$sdExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WD50 ####
pval.wd50=data.frame(alpha=rep(NA,nrow(pars.wd50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.wd50)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
                 meanlog = pars.wd50[i,1],
                 sdlog = pars.wd50[i,2])
  pval.wd50$alpha[i]=pars.wd50[i,1]
  pval.wd50$beta[i]=pars.wd50[i,2]
  pval.wd50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.wd50$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.wd50$cvExceed=0
pval.wd50$sdExceed=0

wd50.cv=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))/mean(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.sd=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))

pval.wd50$cvExceed[pval.wd50$cv>wd50.cv]=1
pval.wd50$sdExceed[pval.wd50$sd>wd50.sd]=1

bpValues$coef.var.pval[5]=sum(pval.wd50$cvExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[5]=sum(pval.wd50$sdExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WE50 ####
pval.we50=data.frame(alpha=rep(NA,nrow(pars.we50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.we50)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
                 meanlog = pars.we50[i,1],
                 sdlog = pars.we50[i,2])
  pval.we50$alpha[i]=pars.we50[i,1]
  pval.we50$beta[i]=pars.we50[i,2]
  pval.we50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.we50$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.we50$cvExceed=0
pval.we50$sdExceed=0

we50.cv=sd(log(modDat$exp.rate[modDat$treat=="we50"]))/mean(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.sd=sd(log(modDat$exp.rate[modDat$treat=="we50"]))

pval.we50$cvExceed[pval.we50$cv>we50.cv]=1
pval.we50$sdExceed[pval.we50$sd>we50.sd]=1

bpValues$coef.var.pval[6]=sum(pval.we50$cvExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[6]=sum(pval.we50$sdExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

kable(bpValues, digits=3, caption="Bayesian p-values for two variance metrics and each modeled data set.")
```

### Inference

Having established that the models are fitting the data well, some inference can be gained as to whether or not the reductions in creel effort proposed here would result in a meaningful change in the exploitation rates estimated from the data.

```{r u-data-comparison-to-actual, fig.width=8,fig.height=6,fig.cap="Comparison of the distribution of actual exploitation rates calculated from creel data and exploitation rates calculated from simulated creel data for each data reduction scenario and the resulting median parameter values for their respective model fits."}

aComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                    meanlog = median(pars.a[,1]),
                                                                    sdlog = median(pars.a[,2]))),
                 treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

nwComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                       meanlog = median(pars.nw[,1]),
                                                                       sdlog = median(pars.nw[,2]))),
                  treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

maComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.ma[,1]),
                                                                     sdlog = median(pars.ma[,2]))),
                  treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

wd25Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.wd25[,1]),
                                                                     sdlog = median(pars.wd25[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

wd50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.wd50[,1]),
                                                                     sdlog = median(pars.wd50[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

we50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.we50[,1]),
                                                                     sdlog = median(pars.we50[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

a.p=ggplot(aComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "Actual", fill=element_blank())
nw.p=ggplot(nwComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "No Winter Data \n(April - October)", fill=element_blank())
ma.p=ggplot(maComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "May - August \nData Only", fill=element_blank())
wd25.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "25% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
wd50.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
we50.p=ggplot(we50Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekdend \nCreel Visits/Month \nRemoved", fill=element_blank())

ggarrange(a.p,nw.p,ma.p,wd25.p,wd50.p,we50.p, common.legend = T)
```

A Bayesian p-value can be employed here too.
A test between the cv and sd of the actual data and the cv and sd of the simulated reduction data can describe whether the model of the reduced data is able to approximate the actual data or not.
Successful approximations are p-values close to 0.5 with reduced fit as values increase or decrease beyond 0.5.
Generally, the rule of thumb is that p-values $<0.10$ or $>0.90$ signal unacceptable fits.

The results of these test suggest that when using coefficient of variation as the variance metric to assess model fit and the $<0.10$ or $>0.90$ rule that the no winter and may-august data reduction models both approximate the actual data ok, but not as well as the 3 percentage reductions whose Bayesian p-values suggest they produce exploitation rate distributions that are very similar to the actual data (\@ref(tab:b-p-value-comparison)).
When using standard deviation as the metric for comparison then all models approximate the actual data pretty well as the model fit to the actual data (\@ref(tab:b-p-value-comparison)).
The 'No Winter' scenario and two 50% reduction scenarios are starting to creep higher but still not to a point where they're different from the actual data.
The reason for this discrepancy between metrics is likely that the coefficient of variation is standardized based on the mean of the distribution while the standard deviation is not and is a raw variance metric.
In this case using the coefficient of variation results seems like the most pragmatic choice, it is also supported the by multi-panel plot comparing the actual exploitation rates to the distributions of simulated data from each scenario (\@ref(fig:u-data-comparison-to-actual)).
That would appear to be two lines of evidence pointing in the same direction.

```{r b-p-value-comparison, warning=F, message=F}
## p-value tests comparing the scenarios to actual to show that some scenarios approximate the actual quite well.

bpval.comp=data.frame(scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                      coef.var.pval=NA,
                      sd.pval=NA)
pval.actual$cvComp=0
pval.actual$sdComp=0
pval.actual$cvComp[pval.actual$cv>actual.cv]=1
pval.actual$sdComp[pval.actual$sd>actual.sd]=1

pval.noWinter$cvComp=0
pval.noWinter$sdComp=0
pval.noWinter$cvComp[pval.noWinter$cv>actual.cv]=1
pval.noWinter$sdComp[pval.noWinter$sd>actual.sd]=1

pval.mayAug$cvComp=0
pval.mayAug$sdComp=0
pval.mayAug$cvComp[pval.mayAug$cv>actual.cv]=1
pval.mayAug$sdComp[pval.mayAug$sd>actual.sd]=1

pval.wd25$cvComp=0
pval.wd25$sdComp=0
pval.wd25$cvComp[pval.wd25$cv>actual.cv]=1
pval.wd25$sdComp[pval.wd25$sd>actual.sd]=1

pval.wd50$cvComp=0
pval.wd50$sdComp=0
pval.wd50$cvComp[pval.wd50$cv>actual.cv]=1
pval.wd50$sdComp[pval.wd50$sd>actual.sd]=1

pval.we50$cvComp=0
pval.we50$sdComp=0
pval.we50$cvComp[pval.we50$cv>actual.cv]=1
pval.we50$sdComp[pval.we50$sd>actual.sd]=1

bpval.comp$coef.var.pval=c(sum(pval.actual$cvComp)/nrow(pval.actual),
                           sum(pval.noWinter$cvComp)/nrow(pval.noWinter),
                           sum(pval.mayAug$cvComp)/nrow(pval.mayAug),
                           sum(pval.wd25$cvComp)/nrow(pval.wd25),
                           sum(pval.wd50$cvComp)/nrow(pval.wd50),
                           sum(pval.we50$cvComp)/nrow(pval.we50))

bpval.comp$sd.pval=c(sum(pval.actual$sdComp)/nrow(pval.actual),
                           sum(pval.noWinter$sdComp)/nrow(pval.noWinter),
                           sum(pval.mayAug$sdComp)/nrow(pval.mayAug),
                           sum(pval.wd25$sdComp)/nrow(pval.wd25),
                           sum(pval.wd50$sdComp)/nrow(pval.wd50),
                           sum(pval.we50$sdComp)/nrow(pval.we50))
kable(bpval.comp, digits=3, caption="Bayesian p-values for the comparison between the actual data and the scenario-specific model. This test is asking whether the scenario-specific model can approximate the actual data. When true this signals no effect of the data reduction for that scenario on the resulting exploitation rates.")
```


### Year Specific Analyses

Instead of modeling the full population of exploitation rates as was done above, here I have done the model fitting on individual creel-years so that I can see how the removal of data for a creel-year (which is a comparatively large amount of missing data since on 16=20 lakes are creeled each year) impacts the exploitation rates estimated that year and used to set safe harvest limits.
This analysis will follow the same routine as the whole-population analyses above but only consider one creel-year at a time.

I first started with some basic analyses of the data on a year by year basis to understand how the data changes from year to year and if any lake characteristics explained variation (if there is any) in exploitation rates across data reduction scenarios for specific creel-years (\@ref(fig:year-analysis-example-lakes)).

```{r year-analysis-example-lakes, fig.width=8,fig.cap="Comparison of the exploitation rates estimated from the reduced data for a handful of creel surveys. Most data reductions do not result in exploitaion rates that are much different from the actual data (most are within 1 SD of the actual). Colors represent data reduction scenarios, points are mean exploitaiton rate estimates across all months of that creel survey and vertical lines represent 1 standard deviation."}
ttrExp=ttrExp%>%
  left_join(cserv[,2:5])

pdat=ttrExp[ttrExp$year%in%c(2015,2019,2022),]
ggplot(pdat)+theme_classic()+
  geom_pointrange(aes(x=paste(year,waterbody.name,sep = "_"), 
                      y=exp.rate, ymin=exp.rate-exp.rate.sd, 
                      ymax=exp.rate+exp.rate.sd, color=treat), 
                  position = position_dodge(width = 1))+
  coord_cartesian(ylim = c(0,0.2))+
  theme(axis.text.x = element_text(angle=45,hjust=1), legend.position = c(.75,.75))+
  labs(x="Lake-Year",y="Exploitation Rate (+/- 1 SD)",color="Scenario")+
  scale_color_viridis_d()

```

```{r year-analysis-diff-actual, fig.cap="Boxplot of the difference between the actual exploitaiton rate and the data-reduced exploitaion rate for each creel survey in the creel dataset. Most of the data exhibits differences very near 0, dots represent outliers (x<|> x's percentile-1.5*interquartile range)"}

# metrics comparing differences in individual lake estimates

trLake=ttrExp
trLake$a.diff=NA
trLake$a.exceed=NA

for(i in 1:nrow(trLake)){
  trLake$a.diff[i]=trLake$exp.rate[trLake$treat=="actual" & trLake$survey.seq.no==trLake$survey.seq.no[i]]-trLake$exp.rate[i]
  trLake$a.exceed[i]=trLake$a.diff[i]>trLake$exp.rate.sd[trLake$treat=="actual" & trLake$survey.seq.no==trLake$survey.seq.no[i]]
}

ggplot(trLake)+theme_classic()+
   geom_boxplot(aes(y=a.diff, x=treat))+
  labs(x="Data Reduction Scenario",y="Difference from actual")
```

```{r year-analysis-exceedenceTable}
exceedSummary=trLake%>%
  group_by(treat)%>%
  summarise(nTrue=sum(a.exceed,na.rm = T),
            nFalse=sum(a.exceed==F,na.rm = T),
            nNA=sum(is.na(a.exceed)))
kable(exceedSummary,
      col.names = c("Data Reduction Scenario","u's exceeding actual","u's not exceeding actual","NAs"),
      caption = "Counts of the number of times the exploitation rate estimate for the reduced data scenario differes from the actual exploitaion rate by more than 1 standard deviation. NAs ocurred where there was no variance in the exploitation rate estimate and thus no standard deviation could be calculated for the actual data.")

```

```{r year-analysis-recCode, fig.cap="Comparison of the magnitutde of the difference between the mean actual exploitation rate and the estimate derived from each reduced data set. Vertical lines represent 1 standard deviation. Walleye recruitment codes are along the x axis. Note the y-axis scale, many of the differences are small."}

# walleye recruitment class from lchar
# effort from ceff pooled to survey level
lEff=calc_creel_effort(creel_count_data = ccou,
                       creel_int_data = cint,
                       grouping = c("wbic","survey.seq.no","month","daytype"))
# current lake classes
lc=read.csv('lake class predictions.csv')
lc$LakeClass=gsub(" ","-",lc$LakeClass)

chars=lEff%>%
  group_by(wbic,survey.seq.no)%>%
  summarise(effort_hrs=sum(total.effort),
            effort_hrs.sd=sd(total.effort,na.rm=T))%>%
  left_join(lchar[,c(1,3,15,35)])%>%
  mutate(effortHrs.acre=effort_hrs/lake.area,
         effortHrs.acre.sd=sd(effort_hrs/lake.area,na.rm=T))%>%
  left_join(lc[,c(1,10)],by=c('wbic'='WBIC'))
trLake_chars=trLake%>%
  left_join(chars)

stClass=trLake_chars%>%
  group_by(wae.code,treat)%>%
  summarise(meanDiff=mean(a.diff,na.rm = T),
            sdDiff=sd(a.diff,na.rm=T))
ggplot(stClass)+theme_classic()+
  geom_pointrange(aes(x=wae.code,y=meanDiff,ymin=meanDiff-sdDiff,ymax=meanDiff+sdDiff,color=treat),position = position_dodge(width = 1))+
  scale_color_viridis_d()+theme(legend.position = c(0.7,0.3))+
  labs(x="WAE Recruitment Code",y="Mean Difference from Actual Exploitation Rate",color="Scenario")


```

```{r year-analysis-lakeArea, fig.cap="Comparison of the magnitutde of the difference between the mean actual exploitation rate and the estimate derived from each reduced data set. Note the y-axis scale, many of the differences are small."}
ggplot(trLake_chars)+theme_classic()+
  geom_point(aes(y=a.diff,x=log(lake.area),color=treat))+
  scale_color_viridis_d()+theme(legend.position = c(0.8,0.4))+
  labs(x="Log(Lake Area in Acres)", y="Difference from Actual exploitation rate",color="Scenario")
```

```{r year-analysis-effDens, fig.cap="Comparison of the magnitutde of the difference between the mean actual exploitation rate and the estimate derived from each reduced data set. Note the y-axis scale, many of the differences are small."}
ggplot(trLake_chars)+theme_classic()+
  geom_point(aes(y=a.diff,x=log(effortHrs.acre),color=treat))+
  scale_color_viridis_d()+theme(legend.position = c(0.8,0.4))+
  labs(x="Log(Effort/Acre)",y="Difference from Actual exploitation rate",color="Scenario")
```

```{r year-analysis-lakeClass,fig.cap="Comparison of the magnitutde of the difference between the mean actual exploitation rate and the estimate derived from each reduced data set. Vertical lines represent 1 standard deviation. Note the y-axis scale, many of the differences are very small."}
lcSum=trLake_chars%>%
  group_by(LakeClass,treat)%>%
  summarise(meanDiff=mean(a.diff,na.rm = T),
            sdDiff=sd(a.diff,na.rm=T))
ggplot(lcSum)+theme_classic()+
  geom_pointrange(aes(x=LakeClass,y=meanDiff, ymin=meanDiff-sdDiff,ymax=meanDiff+sdDiff,color=treat),position = position_dodge(width = 1))+
  scale_color_viridis_d()+
  theme(axis.text.x = element_text(angle=45,hjust = 1), legend.position = c(0.8,0.25))+
  labs(x="Lake Class",y="Mean Difference from Actual Exploitation Rate", color="Scenario")
```

```{r year-loop, fig.width=8, fig.height=8, fig.cap="Distribution of exploitation rates across years in the creel data set. Different data reduction scenarios are noted with varying colors."}
# MODELING EFFECTS OF DATA REDUCTION ON INDIVIDUAL YEARS

# Tom and Joe expressed concern that even though the whole population distribution of exploitation rates doesn't change much there could be meaningful effects on the u estimates for individual lakes

# I think what I need to do is loop through each year and model just that year's data, calculate the p-values and store that information

# first a couple exploratory plots of u distribution by year

ggplot(ttrExp)+theme_classic()+
  geom_density(aes(log(exp.rate),fill=treat),alpha=0.2)+
  facet_wrap(~year,scales = 'free_y')+scale_fill_viridis_d()+
  theme(legend.position = "bottom")+
  labs(y="Density",x="Log(Exploitation Rate)", fill="Scenario")

# likelihoods to fit
uLL.we50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(tdat[tdat$treat=="we50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(tdat$exp.rate[tdat$treat=="we50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}
uLL.wd50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(tdat[tdat$treat=="wd50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(tdat$exp.rate[tdat$treat=="wd50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}
uLL.wd25=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(tdat[tdat$treat=="wd25",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(tdat$exp.rate[tdat$treat=="wd25"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}
uLL.ma=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(tdat[tdat$treat=="mayAug",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(tdat$exp.rate[tdat$treat=="mayAug"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}
uLL.nw=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(tdat[tdat$treat=="noWinter",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(tdat$exp.rate[tdat$treat=="noWinter"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}
uLL.a=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(tdat[tdat$treat=="actual",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(tdat$exp.rate[tdat$treat=="actual"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

loopY=sort(unique(ttrExp$year))

bpval.comp.y=data.frame(year=NA,
                      scenario=NA,
                      coef.var.pval=NA,
                      sd.pval=NA)

bpval.self.y=data.frame(year=NA,
                        scenario=NA,
                        coef.var.pval=NA,
                        sd.pval=NA)

for(y in 1:length(loopY)){
  #first get to year-specific data
  tdat=ttrExp[ttrExp$year==loopY[y],]
  # removing 0s,
  tdat=tdat[tdat$exp.rate!=0,]
  # Bayesian Model Fitting
  #ACTUAL

  prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="actual"])),sd(log(modDat$exp.rate[modDat$treat=="actual"]))), 
                                   sd=c(1,1),
                                   lower = c(-15,0),
                                   upper = c(15,5))
  
  setup.a=createBayesianSetup(uLL.a, prior = prior)
  
  settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
  set.seed(10)
  t.a=runMCMC(bayesianSetup = setup.a, sampler = "DEzs", settings = settings)
  #NW

  prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="noWinter"])),sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))), 
                                   sd=c(1,1),
                                   lower = c(-15,0),
                                   upper = c(15,5))
  
  setup.nw=createBayesianSetup(uLL.nw, prior = prior)
  
  settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
  set.seed(10)
  t.nw=runMCMC(bayesianSetup = setup.nw, sampler = "DEzs", settings = settings)
  #MA

  prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="mayAug"])),sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))), 
                                   sd=c(1,1),
                                   lower = c(-15,0),
                                   upper = c(15,5))
  
  setup.ma=createBayesianSetup(uLL.ma, prior = prior)
  
  settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
  set.seed(10)
  t.ma=runMCMC(bayesianSetup = setup.ma, sampler = "DEzs", settings = settings)
  #WD.25

  prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd25"])),sd(log(modDat$exp.rate[modDat$treat=="wd25"]))), 
                                   sd=c(1,1),
                                   lower = c(-15,0),
                                   upper = c(15,5))
  
  setup.wd25=createBayesianSetup(uLL.wd25, prior = prior)
  
  settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
  set.seed(10)
  t.25wd=runMCMC(bayesianSetup = setup.wd25, sampler = "DEzs", settings = settings)
  #WD.50

  prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd50"])),sd(log(modDat$exp.rate[modDat$treat=="wd50"]))), 
                                   sd=c(1,1),
                                   lower = c(-15,0),
                                   upper = c(15,5))
  
  setup.wd50=createBayesianSetup(uLL.wd50, prior = prior)
  
  settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
  set.seed(10)
  t.50wd=runMCMC(bayesianSetup = setup.wd50, sampler = "DEzs", settings = settings)
  
  #WE.50

  prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="we50"])),sd(log(modDat$exp.rate[modDat$treat=="we50"]))), 
                                   sd=c(1,1),
                                   lower = c(-15,0),
                                   upper = c(15,5))
  
  setup.we50=createBayesianSetup(uLL.we50, prior = prior)
  
  settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
  set.seed(10)
  t.50we=runMCMC(bayesianSetup = setup.we50, sampler = "DEzs", settings = settings)
  
  ## BAYESIAN P-VALUE CALCS
  
  pars.a=getSample(t.a)
  pars.nw=getSample(t.nw)
  pars.ma=getSample(t.ma)
  pars.wd25=getSample(t.25wd)
  pars.wd50=getSample(t.50wd)
  pars.we50=getSample(t.50we)
  
  #### Pb ACTUAL ####
  pval.actual=data.frame(alpha=rep(NA,nrow(pars.a)),
                         beta=NA,
                         cv=NA,
                         sd=NA)
  set.seed(10)
  for(i in 1:nrow(pars.a)){
    tempdat=rlnorm(n=length(tdat$exp.rate[tdat$treat=="actual"]),
                   meanlog = pars.a[i,1],
                   sdlog = pars.a[i,2])
    pval.actual$alpha[i]=pars.a[i,1]
    pval.actual$beta[i]=pars.a[i,2]
    pval.actual$cv[i]=sd(log(tempdat))/mean(log(tempdat))
    pval.actual$sd[i]=sd(log(tempdat))
  }
  
  # now calculate the number of times the cv or sd exceeds that of the real data
  
  pval.actual$cvExceed=0
  pval.actual$sdExceed=0
  
  actual.cv=sd(log(tdat$exp.rate[tdat$treat=="actual"]))/mean(log(tdat$exp.rate[tdat$treat=="actual"]))
  actual.sd=sd(log(tdat$exp.rate[tdat$treat=="actual"]))
  
  pval.actual$cvExceed[pval.actual$cv>actual.cv]=1
  pval.actual$sdExceed[pval.actual$sd>actual.sd]=1
  
  #### Pb NO WINTER ####
  pval.noWinter=data.frame(alpha=rep(NA,nrow(pars.nw)),
                           beta=NA,
                           cv=NA,
                           sd=NA)
  set.seed(10)
  for(i in 1:nrow(pars.nw)){
    tempdat=rlnorm(n=length(tdat$exp.rate[tdat$treat=="noWinter"]),
                   meanlog = pars.nw[i,1],
                   sdlog = pars.nw[i,2])
    pval.noWinter$alpha[i]=pars.nw[i,1]
    pval.noWinter$beta[i]=pars.nw[i,2]
    pval.noWinter$cv[i]=sd(log(tempdat))/mean(log(tempdat))
    pval.noWinter$sd[i]=sd(log(tempdat))
  }
  
  # now calculate the number of times the cv or sd exceeds that of the real data
  
  pval.noWinter$cvExceed=0
  pval.noWinter$sdExceed=0
  
  noWinter.cv=sd(log(tdat$exp.rate[tdat$treat=="noWinter"]))/mean(log(tdat$exp.rate[tdat$treat=="noWinter"]))
  noWinter.sd=sd(log(tdat$exp.rate[tdat$treat=="noWinter"]))
  
  pval.noWinter$cvExceed[pval.noWinter$cv>noWinter.cv]=1
  pval.noWinter$sdExceed[pval.noWinter$sd>noWinter.sd]=1
  
  #### Pb MAYAUGUST ####
  pval.mayAug=data.frame(alpha=rep(NA,nrow(pars.ma)),
                         beta=NA,
                         cv=NA,
                         sd=NA)
  set.seed(10)
  for(i in 1:nrow(pars.ma)){
    tempdat=rlnorm(n=length(tdat$exp.rate[tdat$treat=="mayAug"]),
                   meanlog = pars.ma[i,1],
                   sdlog = pars.ma[i,2])
    pval.mayAug$alpha[i]=pars.ma[i,1]
    pval.mayAug$beta[i]=pars.ma[i,2]
    pval.mayAug$cv[i]=sd(log(tempdat))/mean(log(tempdat))
    pval.mayAug$sd[i]=sd(log(tempdat))
  }
  
  # now calculate the number of times the cv or sd exceeds that of the real data
  
  pval.mayAug$cvExceed=0
  pval.mayAug$sdExceed=0
  
  mayAug.cv=sd(log(tdat$exp.rate[tdat$treat=="mayAug"]))/mean(log(tdat$exp.rate[tdat$treat=="mayAug"]))
  mayAug.sd=sd(log(tdat$exp.rate[tdat$treat=="mayAug"]))
  
  pval.mayAug$cvExceed[pval.mayAug$cv>mayAug.cv]=1
  pval.mayAug$sdExceed[pval.mayAug$sd>mayAug.sd]=1
  
  #### Pb WD25 ####
  pval.wd25=data.frame(alpha=rep(NA,nrow(pars.wd25)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
  set.seed(10)
  for(i in 1:nrow(pars.wd25)){
    tempdat=rlnorm(n=length(tdat$exp.rate[tdat$treat=="wd25"]),
                   meanlog = pars.wd25[i,1],
                   sdlog = pars.wd25[i,2])
    pval.wd25$alpha[i]=pars.wd25[i,1]
    pval.wd25$beta[i]=pars.wd25[i,2]
    pval.wd25$cv[i]=sd(log(tempdat))/mean(log(tempdat))
    pval.wd25$sd[i]=sd(log(tempdat))
  }
  
  # now calculate the number of times the cv or sd exceeds that of the real data
  
  pval.wd25$cvExceed=0
  pval.wd25$sdExceed=0
  
  wd25.cv=sd(log(tdat$exp.rate[tdat$treat=="wd25"]))/mean(log(tdat$exp.rate[tdat$treat=="wd25"]))
  wd25.sd=sd(log(tdat$exp.rate[tdat$treat=="wd25"]))
  
  pval.wd25$cvExceed[pval.wd25$cv>wd25.cv]=1
  pval.wd25$sdExceed[pval.wd25$sd>wd25.sd]=1
  
  #### Pb WD50 ####
  pval.wd50=data.frame(alpha=rep(NA,nrow(pars.wd50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
  set.seed(10)
  for(i in 1:nrow(pars.wd50)){
    tempdat=rlnorm(n=length(tdat$exp.rate[tdat$treat=="wd50"]),
                   meanlog = pars.wd50[i,1],
                   sdlog = pars.wd50[i,2])
    pval.wd50$alpha[i]=pars.wd50[i,1]
    pval.wd50$beta[i]=pars.wd50[i,2]
    pval.wd50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
    pval.wd50$sd[i]=sd(log(tempdat))
  }
  
  # now calculate the number of times the cv or sd exceeds that of the real data
  
  pval.wd50$cvExceed=0
  pval.wd50$sdExceed=0
  
  wd50.cv=sd(log(tdat$exp.rate[tdat$treat=="wd50"]))/mean(log(tdat$exp.rate[tdat$treat=="wd50"]))
  wd50.sd=sd(log(tdat$exp.rate[tdat$treat=="wd50"]))
  
  pval.wd50$cvExceed[pval.wd50$cv>wd50.cv]=1
  pval.wd50$sdExceed[pval.wd50$sd>wd50.sd]=1
  
  #### Pb WE50 ####
  pval.we50=data.frame(alpha=rep(NA,nrow(pars.we50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
  set.seed(10)
  for(i in 1:nrow(pars.we50)){
    tempdat=rlnorm(n=length(tdat$exp.rate[tdat$treat=="we50"]),
                   meanlog = pars.we50[i,1],
                   sdlog = pars.we50[i,2])
    pval.we50$alpha[i]=pars.we50[i,1]
    pval.we50$beta[i]=pars.we50[i,2]
    pval.we50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
    pval.we50$sd[i]=sd(log(tempdat))
  }
  
  # now calculate the number of times the cv or sd exceeds that of the real data
  
  pval.we50$cvExceed=0
  pval.we50$sdExceed=0
  
  we50.cv=sd(log(tdat$exp.rate[tdat$treat=="we50"]))/mean(log(tdat$exp.rate[tdat$treat=="we50"]))
  we50.sd=sd(log(tdat$exp.rate[tdat$treat=="we50"]))
  
  pval.we50$cvExceed[pval.we50$cv>we50.cv]=1
  pval.we50$sdExceed[pval.we50$sd>we50.sd]=1
  
  #df to hold pvals for model comparison to self, a way of knowing the model fit the data well
  t.pself=data.frame(year=rep(loopY[y],6),
                     scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                     coef.var.pval=NA,
                     sd.pval=NA)
  # adding self comparison pvals
  t.pself$coef.var.pval=c(sum(pval.actual$cvExceed)/nrow(pval.actual),
                          sum(pval.noWinter$cvExceed)/nrow(pval.noWinter),
                          sum(pval.mayAug$cvExceed)/nrow(pval.mayAug),
                          sum(pval.wd25$cvExceed)/nrow(pval.wd25),
                          sum(pval.wd50$cvExceed)/nrow(pval.wd50),
                          sum(pval.we50$cvExceed)/nrow(pval.we50))
  t.pself$sd.pval=c(sum(pval.actual$sdExceed)/nrow(pval.actual),
                          sum(pval.noWinter$sdExceed)/nrow(pval.noWinter),
                          sum(pval.mayAug$sdExceed)/nrow(pval.mayAug),
                          sum(pval.wd25$sdExceed)/nrow(pval.wd25),
                          sum(pval.wd50$sdExceed)/nrow(pval.wd50),
                          sum(pval.we50$sdExceed)/nrow(pval.we50))
  bpval.self.y=rbind(bpval.self.y,t.pself)
  ## p-value tests comparing the scenarios to actual to show that some scenarios approximate the actual quite well.
  
  t.pcomp=data.frame(year=rep(loopY[y],6),
                     scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                     coef.var.pval=NA,
                     sd.pval=NA)
  
  pval.actual$cvComp=0
  pval.actual$sdComp=0
  pval.actual$cvComp[pval.actual$cv>actual.cv]=1
  pval.actual$sdComp[pval.actual$sd>actual.sd]=1
  
  pval.noWinter$cvComp=0
  pval.noWinter$sdComp=0
  pval.noWinter$cvComp[pval.noWinter$cv>actual.cv]=1
  pval.noWinter$sdComp[pval.noWinter$sd>actual.sd]=1
  
  pval.mayAug$cvComp=0
  pval.mayAug$sdComp=0
  pval.mayAug$cvComp[pval.mayAug$cv>actual.cv]=1
  pval.mayAug$sdComp[pval.mayAug$sd>actual.sd]=1
  
  pval.wd25$cvComp=0
  pval.wd25$sdComp=0
  pval.wd25$cvComp[pval.wd25$cv>actual.cv]=1
  pval.wd25$sdComp[pval.wd25$sd>actual.sd]=1
  
  pval.wd50$cvComp=0
  pval.wd50$sdComp=0
  pval.wd50$cvComp[pval.wd50$cv>actual.cv]=1
  pval.wd50$sdComp[pval.wd50$sd>actual.sd]=1
  
  pval.we50$cvComp=0
  pval.we50$sdComp=0
  pval.we50$cvComp[pval.we50$cv>actual.cv]=1
  pval.we50$sdComp[pval.we50$sd>actual.sd]=1
  
  t.pcomp$coef.var.pval=c(sum(pval.actual$cvComp)/nrow(pval.actual),
                             sum(pval.noWinter$cvComp)/nrow(pval.noWinter),
                             sum(pval.mayAug$cvComp)/nrow(pval.mayAug),
                             sum(pval.wd25$cvComp)/nrow(pval.wd25),
                             sum(pval.wd50$cvComp)/nrow(pval.wd50),
                             sum(pval.we50$cvComp)/nrow(pval.we50))
  
  t.pcomp$sd.pval=c(sum(pval.actual$sdComp)/nrow(pval.actual),
                       sum(pval.noWinter$sdComp)/nrow(pval.noWinter),
                       sum(pval.mayAug$sdComp)/nrow(pval.mayAug),
                       sum(pval.wd25$sdComp)/nrow(pval.wd25),
                       sum(pval.wd50$sdComp)/nrow(pval.wd50),
                       sum(pval.we50$sdComp)/nrow(pval.we50))
  bpval.comp.y=rbind(bpval.comp.y,t.pcomp)
}

# removing the placeholder row of NAs
bpval.self.y=bpval.self.y[!is.na(bpval.self.y$year),]
bpval.comp.y=bpval.comp.y[!is.na(bpval.comp.y$year),]
```

```{r year-loop-model-fit, fig.cap="Comparison of coefficient of variations for bayesian p-values comparing the actual data to the simulated data for the creel year and data reduction scenario."}

ggplot(bpval.self.y)+theme_classic()+
  geom_point(aes(x=year, y=coef.var.pval))+facet_wrap(~scenario)+
  geom_hline(yintercept = c(0.9,0.5,0.1),color="red")+
  coord_cartesian(ylim=c(0,1))
```

```{r year-loop-comparison-to-actual, fig.cap="Comparison of the coeffienct of variations for bayesian p-values comparing the simulated data from each creel year and data reduction scenario to the actual data for that same creel year."}

ggplot(bpval.comp.y)+theme_classic()+
  geom_point(aes(x=year, y=coef.var.pval))+facet_wrap(~scenario)+
  geom_hline(yintercept = c(0.9,0.5,0.1),color="red")+
  coord_cartesian(ylim=c(0,1))
```

The results of these creel-year specific analyses suggest that even on an individual creel year level, the estimated exploitation rate from the data-reduced scenario generally is not significantly different from the exploitation rate calculated from the full data set (\@ref(fig:year-loop-comparison-to-actual)). 
Compared to the same analysis for the full population of creel data all at once instead of on a year-by-year basis, the estimated exploitation rates from the reduced data here don't match the actual quite as well but the majority of years still fall within the typical bounds of acceptance for this test.


There did not appear to be any obvious characteristics of the lakes themselves or their walleye populations that correlated with the magnitude of the difference between the actual estimated exploitation rate and the reduced-data estimation.
Walleye recruitment code (\@ref(fig:year-analysis-recCode)), angler effort density (\@ref(fig:year-analysis-effDens)), lake size (\@ref(fig:year-analysis-lakeArea)), and lake class (\@ref(fig:year-analysis-lakeClass)) were all examined and showed no clear trend. 
This may be in large part due to the relatively small differences between each estimate of exploitation rate and the reduced-data estimate that could make it hard to see an effects, which would further point towards there being minimal effect of data reductions on exploitation rate estimate even at the individual creel-year level.

### Important Caveats

Here are the important caveats for this analysis.
These are things that I would expect another researcher to raise as potential weak points of this work. 
Numbers 1 and 2 can be addressed by me in consultation with other researchers and the scientific literature.
Numbers 3 and 4 are more potentially informed by the legal context of the situation. These don't have 'right' answers in the way that the other two do. Here the 'right' answer will be values-based and likely depends on what the group, or legal system, decides.

1. How to deal with exploitation rate estimates equal to 0. I have outlined my approach to this above and the rationale behind it, but that doesn't mean someone else might not pick at that and say it's a flaw. One way to address this would probably be to model the data using a beta or gamma distribution instead of a lognormal distribution. This would allow the 0s to be included.
2. Accuracy measure. In order to compare the exploitation rate estimates from the reduced data to the actual data I used 1 standard deviation as my measure. This means that a reduced-data exploitation rate estimate that is within 1 SD of the actual data exploitation rate estimate was considered to be 'the same'. There may be better metrics to use here, and different people may have different opinions about the best metric to use. My rationale for using this metric has been outlined above.
3. Bayesian p-value metrics. when calculating the Bayesian p-value to understand if the data from the reduced scenarios resembled that of the actual data I used the coefficient of variation and standard deviation. In reality any variance metric could be used to compare the two data sets. I chose these two because I felt that were intuitive enough for anyone to understand and defensible. But that may be others that should be explored and the results of the p-value tests may differ based on the metric used.
4. Related to the p-value metric are the cutoffs to use to decide whether there is a meaningful difference between the actual exploitation rate and the data reduced rate. The rule of thumb for this test is values <0.1 or >0.9 indicate a significant difference between the actual data estimate and the reduced data estimate. In reality it is up to the user to decide what those cutoffs should be based on the context of their problem. Whether or not we consider the reduced data exploitation rates similar or different from the actual can obviously be influenced by the cutoff numbers we chose. 