---
title: "Exploitation Rate Creel Titration"
author: "Colin Dassow"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: TRUE
    toc_depth: 4
    number_sections: TRUE
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F, cache = T, tidy = TRUE, tidy.opts = list(width.cutoff = 60))

rm(list=ls())
library(wdnr.fmdb)
library(tidyverse)
library(ggpubr)
library(lubridate)
library(knitr)
library(BayesianTools)

setwd("C:/Users/dassocju/OneDrive - State of Wisconsin/Office_of_Applied_Science/Creel_SFR")

#reading in objects that take a long time to produce to improve rendering speed of this document
call=readRDS("creelDataSet_all.RData")
# unlist, etc. to get back to individual dfs
cserv=call[[1]]
cvis=call[[2]]
ccou=call[[3]]
cint=call[[4]]
cfish=call[[5]]
cfish.i=call[[6]]

```

# WDNR Creel Titration

## **Purpose**

The term 'titration' is used here to describe the main question this analysis seeks to answer:

> **How much less creel data could be collected, on an individual survey basis while still maintaining the stratified random sampling design, and still produce exploitation rate ($u$) estimates that are NOT significantly different from exploitation rates obtained using the current creel survey design?**

The data informing these analyses has all been pulled from the Fisheries Management Information System (FMIS) using the `wdnr.fmdb` `R` package built to interface with this database.
All analyses were conducted in `R` using `r version$version.string`.

### Current Creel Data as Baseline

DNR has amassed a large amount of inland creel information over many decades (1984-2023) across many waterbodies (n=314).
Using this data and functionality of `wdnr.fmdb` it is possible to calculate effort, catch, harvest, and harvest rate for multiple species for each unique creel survey conducted.
This information can be further grouped by month, season, day type, etc. to provide insight into important temporal patterns in the fisheries metrics of interest.
These calculations are accomplished fairly easily using the following code:

```{r creel-data-retreival, eval=FALSE, echo=T}

# reading in DNR creel data

cserv=get_creel_surveys()
cvis=get_creel_visits()
ccou=get_creel_counts()
cint=get_creel_int_party()
cfish=get_creel_fish_data()

```

```{r baseline-fisheries-metrics, echo=T}

ceff=calc_creel_effort(creel_count_data=ccou, creel_int_data=cint) 

charv=calc_creel_harvest(creel_count_data = ccou, creel_int_data = cint,creel_fish_data = cfish) 

charvR=calc_creel_harvest_rates(creel_fish_data=cfish)

```

### Walleye Exploitation Rate *u*

Here I'm dealing specifically with walleye exploitation rate estimates for ceded territory lakes only.
Exploitation rate is a key metric under the Voigt Decision and any proposed changes to creel surveys would need to ensure that the proper exploitation rate information can still be gathered.

What I've done here is calculate exploitation rates based on the full datasets as well as for 5 scenarios:

1.  all winter creel information removed

2.  only creel information from 'summer' May to August is used

3.  25% of weekday creel visits per month are removed

4.  50% of weekday creel visits per month are removed

5.  50% of weekend creel visits per month are removed.
    Creel visits are removed on a month by month basis by randomly removing creel visits to the lake that day

This random removal of days per month and day type should maintain the statistical integrity of the stratified random design that is a part of the creel survey.
For context, this was not how data was removed in a similar analysis done by Deroba et al. (2007) where they removed entire weeks (randomly selected) per month instead of randomly selecting days.
Their analysis also employed t-tests to analyze the resulting exploitation rates in each of their reduced datasets which relies on the assumption of normality and has been discussed above.

Exploitation rate ($u$) was calculated as:

$$ u=\frac{R}{M} $$ $$ R=(\frac{C}{T})H$$

Where $R$ is the estimated total number of marked fish harvested for a given creel strata and $M$ is the total number of walleye marked that spring during fyke net surveys.
The estimate of the number of marked fish harvested per strata ($R$) is obtained by applying the proportion of marked fish observed in the creel to the total harvest estimate for that strata ($H$).
$C$ is the actual number of marked fish observed in a given creel strata and $T$ is the total number of fish examined.

```{r u-rate-calcs}

lchar=get_fmdb_lakechar()
ctwiWBIC=lchar[lchar$trtystat==1 & !is.na(lchar$trtystat),]
ctwiWBIC.creel=ctwiWBIC$wbic[ctwiWBIC$wbic%in%cserv$wbic] # wbics in CTWI that have been creeled

tagdat=read.csv("tags_and_marks.csv") # marking data from fmdb since the package function URLs didn't seem to be working
tagdat$County=standardize_county_names(tagdat$County)
tagdat$Gear=standardize_string(tagdat$Gear)
tagdat$Waterbody.Name=standardize_waterbody_names(tagdat$Waterbody.Name)

fndat=tagdat%>% # this is just in case we care what type of mark is there, but I'm going to overwrite this for now and just get total number marked.
  filter(WBIC%in%ctwiWBIC.creel & Species.Code=="X22" & Gear=="fyke_net", !is.na(Mark.Given) & Mark.Given!="")%>%
  group_by(WBIC, Waterbody.Name,Survey.Year,Survey.Begin.Date,Survey.End.Date,Survey.Seq.No,Mark.Given)%>%
  summarise(nFish=sum(Number.of.Fish))

# now summing across all types of marks
fndat=tagdat%>% 
  filter(WBIC%in%ctwiWBIC.creel & Species.Code=="X22" & Gear=="fyke_net", !is.na(Mark.Given) & Mark.Given!="")%>%
  group_by(WBIC, Waterbody.Name,Survey.Year,Survey.Begin.Date,Survey.End.Date,Survey.Seq.No)%>%
  summarise(nFish=sum(Number.of.Fish))%>%
  mutate(Survey.Begin.Date=lubridate::mdy(Survey.Begin.Date),
         Survey.End.Date=lubridate::mdy(Survey.End.Date),
         begin.md=format(Survey.Begin.Date, "%m-%d"),
         end.md=format(Survey.End.Date, "%m-%d"))%>%
  filter(begin.md<"06-01") # getting rid of any marking surveys that may have taken place well after fishing season opened.

fdat=fndat%>% # now that I have only surveys I want, pooling across start and end dates within a year to create an anual total number marked for comparison to creel data
  group_by(WBIC, Waterbody.Name, Survey.Year)%>%
  summarise(nFN.marked=sum(nFish))

# table of marks found during creel surveys
crRecap=cfish.i%>%
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  mutate(month=month(sample.date),
         daytype=ifelse(wday(sample.date)%in%c(1,7),"weekend","weekday"))%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel
crRecap=crRecap%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# I think spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates=charv%>%
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst=crRecap%>%
  left_join(harvestEstimates)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp=markHarvEst%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked) # checked against the Nebagamon example provided by Tom and I'm really close with these estimates. It looks like my # of clipped fish counts are different from what Tom has in his exploitation DB. Not sure why that is but that's what appears to be causing the slight different in our exploitation rate estimates.

naExps=ang.exp[is.na(ang.exp$exp.rate),] # looking at the lake-years without data to see if there's a problem with my code or just missing data
# looks like it's either cases were no fish were marked in spring fyking for that creel survey or if for a specific strata no walleye were harvested or no marked walleye were harvested, either of those will throw an NA in the exploitation rate calculation

# there are 6 observation where the number of marks returned was higher than what was marked, I'm throwing these out.
ang.exp=ang.exp[!is.na(ang.exp$exp.rate) & ang.exp$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp=ang.exp%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

# now I have 'actual' exploitation rates

# next I want to make a few scenarios with 'reduced' data, a 'noWinter' scenario, 'May-August' scenario, and 3 % removals (0.25 weekdays, 0.5 weekdays, 0.5 weekend)

ttrExp=as.data.frame(matrix(NA,ncol=ncol(cfish.i)))
colnames(ttrExp)=colnames(cfish.i)

ifish=cfish.i%>% # making a data frame to add my groupings too so I can leave cfish.i alone
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  mutate(daytype=ifelse(wday(sample.date)%in%c(1,7),"weekday","weekend"),
         month=month(sample.date),
         season=ifelse(month%in%4:10,"openwater","winter"))

iharv=charv%>% # making a separate data frame to add groupings to so I can leave charv alone
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel)%>%
  mutate(season=ifelse(month%in%4:10,"openwater","winter"))

# now making a reduced dataframe for each scenario

###### NO WINTER ####

ifish.nw=ifish%>%
  filter(season=="openwater")%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

ifish.nw=ifish.nw%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.nw=iharv%>%
  filter(season=="openwater")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.nw=ifish.nw%>%
  left_join(harvestEstimates.nw)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.nw=markHarvEst.nw%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.nw=ang.exp.nw[!is.na(ang.exp.nw$exp.rate) & ang.exp.nw$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp.nw=ang.exp.nw%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

###### SUMMER ONLY ####

ifish.ma=ifish%>%
  filter(month%in%c(5:8))%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

ifish.ma=ifish.ma%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.ma=iharv%>%
  filter(month%in%c(5:8))%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.ma=ifish.ma%>%
  left_join(harvestEstimates.ma)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.ma=markHarvEst.ma%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.ma=ang.exp.ma[!is.na(ang.exp.ma$exp.rate) & ang.exp.ma$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp.ma=ang.exp.ma%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

# for each of these % reductions I'm revoving data based on visit fish seq no to simulate what would happen if that creel clear visit to the lake were removed.
###### 25% WEEKDAYS ####

rm.fish.seq.no=NA
rm.visit.fish.seq.no=NA
ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$daytype=="weekday",]
  ms=unique(full$month)
  if(nrow(full)>0){
    for(m in 1:length(ms)){ # month loop
      visits=unique(full$visit.fish.seq.no[full$month==ms[m]])
      visit.rm=visits[c(round(runif(round(0.25*length(visits)),min=1,max = length(visits))))]
      t.rm=full$fish.data.seq.no[full$visit.fish.seq.no%in%visit.rm]# fish seq nos to remove that associated with the vist seq nos selected for removal
      
      rm.visit.fish.seq.no=c(rm.visit.fish.seq.no,visit.rm) # visit fish seq nos to use to reduce the interview, count, and fish data needed to get harvest estimates
      rm.fish.seq.no=c(rm.fish.seq.no,t.rm) # vector of fish to remove from individual fish data
      
    }
  }
}

ifish.25wd=ifish[!(ifish$fish.data.seq.no%in%rm.fish.seq.no),] # reduced individual fish data
iint.25wd=cint%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced interview data
icou.25wd=ccou%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced count data
ifishAg.25wd=cfish%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced aggregate fish data

iharv.25wd=calc_creel_harvest(creel_count_data = icou.25wd,
                              creel_int_data = iint.25wd,
                              creel_fish_data = ifishAg.25wd) # harvest estimates from the reduced data

wd25=ifish.25wd%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

wd25=wd25%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.25wd=iharv.25wd%>%
  filter(species.code=="X22")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.25wd=wd25%>%
  left_join(harvestEstimates.25wd)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.25wd=markHarvEst.25wd%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.25wd=ang.exp.25wd[!is.na(ang.exp.25wd$exp.rate) & ang.exp.25wd$exp.rate<1.0,]
# creating survey-level exploitation rates
ang.exp.25wd=ang.exp.25wd%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))


###### 50% WEEKDAYS ####

rm.fish.seq.no=NA
rm.visit.fish.seq.no=NA
ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$daytype=="weekday",]
  ms=unique(full$month)
  if(nrow(full)>0){
    for(m in 1:length(ms)){ # month loop
      visits=unique(full$visit.fish.seq.no[full$month==ms[m]])
      visit.rm=visits[c(round(runif(round(0.50*length(visits)),min=1,max = length(visits))))]
      t.rm=full$fish.data.seq.no[full$visit.fish.seq.no%in%visit.rm]# fish seq nos to remove that associated with the vist seq nos selected for removal
      
      rm.visit.fish.seq.no=c(rm.visit.fish.seq.no,visit.rm) # visit fish seq nos to use to reduce the interview, count, and fish data needed to get harvest estimates
      rm.fish.seq.no=c(rm.fish.seq.no,t.rm) # vector of fish to remove from individual fish data
      
    }
  }
}

ifish.50wd=ifish[!(ifish$fish.data.seq.no%in%rm.fish.seq.no),] # reduced individual fish data
iint.50wd=cint%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced interview data
icou.50wd=ccou%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced count data
ifishAg.50wd=cfish%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced aggregate fish data

iharv.50wd=calc_creel_harvest(creel_count_data = icou.50wd,
                              creel_int_data = iint.50wd,
                              creel_fish_data = ifishAg.50wd) # harvest estimates from the reduced data

wd50=ifish.50wd%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

wd50=wd50%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.50wd=iharv.50wd%>%
  filter(species.code=="X22")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.50wd=wd50%>%
  left_join(harvestEstimates.50wd)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.50wd=markHarvEst.50wd%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.50wd=ang.exp.50wd[!is.na(ang.exp.50wd$exp.rate) & ang.exp.50wd$exp.rate<1.0,]

# creating survey-level exploitation rates
ang.exp.50wd=ang.exp.50wd%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

###### 50% WEEKENDS ####

rm.fish.seq.no=NA
rm.visit.fish.seq.no=NA
ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$daytype=="weekdend",]
  ms=unique(full$month)
  if(nrow(full)>0){
    for(m in 1:length(ms)){ # month loop
      visits=unique(full$visit.fish.seq.no[full$month==ms[m]])
      visit.rm=visits[c(round(runif(round(0.50*length(visits)),min=1,max = length(visits))))]
      t.rm=full$fish.data.seq.no[full$visit.fish.seq.no%in%visit.rm]# fish seq nos to remove that associated with the vist seq nos selected for removal
      
      rm.visit.fish.seq.no=c(rm.visit.fish.seq.no,visit.rm) # visit fish seq nos to use to reduce the interview, count, and fish data needed to get harvest estimates
      rm.fish.seq.no=c(rm.fish.seq.no,t.rm) # vector of fish to remove from individual fish data
      
    }
  }
}

ifish.50we=ifish[!(ifish$fish.data.seq.no%in%rm.fish.seq.no),] # reduced individual fish data
iint.50we=cint%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced interview data
icou.50we=ccou%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced count data
ifishAg.50we=cfish%>%
  filter(wbic%in%ctwiWBIC.creel & !(visit.fish.seq.no%in%rm.visit.fish.seq.no)) # reduced aggregate fish data

iharv.50we=calc_creel_harvest(creel_count_data = icou.50we,
                              creel_int_data = iint.50we,
                              creel_fish_data = ifishAg.50we) # harvest estimates from the reduced data

we50=ifish.50we%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month,daytype)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

we50=we50%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month,daytype)%>%
  summarise(n.marks.recapped=sum(nfish[!is.na(mark.found)]),
            totalFishExamined=sum(nfish))

# spp.harvest is the column that has the estimate of the harvest for that species in that strata
harvestEstimates.50we=iharv.50we%>%
  filter(species.code=="X22")%>%
  select(year,wbic,waterbody.name,survey.seq.no, month, daytype,spp.harvest,harvest,total.harvest,total.spp.harvest)

# combining harvest estimates with marked proportion estimate to get total number of marks harvested

markHarvEst.50we=we50%>%
  left_join(harvestEstimates.50we)%>%
  mutate(markHarvEstimate=(n.marks.recapped/totalFishExamined)*spp.harvest)

ang.exp.50we=markHarvEst.50we%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked)
# Throwing out NA u's and those >1 (NAs from lack of walleye harvest per strata or missing FN marks, >1's is 6 observations where # marks in creel exceeds # marks at large)
ang.exp.50we=ang.exp.50we[!is.na(ang.exp.50we$exp.rate) & ang.exp.50we$exp.rate<1.0,]

# creating survey-level exploitation rates
ang.exp.50we=ang.exp.50we%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

```

When calculating $u$ for each data set I first calculated $u$ by each month and day type then calculated an average $u$ for that creel survey based on `survey.seq.no` to produce one exploitation rate for that survey.
This was done to match the exploitation rate database which provides exploitation rates on a survey-level and not the strata-level.

> **Another important note, all exploitation rates = 0 were dropped before modeling for two reasons, 1) because the data is lognormally distributed taking the log of 0 isn't possible and the typical approach would be to add a small number to make it non-zero but this leads to skewed data in this case that would require modeling with a different data distribution, likely gamma or beta. This could be done but on short notice here I stuck with lognormal and dropped 0s. 2) If a creel survey produces a 0 now, chances are that as more data is removed from that survey it won't suddenly become non-zero, so the 0s are not likely to change under future creel designs. If anything more lakes may produce 0 exploitation rates if fewer samples are taken and the scenarios here show just that.**

```{r, monthly-rates,fig.width=8,fig.cap="Distribution of exploitation rates by month (panel a) for all years of CTWI creel data. Horizontal red line is overall mean exploitation rate. Black points are outliers. Panel B is the same information presented on a log-scale. In both plots exploitation rates of 0 are omitted."}


ang.exp=markHarvEst%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year","waterbody.name"="Waterbody.Name"))%>%
  mutate(exp.rate=markHarvEstimate/nFN.marked) # checked against the Nebagamon example provided by Tom and I'm really close with these estimates. It looks like my # of clipped fish counts are different from what Tom has in his exploitation DB. Not sure why that is but that's what appears to be causing the slight different in our exploitation rate estimates.

# there are observation where the number of marks returned was higher than what was marked, I'm throwing these out.
ang.exp=ang.exp[!is.na(ang.exp$exp.rate) & ang.exp$exp.rate<1.0,]

# plot of exploitation rate by month
ang.exp$plot.month=month(ang.exp$month,label = T)
p1=ggplot(ang.exp)+theme_classic()+
  geom_boxplot(aes(y=exp.rate, x=plot.month), fill="grey")+
  geom_hline(yintercept = mean(ang.exp$exp.rate, na.rm = T), color="red")+
  labs(y="Exploitation Rate", x="Month")

# plot of exploitation rate by month - logged
p2=ggplot(ang.exp)+theme_classic()+
  geom_boxplot(aes(y=log(exp.rate), x=plot.month), fill="grey")+
  geom_hline(yintercept = -4.449, color="red")+ # setting this manually since the 0 exp rates mess up the logging here
  labs(y="Log(Exploitation Rate)", x="Month")

ggarrange(p1,p2,nrow = 1,ncol=2,labels = 'auto')
```

```{r u-rate-plot, fig.cap="Distribution, on a log scale, of exploitation rates for the full data set plus 5 scenarios with reduced data. Actual = full data set, mayAug = May to August creel data only, noWinter = winter creel data removed, wd25 = 25% of weekday creel visits per month removed, wd50 = 50% of weekday creel visits per month removed, we50 = 50% of weekend creel visits per month removed."}

# creating survey-level exploitation rates for the actual data
ang.exp=ang.exp%>%
  group_by(survey.seq.no)%>%
  summarise(meanU=mean(exp.rate,na.rm = T),
            exp.rate.sd=sd(exp.rate,na.rm = T))

# combining actual and reduced dataframes into one big one for plotting

ttrExp=rbind(cbind(ang.exp,treat=rep("actual",nrow(ang.exp))),
             cbind(ang.exp.nw, treat=rep("noWinter",nrow(ang.exp.nw))),
             cbind(ang.exp.ma, treat=rep("mayAug",nrow(ang.exp.ma))),
             cbind(ang.exp.25wd, treat=rep("wd25",nrow(ang.exp.25wd))),
             cbind(ang.exp.50wd, treat=rep("wd50",nrow(ang.exp.50wd))),
             cbind(ang.exp.50we, treat=rep("we50",nrow(ang.exp.50we))))

colnames(ttrExp)=c("survey.seq.no","exp.rate","exp.rate.sd","treat")
ggplot(ttrExp)+theme_classic()+
  geom_density(aes(x=log(exp.rate), fill=treat),alpha=0.2)+
  labs(x="Log(Exploitation Rate)", y="Density", fill="Scenario")
# data is non normally distributed, exp.rate is continuous and greater than 0, consider gamma or lognormal distribution for modeling
```

#### Bayesian Model Fitting

In order to estimate parameters for a model of the exploitation rate distribution that produced what is represented in the creel data a Bayesian modeling approach was used.
This is because the exploitation rate data are not normally distributed and thus don't meet the assumptions of typical analyses based on normality.
Here I've chosen to model the data with a lognormal distribution, other potential options for the type of data are the gamma and beta distributions.
A Bayesian approach also allows for the inclusion of prior information about the system and does not rely on p-values (though some Bayesian p-values are presented later on) which are arbitrary in nature and can be manipulated via high sample sizes.
The Bayesian approach can be adopted to accommodate and prior distribution we feel is necessary be it lognormal, beta, or gamma.

Individual likelihoods were constructed for each reduced dataset and fit using the functions in the `BayesianTools` package in `R`.
These model fits were checked for convergence using visual inspection of trace plots, posterior distributions, and Gelman-Rubin Diagnostic tests to ensure that models had converged.
All models were fit using Differential Evolution Markov-Chain-Monte-Carlo algorithms run for 10,000 iterations with the first 5,000 discarded as burn-in.

```{r u-bt-model-fits, echo=T}
# one likelihood to estimate parms for using the 6 different treatments
# creating data frame to model with u rates that are NA removed, these are 6% of the data and are from years where creel happened but no fish were FN marked concurrently.
modDat=ttrExp[!is.na(ttrExp$exp.rate),]

#removing 0s since they can't be logged and if I were to make them a small number they would throw off the data and make it bimodal which would probably mean switching to a gamma or beta distribution to model the data. Could work, but I don't have time to mess with that right now. I'm going to operate under the assumption that if a survey give a 0 exploitation rate with the full survey, then us collecting less data isn't going to change that number. 
zeros.actual=sum(modDat$exp.rate[modDat$treat=="actual"]==0)
zeros.nw=sum(modDat$exp.rate[modDat$treat=="noWinter"]==0)
zeros.ma=sum(modDat$exp.rate[modDat$treat=="mayAug"]==0)
zeros.wd25=sum(modDat$exp.rate[modDat$treat=="wd25"]==0)
zeros.wd50=sum(modDat$exp.rate[modDat$treat=="wd50"]==0)
zeros.we50=sum(modDat$exp.rate[modDat$treat=="we50"]==0)

Ztab=data.frame(Scenario=c("Actual","No Winter","May-August","25% Weekday Reduction","50% Weekday Reduction","50% Weekend Reduction"),
                Zeros=c(zeros.actual,zeros.nw,zeros.ma,zeros.wd25,zeros.wd50,zeros.we50))
kable(Ztab,
      format='latex',
      caption="Table of the number of surveys with exploitation rate estimates equal to 0 for each data scenario.")

# removing 0s here as described in the text.
modDat=modDat[modDat$exp.rate!=0,]

#### ACTUAL ####
uLL.a=function(param){
  alpha=param[1]
  beta=param[2]

  us=rlnorm(nrow(modDat[modDat$treat=="actual",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="actual"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="actual"])),sd(log(modDat$exp.rate[modDat$treat=="actual"]))),
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.actual=createBayesianSetup(uLL.a, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.actual=runMCMC(bayesianSetup = setup.actual, sampler = "DEzs", settings = settings)


#### NW ####
uLL.nw=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="noWinter",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="noWinter"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="noWinter"])),sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.nw=createBayesianSetup(uLL.nw, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.nw=runMCMC(bayesianSetup = setup.nw, sampler = "DEzs", settings = settings)

#### MA ####

uLL.ma=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="mayAug",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="mayAug"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="mayAug"])),sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.ma=createBayesianSetup(uLL.ma, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.ma=runMCMC(bayesianSetup = setup.ma, sampler = "DEzs", settings = settings)

#### WD25 ####

uLL.wd25=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="wd25",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="wd25"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd25"])),sd(log(modDat$exp.rate[modDat$treat=="wd25"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.wd25=createBayesianSetup(uLL.wd25, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.25wd=runMCMC(bayesianSetup = setup.wd25, sampler = "DEzs", settings = settings)

#### WD50 ####
uLL.wd50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="wd50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="wd50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd50"])),sd(log(modDat$exp.rate[modDat$treat=="wd50"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.wd50=createBayesianSetup(uLL.wd50, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.50wd=runMCMC(bayesianSetup = setup.wd50, sampler = "DEzs", settings = settings)

#### WE50 ####
uLL.we50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="we50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="we50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="we50"])),sd(log(modDat$exp.rate[modDat$treat=="we50"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.we50=createBayesianSetup(uLL.we50, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
set.seed(10)
expR.50we=runMCMC(bayesianSetup = setup.we50, sampler = "DEzs", settings = settings)

```

#### Model Checking

Here model fit is being checked in several ways.
Initially a visual inspection comparing data simulated using median parameter values form the posterior compared to the observed data for that scenario.
A good model fit here is evident when the distribution of the observed and predicted data overlap nearly entirely.

Gelman-Rubin diagnostics were also used to assess convergence, while this doesn't assess model fit it does describe whether or not the MCMC algorithm converged on a solution or whether the parameter space has not been fully explored yet.
For this test values below 1.1 are considered 'converged' with a value of 1 being the lowest possible score.

Lastly, a Bayesian p-value was calculated for each model to further describe model fit to the data.
Bayesian p-values are based on the same idea as a frequentist p-value : *What is the probability of observing a more extreme test statistic than the one calculated from the observed data*.
To calculate a Bayesian p-value each set of parameter values in the MCMC chain is used to parameterize a log-normal distribution from which a set of 'new' data are drawn.
A test statistic is calculated for this new data and determined to either be more or less extreme than the test statistic is for observed data.
The proportion of these test statistics that are more extreme than the observed is then calculated.
If the model has done a good job of fitting the data then the parameter values should generally generate data that looks like the observed data and the test statistic should be equally likely to be more or less extreme than the observed value.
Thus, with Bayesian p-values a value of 0.5 is ideal as it means the model can generate data that matches the distribution of the observed data really well.
If this p-values was very low (\<.10) or very high (\>0.9) that would indicate that the model is not fitting the data well because it is unable to reproduce the data.

In this analysis two test statistics have been chosen to provide alternate, but not unrelated, measure of model fit.
These are the coefficient of variation and standard deviation.
Any test statistic of choice could be chosen as long as it can be justified for the objectives of the analysis at hand.

```{r u-data-reproduction, fig.width=8, fig.height=6 ,fig.cap="Distributions of the observed and model predicted data for each scenario. Model predicted data comes from lognormal distribution paramterized using the median parmater estimate of the model posterior."}

# looking to see if parm estimates produce data that visually at least looks like the observed data for that scenario
pars.a=getSample(expR.actual)
pars.nw=getSample(expR.nw)
pars.ma=getSample(expR.ma)
pars.wd25=getSample(expR.25wd)
pars.wd50=getSample(expR.50wd)
pars.we50=getSample(expR.50we)

aComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                         meanlog = median(pars.a[,1]),
                                                                         sdlog = median(pars.a[,2]))),
                    treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="actual"])),rep("pred",length(modDat$exp.rate[modDat$treat=="actual"]))))

nwComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="noWinter"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
                                                                    meanlog = median(pars.nw[,1]),
                                                                    sdlog = median(pars.nw[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="noWinter"])),rep("pred",length(modDat$exp.rate[modDat$treat=="noWinter"]))))

maComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="mayAug"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
                                                                    meanlog = median(pars.ma[,1]),
                                                                    sdlog = median(pars.ma[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="mayAug"])),rep("pred",length(modDat$exp.rate[modDat$treat=="mayAug"]))))

wd25Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="wd25"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
                                                                    meanlog = median(pars.wd25[,1]),
                                                                    sdlog = median(pars.wd25[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="wd25"])),rep("pred",length(modDat$exp.rate[modDat$treat=="wd25"]))))

wd50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="wd50"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
                                                                    meanlog = median(pars.wd50[,1]),
                                                                    sdlog = median(pars.wd50[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="wd50"])),rep("pred",length(modDat$exp.rate[modDat$treat=="wd50"]))))

we50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="we50"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
                                                                    meanlog = median(pars.we50[,1]),
                                                                    sdlog = median(pars.we50[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="we50"])),rep("pred",length(modDat$exp.rate[modDat$treat=="we50"]))))

a.p=ggplot(aComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "Actual", fill=element_blank())
nw.p=ggplot(nwComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "No Winter Data \n(April - October)", fill=element_blank())
ma.p=ggplot(maComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "May - August \nData Only", fill=element_blank())
wd25.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "25% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
wd50.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
we50.p=ggplot(we50Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekdend \nCreel Visits/Month \nRemoved", fill=element_blank())

ggarrange(a.p,nw.p,ma.p,wd25.p,wd50.p,we50.p, common.legend = T)
```

```{r u-gelman-rubin-diagnostics, echo=T}

gelmanDiagnostics(expR.actual) # converged
gelmanDiagnostics(expR.nw) # converged
gelmanDiagnostics(expR.ma) # converged
gelmanDiagnostics(expR.25wd) # converged
gelmanDiagnostics(expR.50wd) # converged
gelmanDiagnostics(expR.50we) # converged

```

```{r u-b-p-value}

# dataframe to hold output
bpValues=data.frame(scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                    coef.var.pval=NA,
                    sd.pval=NA)
#### Pb ACTUAL ####
pval.actual=data.frame(alpha=rep(NA,nrow(pars.a)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.a)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                 meanlog = pars.a[i,1],
                 sdlog = pars.a[i,2])
  pval.actual$alpha[i]=pars.a[i,1]
  pval.actual$beta[i]=pars.a[i,2]
  pval.actual$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.actual$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.actual$cvExceed=0
pval.actual$sdExceed=0

actual.cv=sd(log(modDat$exp.rate[modDat$treat=="actual"]))/mean(log(modDat$exp.rate[modDat$treat=="actual"]))
actual.sd=sd(log(modDat$exp.rate[modDat$treat=="actual"]))

pval.actual$cvExceed[pval.actual$cv>actual.cv]=1
pval.actual$sdExceed[pval.actual$sd>actual.sd]=1

bpValues$coef.var.pval[1]=sum(pval.actual$cvExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[1]=sum(pval.actual$sdExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb NO WINTER ####
pval.noWinter=data.frame(alpha=rep(NA,nrow(pars.nw)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.nw)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
                 meanlog = pars.nw[i,1],
                 sdlog = pars.nw[i,2])
  pval.noWinter$alpha[i]=pars.nw[i,1]
  pval.noWinter$beta[i]=pars.nw[i,2]
  pval.noWinter$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.noWinter$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.noWinter$cvExceed=0
pval.noWinter$sdExceed=0

noWinter.cv=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))/mean(log(modDat$exp.rate[modDat$treat=="noWinter"]))
noWinter.sd=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))

pval.noWinter$cvExceed[pval.noWinter$cv>noWinter.cv]=1
pval.noWinter$sdExceed[pval.noWinter$sd>noWinter.sd]=1

bpValues$coef.var.pval[2]=sum(pval.noWinter$cvExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[2]=sum(pval.noWinter$sdExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb MAYAUGUST ####
pval.mayAug=data.frame(alpha=rep(NA,nrow(pars.ma)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.ma)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
                 meanlog = pars.ma[i,1],
                 sdlog = pars.ma[i,2])
  pval.mayAug$alpha[i]=pars.ma[i,1]
  pval.mayAug$beta[i]=pars.ma[i,2]
  pval.mayAug$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.mayAug$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.mayAug$cvExceed=0
pval.mayAug$sdExceed=0

mayAug.cv=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))/mean(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.sd=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))

pval.mayAug$cvExceed[pval.mayAug$cv>mayAug.cv]=1
pval.mayAug$sdExceed[pval.mayAug$sd>mayAug.sd]=1

bpValues$coef.var.pval[3]=sum(pval.mayAug$cvExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[3]=sum(pval.mayAug$sdExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WD25 ####
pval.wd25=data.frame(alpha=rep(NA,nrow(pars.wd25)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.wd25)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
                 meanlog = pars.wd25[i,1],
                 sdlog = pars.wd25[i,2])
  pval.wd25$alpha[i]=pars.wd25[i,1]
  pval.wd25$beta[i]=pars.wd25[i,2]
  pval.wd25$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.wd25$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.wd25$cvExceed=0
pval.wd25$sdExceed=0

wd25.cv=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))/mean(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.sd=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))

pval.wd25$cvExceed[pval.wd25$cv>wd25.cv]=1
pval.wd25$sdExceed[pval.wd25$sd>wd25.sd]=1

bpValues$coef.var.pval[4]=sum(pval.wd25$cvExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[4]=sum(pval.wd25$sdExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WD50 ####
pval.wd50=data.frame(alpha=rep(NA,nrow(pars.wd50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.wd50)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
                 meanlog = pars.wd50[i,1],
                 sdlog = pars.wd50[i,2])
  pval.wd50$alpha[i]=pars.wd50[i,1]
  pval.wd50$beta[i]=pars.wd50[i,2]
  pval.wd50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.wd50$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.wd50$cvExceed=0
pval.wd50$sdExceed=0

wd50.cv=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))/mean(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.sd=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))

pval.wd50$cvExceed[pval.wd50$cv>wd50.cv]=1
pval.wd50$sdExceed[pval.wd50$sd>wd50.sd]=1

bpValues$coef.var.pval[5]=sum(pval.wd50$cvExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[5]=sum(pval.wd50$sdExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WE50 ####
pval.we50=data.frame(alpha=rep(NA,nrow(pars.we50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.we50)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
                 meanlog = pars.we50[i,1],
                 sdlog = pars.we50[i,2])
  pval.we50$alpha[i]=pars.we50[i,1]
  pval.we50$beta[i]=pars.we50[i,2]
  pval.we50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.we50$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.we50$cvExceed=0
pval.we50$sdExceed=0

we50.cv=sd(log(modDat$exp.rate[modDat$treat=="we50"]))/mean(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.sd=sd(log(modDat$exp.rate[modDat$treat=="we50"]))

pval.we50$cvExceed[pval.we50$cv>we50.cv]=1
pval.we50$sdExceed[pval.we50$sd>we50.sd]=1

bpValues$coef.var.pval[6]=sum(pval.we50$cvExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[6]=sum(pval.we50$sdExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

kable(bpValues, digits=3, caption="Bayesian p-values for two variance metrics and each modeled data set.")
```

#### Inference

Having established that the models are fitting the data well, some inference can be gained as to whether or not the reductions in creel effort proposed here would result in a meaningful change in the exploitation rates estimated from the data.

```{r u-data-comparison-to-actual, fig.width=8,fig.height=6,fig.cap="Comparison of the distribution of actual exploitation rates calculated from creel data and exploitation rates calculated from simulated creel data for each data reduction scenario and the resulting median parameter values for their respective model fits."}

aComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                    meanlog = median(pars.a[,1]),
                                                                    sdlog = median(pars.a[,2]))),
                 treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

nwComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                       meanlog = median(pars.nw[,1]),
                                                                       sdlog = median(pars.nw[,2]))),
                  treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

maComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.ma[,1]),
                                                                     sdlog = median(pars.ma[,2]))),
                  treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

wd25Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.wd25[,1]),
                                                                     sdlog = median(pars.wd25[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

wd50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.wd50[,1]),
                                                                     sdlog = median(pars.wd50[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

we50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.we50[,1]),
                                                                     sdlog = median(pars.we50[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

a.p=ggplot(aComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "Actual", fill=element_blank())
nw.p=ggplot(nwComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "No Winter Data \n(April - October)", fill=element_blank())
ma.p=ggplot(maComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "May - August \nData Only", fill=element_blank())
wd25.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "25% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
wd50.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekday \nCreel Visits/Month \nRemoved", fill=element_blank())
we50.p=ggplot(we50Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekdend \nCreel Visits/Month \nRemoved", fill=element_blank())

ggarrange(a.p,nw.p,ma.p,wd25.p,wd50.p,we50.p, common.legend = T)
```

A Bayesian p-value can be employed here too.
A test between the cv and sd of the actual data and the cv and sd of the simulated reduction data can describe whether the model of the reduced data is able to approximate the actual data or not.
Successful approximations are p-values close to 0.5 with reduced fit as values increase or decrease beyond 0.5.
Generally, the rule of thumb is that p-values $<0.10$ or $>0.90$ signal unacceptable fits.

The results of these test suggest that when using coefficient of variation as the variance metric to assess model fit and the $<0.10$ or $>0.90$ rule that the no winter and may-august data reduction models both approximate the actual data ok, but not as well as the 3 percentage reductions whose Bayesian p-values suggest they produce exploitation rate distributions that are very similar to the actual data.
When using standard deviation as the metric for comparison then all models approximate the actual data pretty well as the model fit to the actual data.
The 'No Winter' scenario and two 50% reduction scenarios are starting to creep higher but still not to a point where they're different from the actual data.
The reason for this discrepancy between metrics is likely that the coefficient of variation is standardized based on the mean of the distribution while the standard deviation is not and is a raw variance metric.
In this case using the coefficient of variation results seems like the most pragmatic choice, it is also supported the by multi-panel plot comparing the actual exploitation rates to the distributions of simulated data from each scenario.
That would appear to be two lines of evidence pointing in the same direction.

```{r b-p-value-comparison, warning=F, message=F}
## p-value tests comparing the scenarios to actual to show that some scenarios approximate the actual quite well.

bpval.comp=data.frame(scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                      coef.var.pval=NA,
                      sd.pval=NA)
pval.actual$cvComp=0
pval.actual$sdComp=0
pval.actual$cvComp[pval.actual$cv>actual.cv]=1
pval.actual$sdComp[pval.actual$sd>actual.sd]=1

pval.noWinter$cvComp=0
pval.noWinter$sdComp=0
pval.noWinter$cvComp[pval.noWinter$cv>actual.cv]=1
pval.noWinter$sdComp[pval.noWinter$sd>actual.sd]=1

pval.mayAug$cvComp=0
pval.mayAug$sdComp=0
pval.mayAug$cvComp[pval.mayAug$cv>actual.cv]=1
pval.mayAug$sdComp[pval.mayAug$sd>actual.sd]=1

pval.wd25$cvComp=0
pval.wd25$sdComp=0
pval.wd25$cvComp[pval.wd25$cv>actual.cv]=1
pval.wd25$sdComp[pval.wd25$sd>actual.sd]=1

pval.wd50$cvComp=0
pval.wd50$sdComp=0
pval.wd50$cvComp[pval.wd50$cv>actual.cv]=1
pval.wd50$sdComp[pval.wd50$sd>actual.sd]=1

pval.we50$cvComp=0
pval.we50$sdComp=0
pval.we50$cvComp[pval.we50$cv>actual.cv]=1
pval.we50$sdComp[pval.we50$sd>actual.sd]=1

bpval.comp$coef.var.pval=c(sum(pval.actual$cvComp)/nrow(pval.actual),
                           sum(pval.noWinter$cvComp)/nrow(pval.noWinter),
                           sum(pval.mayAug$cvComp)/nrow(pval.mayAug),
                           sum(pval.wd25$cvComp)/nrow(pval.wd25),
                           sum(pval.wd50$cvComp)/nrow(pval.wd50),
                           sum(pval.we50$cvComp)/nrow(pval.we50))

bpval.comp$sd.pval=c(sum(pval.actual$sdComp)/nrow(pval.actual),
                           sum(pval.noWinter$sdComp)/nrow(pval.noWinter),
                           sum(pval.mayAug$sdComp)/nrow(pval.mayAug),
                           sum(pval.wd25$sdComp)/nrow(pval.wd25),
                           sum(pval.wd50$sdComp)/nrow(pval.wd50),
                           sum(pval.we50$sdComp)/nrow(pval.we50))
kable(bpval.comp, digits=3, caption="Bayesian p-values for the comparison between the actual data and the scenario-specific model. This test is asking whether the scenario-specific model can approximate the actual data. When true this signals no effect of the data reduction for that scenario on the resulting exploitation rates.")
```
