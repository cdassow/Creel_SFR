---
title: "Creel Titration"
author: "Colin Dassow"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: TRUE
    toc_depth: 4
    number_sections: TRUE
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, warning=F, message=F}
knitr::opts_chunk$set(echo = F, cache = T)

rm(list=ls())
library(wdnr.fmdb)
library(tidyverse)
library(ggpubr)
library(lubridate)
library(knitr)
library(BayesianTools)

setwd("C:/Users/dassocju/OneDrive - State of Wisconsin/Office_of_Applied_Science/Creel_SFR")

#reading in objects that take a long time to produce to improve rendering speed of this document
call=readRDS("creelDataSet_all.RData")
# unlist, etc. to get back to individual dfs
cserv=call[[1]]
cvis=call[[2]]
ccou=call[[3]]
cint=call[[4]]
cfish=call[[5]]
cfish.i=call[[6]]

ttrList=readRDS("titrationOutput.RData")

ttrEff=ttrList[[1]];ttrEff=ttrEff[!is.na(ttrEff$wbic),]
ttrHarv=ttrList[[2]];ttrHarv=ttrHarv[!is.na(ttrHarv$wbic),]
ttrHarvR=ttrList[[3]];ttrHarvR=ttrHarvR[!is.na(ttrHarvR),]

```

# WDNR Creel Titration

## **Purpose**

This document is meant to serve as a record of analyses carried out with the objective of titrating DNR creel data.
The term 'titration' is used here to describe the main question this analysis seeks to answer:

> **How much less creel data could be collected, on an individual survey basis, while still maintaining effort, catch, and harvest estimates within an accuracy tolerance limit of the estimates produced via the current creel survey method?**

The following analyses have different specific goals and methods but generally all of them are seeking to answer this main question posed above.
The data informing these analyses has all been pulled from the Fisheries Management Information System (FMIS) using the `wdnr.fmdb` `R` package built to interface with this database.
All analyses were conducted in `R` using version `4.2.2`.

### Current Creel Data as Baseline

DNR has amassed a large amount of inland creel information over many decades (1984-2023) across many waterbodies (n=314).
Using this data and functionality of `wdnr.fmdb` it is possible to calculate effort, catch, harvest, and harvest rate for multiple species for each unique creel survey conducted.
This information can be further grouped by month, season, day type, etc. to provide insight into important temporal patterns in the fisheries metrics of interest.
These calculations are accomplished fairly easily using the following code:

```{r creel-data-retreival, eval=FALSE, echo=T}

# reading in DNR creel data

cserv=get_creel_surveys()
cvis=get_creel_visits()
ccou=get_creel_counts()
cint=get_creel_int_party()
cfish=get_creel_fish_data()

```

```{r baseline-fisheries-metrics, cache=T, echo=T, warning=F, message=F}

ceff=calc_creel_effort(creel_count_data=ccou, creel_int_data=cint) 

charv=calc_creel_harvest(creel_count_data = ccou, creel_int_data = cint,creel_fish_data = cfish) 

charvR=calc_creel_harvest_rates(creel_fish_data=cfish)

```

### Titrations a.k.a. Power Analyses

#### Randomized Data Removal

This analysis is structured to randomly select and remove data from each individual creel survey and re-calculate the important fisheries metrics above to compare to the baseline values.
Throughout this analysis two types of datasets will be referred to the 'observed' data set which is the full creel dataset as it is represented in FMIS.
Second, a 'reduced' data set will be referred to and this is the data set with some amount of data removed so that it contains less than the 'observed' data.

A wide range of reductions was systematically carried out on the observed data using the code below.
The range of data reductions spanned from 10% to 80% removal of the data in a particular creel survey.
These removals were done for the creel count data, creel interview data, and creel fish data and were linked so that if a creel interview data point was selected for removal the associated fish data was also removed.
The intention here is to prevent extra noise in the analysis that would be introduced by mismatched removals of data from the different data frames that link together (count, interview, and fish data.) Removal in this analysis was randomized across the survey, in other words not stratified by month, season, day type.
Analyses presented later will focus on specific groups like seasons to see the effect of removing data within a season on the overall fisheries metrics.

```{r randomized-removal, eval=F, echo=T}


#empty dfs to hold output
ttrEff=as.data.frame(matrix(NA,nrow=1,ncol=ncol(ceff)))
colnames(ttrEff)=colnames(ceff)
ttrEff$pReduc=NA

ttrHarv=as.data.frame(matrix(NA,nrow=1,ncol = ncol(charv)))
colnames(ttrHarv)=colnames(charv)
ttrHarv$pReduc=NA

ttrHarvR=as.data.frame(matrix(NA,nrow = 1,ncol = ncol(charvR)))
colnames(ttrHarvR)=colnames(charvR)
ttrHarvR$pReduc=NA

pRemove=seq(0.1,0.8, by=0.1) # percent of data to remove at each loop
survs=unique(ccou$survey.seq.no) # unique surveys to remove a percentage of data from (corresponds to a unique lake-year that was creeled)

set.seed(2) # setting seed to make results exactly reproducible included the 'random' selection of data to remove.
for(r in 1:length(pRemove)){
  for(i in 1:length(survs)){

    # reducing data size
    full=ccou[ccou$survey.seq.no==survs[i],]
    reduc=full[-c(runif(n=round(nrow(full)*pRemove[r]),min = 1,max=nrow(full))),]

    full.int=cint[cint$survey.seq.no==survs[i],]
    reduc.int=full.int[-c(runif(n=round(nrow(full.int)*pRemove[r]),min = 1,max = nrow(full.int))),]
    
    full.harv=cfish[cfish$survey.seq.no==survs[i],]
    # removing all fish data associated with the removed interview data
    reduc.harv=full.harv[full.harv$int.party.seq.no%in%reduc.int$int.party.seq.no,] 

    #calculating creel stats
    teff=calc_creel_effort(creel_count_data = reduc,creel_int_data = reduc.int)
    tharv=calc_creel_harvest(creel_count_data = reduc,creel_int_data = reduc.int,creel_fish_data = reduc.harv)
    tharvR=calc_creel_harvest_rates(reduc.harv)

    # adding data to output dataframes
    addEff=cbind(teff,pReduc=rep(pRemove[r],nrow(teff)));colnames(addEff)=colnames(ttrEff)
    addHarv=cbind(tharv,pReduc=rep(pRemove[r],nrow(tharv)));colnames(addHarv)=colnames(ttrHarv)
    addHarvR=cbind(tharvR,pReduc=rep(pRemove[r],nrow(tharvR)));colnames(addHarvR)=colnames(ttrHarvR)

    ttrEff=rbind(ttrEff,addEff)
    ttrHarv=rbind(ttrHarv,addHarv)
    ttrHarvR=rbind(ttrHarvR,addHarvR)

  }
}


```

As an example here is the output of the effort data removal:

```{r randomized-removal-output, warning=F, message=F}
ttrEff=ttrEff[!is.na(ttrEff$wbic),]
kable(rbind(head(ttrEff),tail(ttrEff)), digits=2)

```

On an individual creel survey level, this is the type of information available from the data removal loop performed above.

```{r survey-level-removal1, warning=F, message=F, fig.cap="Monthly total effort estimate for Okauchee Lake, Waukesha County creel survey conduced 2006-2007. Error bars represent +/- 1 standard deviation. Solid horizontal line represents the 'observed' effor estimate. Colors represent different day types surveyed." }

wbic=ttrEff[ttrEff$wbic==ttrEff$wbic[2] & !is.na(ttrEff$month),]
actual=ceff[ceff$wbic==unique(wbic$wbic) & !is.na(ceff$month),]

wbic$month=factor(wbic$month,levels = sort(unique(wbic$month)),labels = sort(unique(wbic$month)))
ggplot(wbic[wbic$year==2006,])+theme_classic()+
  geom_pointrange(aes(x=pReduc,y=total.effort,ymax=total.effort+total.effort.sd,ymin=total.effort-total.effort.sd, color=daytype))+
  geom_line(aes(x=pReduc,y=total.effort,color=daytype))+
  geom_hline(data=actual,aes(yintercept = total.effort, color=daytype))+
  scale_color_viridis_d()+
  facet_wrap(~month,scales = "free_y")+
  labs(x="% of Data Removed",y="Total Monthly Effort +/- 1 SD")


```

```{r survey-level-removal2, warning=F, message=F, fig.cap="Monthly total effort estimate for Okauchee Lake, Waukesha County creel survey conduced 2006-2007. Total bar height is the total effort estimate for that day type. Colors represent different months and their contribution to the total annual estimate." }

ggplot(wbic)+theme_classic()+
  geom_col(aes(y=total.effort,x=daytype,fill=month),position = "stack")+
  scale_fill_viridis_d()+
  labs(x="",y="Total Effort",fill="Month")

```

With an understanding of the type of information available for each survey a summary loop was run to identify and record the point at which the percent reduction in the observed data produced an effort estimate that was \> 1 standard deviation away from the observed effort estimate.
These threshold where accuracy is lost are then summarized across all surveys for each month and day type to summarize how much creel information could generally be omitted while still providing a comparable estimate of effort (repeat analyses for catch, harvest, and harvest rate were also run and will be summarized separately in Appendix A).

```{r threshold-summary-loop, echo=T, warning=F, message=F}

survs=unique(ttrEff$survey.seq.no[!is.na(ttrEff$survey.seq.no)])
thresh=data.frame(wbic=NA,
                  year=NA,
                  month=NA,
                  daytype=NA,
                  survey.seq.no=NA,
                  actualEff=NA,
                  reducedEff=NA,
                  reducedEff.sd=NA,
                  pReduc=NA)

for(i in 1:length(survs)){
  tdat=ttrEff[ttrEff$survey.seq.no==survs[i],]
  adat=ceff[ceff$survey.seq.no==survs[i],]
  for(y in 1:length(unique(adat$year))){
    for(m in 1:length(unique(adat$month))){
      for(d in 1:length(unique(adat$daytype))){
        z=tdat[tdat$year==unique(adat$year)[y] & tdat$month==unique(adat$month)[m] & tdat$daytype==unique(adat$daytype)[d],]
        b=adat[adat$year==unique(adat$year)[y] & adat$month==unique(adat$month)[m] & adat$daytype==unique(adat$daytype)[d],]
        
        if(any(unique(z$total.effort)!=0)){
          pR=max(unique(z$pReduc)[which(!(b$total.effort<(z$total.effort+z$total.effort.sd) & b$total.effort>(z$total.effort-z$total.effort.sd)))])
            if(is.infinite(pR)){
            addDat=c(wbic=unique(z$wbic),
                     year=unique(z$year),
                     month=unique(z$month),
                     daytype=unique(z$daytype),
                     survey.seq.no=survs[i],
                     actualEff=b$total.effort,
                     reducedEff=NA,
                     reducedEff.sd=NA,
                     pReduc=NA)
            
            thresh=rbind(thresh,addDat)
          }else{
          addDat=c(wbic=unique(z$wbic),
                   year=unique(z$year),
                   month=unique(z$month),
                   daytype=unique(z$daytype),
                   survey.seq.no=survs[i],
                   actualEff=b$total.effort,
                   reducedEff=z$total.effort[z$pReduc==pR],
                   reducedEff.sd=z$total.effort.sd[z$pReduc==pR],
                   pReduc=pR)
          
          thresh=rbind(thresh,addDat)
          }
        }
      }
    } 
  }
}
thresh=thresh[!is.na(thresh$wbic),]

```

```{r threshold-monthly-summary1, warning=F, message=F, fig.cap="Distribution of the % reduction where accuracy threshold was exceeded. X-axis values of 'NA' represent scenarios where accuracy was never lost."}

ggplot(thresh)+theme_classic()+
  geom_bar(aes(x=pReduc,fill=daytype),position = "dodge")+
  scale_fill_viridis_d()+
  facet_wrap(~month, scales = "free")+
  labs(x="Proportion of Data Removed", y="Count (no. creel surveys)",fill= "Day Type")+
  theme(legend.position = "bottom")

```

```{r threshold-montly-summary2, warning=F, message=F, fig.cap="Distribution of average % reduction where accuracy was lost per survey. Calculated as the mean % reduction where accuracy was lost across all months of the survey. Colors represent different day types."}

# grouping across month

annualThresh=thresh%>%
  group_by(wbic,year,daytype, survey.seq.no)%>%
  summarise(actualEff=sum(as.numeric(actualEff)),
            reducedEff=sum(as.numeric(reducedEff),na.rm = T),
            reducedEff.sd=sd(as.numeric(reducedEff),na.rm = T),
            meanPR=mean(as.numeric(pReduc),na.rm=T))

ggplot(annualThresh)+theme_classic()+
  geom_density(aes(x=meanPR,fill=daytype),alpha=0.2)+
  scale_fill_viridis_d()+
  labs(x="Mean % Reduction", fill="Day Type", y="Density")

```

#### Winter Creel Removal

Here creel data during the winter months has been removed and comparisons will be made between 'full' data sets including winter data and 'noWinter' data sets where winter data has been excluded.
Winter months are defined here as January to March, November, and December.
The plots associated with this analysis visualize the distribution of effort, catch, harvest, and harvest rate calculated from the full and noWinter data sets.
Statistical tests are performed to assess whether the distribution of data in the two groups significantly differ or not.
A significant difference in the distribution would signal that the removal of the winter data constitutes a meaningful subtraction effecting the overall effort, catch, harvest, or harvest rate that survey would have produced.
Non-significant differences signal that the removal of the winter creel data does not have an impact on the important fishery metric estimates.

This analysis was carried using a Bayesian method to better account for the non-normal distributions of the creel data.
The full and noWinter data sets where modeled separately to estimate the parameters of the non-normal distribution fitting the data.
If the central tendency parameter estimates of the distribution fit to each data set have overlapping confidence intervals to indicate that the removal of the winter creel data does not effect the fishery metric in question.

```{r excluding-winter, echo=T, warning=F, message=F}

nw.cserv=cserv # nothing to change here, survey-level info but renamed for completeness

# keeping only the non-winter months 4:10
nw.cvis=cvis%>%
  mutate(month=month(sample.date))%>%
  filter(month%in%c(4:10))
nw.ccou=ccou%>%
  mutate(month=month(sample.date))%>%
  filter(month%in%c(4:10))
nw.cint=cint%>%
  mutate(month=month(sample.date))%>%
  filter(month%in%c(4:10))
nw.cfish=cfish%>%
  mutate(month=month(sample.date))%>%
  filter(month%in%c(4:10))

# calculating important fishery metrics
nw.ceff=calc_creel_effort(creel_count_data=nw.ccou,creel_int_data=nw.cint) 

nw.charv=calc_creel_harvest(creel_count_data = nw.ccou,creel_int_data = nw.cint,creel_fish_data = nw.cfish) 

nw.charvR=calc_creel_harvest_rates(creel_fish_data=nw.cfish)

# combining fishery metrics calculated with and without winter into one dataframe for analysis
effComp=rbind(cbind(ceff,treat=rep("full",nrow(ceff))),
              cbind(nw.ceff,treat=rep("noWinter",nrow(nw.ceff))))

```

##### Effort

Creel effort is log-normally distributed but there are a small number of 0 effort estimates that must be dealt with because this analysis will require the use of logs and `is.infinite(log(0))==TRUE`.
There are a total of 1,514 estimates of effort from the creel data (not species specific).
24 of these estimates are 0, amounting to 1.5% of the data.
For simplicity here and to improve model fit those 0 estimates have been excluded.

```{r bt-effort-analysis, echo=T, warning=F, message=F, eval=F}

library(BayesianTools)

# annual effort totals
yrSum.EF=effComp%>%
  group_by(wbic, survey.seq.no,treat)%>%
  summarise(sumEff=sum(total.effort))

# removing 0 effort surveys, only 1.5% of data 
yrSum.EF=yrSum.EF[yrSum.EF$sumEff!=0,]

#creating a truncated normal prior because the normal distribution is the conjugate of the lognormal and thus the meanlog of the lognormal should be approximately normally distributed.
prior=createTruncatedNormalPrior(mean=c(mean(log(yrSum.EF$sumEff)),sd(log(yrSum.EF$sumEff))), 
                                 sd=c(1,1),
                                 lower = c(5,0),
                                 upper = c(15,5))
# likelihood functions for the full and noWinter data sets
effLL.full=function(param){
  alpha=param[1]
  beta=param[2]

  points=rlnorm(sum(yrSum.EF$treat=="full"), meanlog = alpha, sdlog = beta)
  ll=dlnorm(yrSum.EF$sumEff[yrSum.EF$treat=="full"], meanlog = mean(log(points)), sdlog = sd(log(yrSum.EF$sumEff[yrSum.EF$treat=="full"])), log = T)
  return(sum(ll))
}

effLL.nw=function(param){
  alpha=param[1]
  beta=param[2]
  
  points=rlnorm(sum(yrSum.EF$treat=="noWinter"), meanlog = alpha, sdlog = beta)
  ll=dlnorm(yrSum.EF$sumEff[yrSum.EF$treat=="noWinter"], meanlog = mean(log(points)), sdlog = sd(log(yrSum.EF$sumEff[yrSum.EF$treat=="noWinter"])), log = T)
  return(sum(ll))
}

# setting up models to run
setup.full=createBayesianSetup(effLL.full, prior = prior)
setup.nw=createBayesianSetup(effLL.nw, prior = prior)

# applying the same run setting to both models
settings=list(iterations=10000, nrChains=3, message=F)

# running the models
out.full=runMCMC(bayesianSetup = setup.full, sampler = "DEzs", settings = settings)
out.nw=runMCMC(bayesianSetup = setup.nw, sampler = "DEzs", settings = settings)


```

```{r bayesEffort-readIn, message=F, warning=F}
datIn=readRDS("bayesEffort.RData")

out.full=datIn[[1]]
out.nw=datIn[[2]]

```

```{r bt-effort-parmOverlaps, warning=F, message=F, echo=T}

# showing that the parameters and their CIs overlap
full=getSample(out.full)
nw=getSample(out.nw)

parComp.alpha=data.frame(dataset=c("full","noWinter"),
                    lower.95.ci=c(sort(full[,1])[length(full[,1])*0.025],
                                  sort(nw[,1])[length(nw[,1])*0.025]),
                    median=c(median(full[,1]), median(nw[,1])),
                    upper.95.ci=c(sort(full[,1])[length(full[,1])*0.975],
                                  sort(nw[,1])[length(nw[,1])*0.975]))

parComp.beta=data.frame(dataset=c("full","noWinter"),
                         lower.95.ci=c(sort(full[,2])[length(full[,2])*0.025],
                                       sort(nw[,2])[length(nw[,2])*0.025]),
                         median=c(median(full[,2]), median(nw[,2])),
                         upper.95.ci=c(sort(full[,2])[length(full[,2])*0.975],
                                       sort(nw[,2])[length(nw[,2])*0.975]))

kable(parComp.alpha, digits=3, caption="Median, and 95% CI of parameter estimate for lognormal shape parameter alpha (analgous to mean on a log-scale).")

kable(parComp.beta, digits=3, caption="Median, and 95% CI of parameter estimate for lognormal shape parameter beta (analgous to standard deviation on a log-scale).")

```

```{r bt-effort-simBack1, warning=F, message=F, echo=T, fig.cap="Using the parameters estimated from the model of the full data to simulate some hypothetical effort data to show that the distribution of the hypothetical data matches that of the observed data. Basically a sanity check to know that the model is producing what it should be."}
pars=getSample(out.full)

# annual effort totals
yrSum.EF=effComp%>%
  group_by(wbic, survey.seq.no,treat)%>%
  summarise(sumEff=sum(total.effort))

# removing 0 effort surveys, only 1.5% of data and we really should be surveying wbics that get effort to begin with so having that 0 info in here isn't that useful other than to say they were a waste of a survey.
yrSum.EF=yrSum.EF[yrSum.EF$sumEff!=0,]
tparms=c(mean(log(yrSum.EF$sumEff[yrSum.EF$treat=="full"])), sd(log(yrSum.EF$sumEff[yrSum.EF$treat=="full"])))

fullComp=data.frame(eff=c(yrSum.EF$sumEff[yrSum.EF$treat=="full"],rlnorm(n=length(yrSum.EF$sumEff[yrSum.EF$treat=="full"]),
                                                                         meanlog = tparms[1],
                                                                         sdlog = tparms[2])),
                    treat=c(rep("observed",length(yrSum.EF$sumEff[yrSum.EF$treat=="full"])),rep("pred",length(yrSum.EF$sumEff[yrSum.EF$treat=="full"]))))

ggplot(fullComp)+theme_classic()+
  geom_density(aes(x=log(eff),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+
  labs(x="Log of Effort Estimate", y="Density", fill="Data type")


```

```{r bt-effort-simBack2, warning=F, message=F, echo=T, fig.cap="Using the parameters estimated from the model of the noWinter data to simulate some hypothetical effort data to show that the distribution of the hypothetical data matches that of the observed data. Basically a sanity check to know that the model is producing what it should be."}
pars=getSample(out.nw)

nwComp=data.frame(eff=c(yrSum.EF$sumEff[yrSum.EF$treat=="noWinter"],rlnorm(n=length(yrSum.EF$sumEff[yrSum.EF$treat=="noWinter"]),
                                                                         meanlog = tparms[1],
                                                                         sdlog = tparms[2])),
                    treat=c(rep("observed",length(yrSum.EF$sumEff[yrSum.EF$treat=="noWinter"])),rep("pred",length(yrSum.EF$sumEff[yrSum.EF$treat=="noWinter"]))))

ggplot(nwComp)+theme_classic()+
  geom_density(aes(x=log(eff),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+
  labs(x="Log of Effort Estimate", y="Density", fill="Data type")



```

The various analyses presented here all point to the distribution of the effort calculations for full and noWinter data being the same.
In other words, removing the winter creel data has no statistically significant effect on the effort estimates we get from creel surveys.

The analysis presented here was repeated again for 14 fish species along measures of catch, harvest, and harvest rate.
The full detail of those analyses are provided in Appendix A. Important summary information are provided here.

##### Catch

Catch is poisson distributed since it's a non-negative integer.
The creel data provides roughly 6,700 observations of total catch per creel survey (May - following March) across 13 species (brown_trout was dropped from this analysis because of lack of data).

The Bayesian analysis found... **INCOMPLETE** SHIFTED FOCUS TO EXPLOITATION RATE

In addition to the Bayesian analysis presented above a Kolmogorov-Smirnow test was also conducted on the vectors of catch estimates for the full and noWinter datasets to see if they differed significantly.
The Kolmogorov-Smirnov test appears to be the best option for a frequentist-type analysis as it allows for non-normally distributed data but does assume observations are independent of each other which is not the case for this data since the catch of black crappie in one month may influence catch in a subsequent month since those fish may no longer be available.
Our creel data also amounts to multiple measurements from the same test subject (i.e. lake), but this test appears to be the best we can do for the time being.
A significant p-value here would mean that the distribution of mean catch values for the data set with no winter creel information is significantly different from the distribution of mean catch values for the full dataset.

```{r ks-test-catch, warning=F, message=F, echo=T}

# comparison of effort and harvest calculates with and without winter months data
harvComp=rbind(cbind(charv,treat=rep("full",nrow(charv))),
                cbind(nw.charv,treat=rep("noWinter",nrow(nw.charv))))

# annual harvest totals
yrSum=harvComp%>%
  group_by(wbic, survey.seq.no,treat,species)%>%
  summarise(sumCatch=sum(total.spp.catch),
            sumHarv=sum(total.spp.harvest),
            meanHarvR=mean(spp.harvest.rate))%>%
  filter(species%in%c("black_crappie","bluegill","largemouth_bass","muskellunge","northern_pike","pumpkinseed","rock_bass","smallmouth_bass","walleye","yellow_perch","brook_trout","brown_trout","lake_trout","rainbow_trout")) # pulling out some relevant species

trts=unique(yrSum$treat)
spps=unique(yrSum$species)
out=data.frame(species=spps,
               catch.sigDiff=NA,
               harv.sigDiff=NA,
               harvR.sigDiff=NA)

for(s in 1:length(spps)){
    testC=ks.test(yrSum$sumCatch[yrSum$treat=="full" & yrSum$species==spps[s]],
                  yrSum$sumCatch[yrSum$treat=="noWinter" & yrSum$species==spps[s]], 
                  alternative = "two.sided")
    testH=ks.test(yrSum$sumHarv[yrSum$treat=="full" & yrSum$species==spps[s]],
                  yrSum$sumHarv[yrSum$treat=="noWinter" & yrSum$species==spps[s]], 
                  alternative = "two.sided")
    testHR=ks.test(yrSum$meanHarvR[yrSum$treat=="full" & yrSum$species==spps[s]],
                  yrSum$meanHarvR[yrSum$treat=="noWinter" & yrSum$species==spps[s]], 
                  alternative = "two.sided")
    out$catch.sigDiff[out$species==spps[s]]=ifelse(testC$p.value<0.05,T,F)
    out$harv.sigDiff[out$species==spps[s]]=ifelse(testH$p.value<0.05,T,F)
    out$harvR.sigDiff[out$species==spps[s]]=ifelse(testHR$p.value<0.05,T,F)
}

kable(out[,c(1:2)], caption="Results of Kolmogorov-Smirnov test between the full and noWinter datasets for each of the 14 species of interest." )

```

##### Harvest

In addition to the Bayesian analysis presented above a Kolmogorov-Smirnov test was also conducted on the vectors of harvest estimates for the full and noWinter datasets to see if they differed significantly.
The Kolmogorov-Smirnov test appears to be the best option for a frequentist-type analysis as it allows for non-normally distributed data but does assume observations are independent of each other which is not the case for this data since the harvest of black crappie in one month may influence harvest in a subsequent month since those fish are no longer available.
Our creel data also amounts to multiple measurements from the same test subject (i.e. lake), but this test appears to be the best we can do for the time being.
A significant p-value here would mean that the distribution of mean catch values for the data set with no winter creel information is significantly different from the distribution of mean catch values for the full dataset.

```{r ks-test-harvest, warning=F, message=F}



kable(out[,c(1,3)], caption="Results of Kolmogorov-Smirnov test between the full and noWinter datasets for each of the 14 species of interest." )

```

##### Harvest Rate

In addition to the Bayesian analysis presented above a Kolmogorov-Smirnov test was also conducted on the vectors of harvest rate estimates for the full and noWinter datasets to see if they differed significantly.
The Kolmogorov-Smirnov test appears to be the best option for a frequentist-type analysis as it allows for non-normally distributed data but does assume observations are independent of each other which is not the case for this data since the harvest rate of black crappie in one month may influence harvest rate in a subsequent month since those fish are no longer available.
Our creel data also amounts to multiple measurements from the same test subject (i.e. lake), but this test appears to be the best we can do for the time being.
A significant p-value here would mean that the distribution of mean catch values for the data set with no winter creel information is significantly different from the distribution of mean catch values for the full dataset.

```{r ks-test-harvestR, warning=F, message=F}



kable(out[,c(1,4)], caption="Results of Kolmogorov-Smirnov test between the full and noWinter datasets for each of the 14 species of interest." )

```

#### Walleye Exploitation Rate *u*

This section deals specifically with walleye exploitation rate estimates for ceded territory lakes only.
Exploitation rate is a key metric under the Voigt Decision and any proposed changes to creel surveys would need to ensure that the proper exploitation rate information can still be gathered.

What I've done here is calculate exploitation rates based on the full datasets as well as for 5 scenarios:

1.  all winter creel information removed

2.  only creel information from 'summer' May to August is used

3.  25% of weekday creel visits per month are removed

4.  50% of weekday creel visits per month are removed

5.  50% of weekend creel visits per month are removed.
    Creel visits are removed on a month by month basis by randomly removing creel visits to the lake that day

This random removal of days per month should maintain the statistical integrity of the stratified random design that is a part of the creel survey.
For context, this was not how data was removed in a similar analysis done by Deroba et al. (2007) where they removed entire weeks (randomly selected) per month instead of randomly selecting days.
Their analysis also employed t-tests to analyze the resulting exploitation rates in each of their reduced datasets which relies on the assumption of normality and has been discussed above.

Exploitation rate (*u*) is calculated according to the accepted CTWI standard outlined in Deroba et al. (2007):

$$ u= \frac{R}{M} $$ Where $R$ is the number of marked walleye encountered (or 'recaptured') in the creel survey out of the total number ($M$) of walleye marked that spring during fyke net surveys.

```{r u-rate-calcs, warning=F, message=F}

lchar=get_fmdb_lakechar()
ctwiWBIC=lchar[lchar$trtystat==1 & !is.na(lchar$trtystat),]
ctwiWBIC.creel=ctwiWBIC$wbic[ctwiWBIC$wbic%in%cserv$wbic] # wbics in CTWI that have been creeled

tagdat=read.csv("tags_and_marks.csv") # marking data from fmdb since the package function URLs didn't seem to be working
tagdat$County=standardize_county_names(tagdat$County)
tagdat$Gear=standardize_string(tagdat$Gear)
tagdat$Waterbody.Name=standardize_waterbody_names(tagdat$Waterbody.Name)

fndat=tagdat%>% # this is just in case we care what type of mark is there, but I'm going to overwrite this for now and just get total number marked.
  filter(WBIC%in%ctwiWBIC.creel & Species.Code=="X22" & Gear=="fyke_net", !is.na(Mark.Given) & Mark.Given!="")%>%
  group_by(WBIC, Waterbody.Name,Survey.Year,Survey.Begin.Date,Survey.End.Date,Survey.Seq.No,Mark.Given)%>%
  summarise(nFish=sum(Number.of.Fish))

# now summing across all types of marks
fndat=tagdat%>% 
  filter(WBIC%in%ctwiWBIC.creel & Species.Code=="X22" & Gear=="fyke_net", !is.na(Mark.Given) & Mark.Given!="")%>%
  group_by(WBIC, Waterbody.Name,Survey.Year,Survey.Begin.Date,Survey.End.Date,Survey.Seq.No)%>%
  summarise(nFish=sum(Number.of.Fish))%>%
  mutate(Survey.Begin.Date=lubridate::mdy(Survey.Begin.Date),
         Survey.End.Date=lubridate::mdy(Survey.End.Date),
         begin.md=format(Survey.Begin.Date, "%m-%d"),
         end.md=format(Survey.End.Date, "%m-%d"))%>%
  filter(begin.md<"06-01") # getting rid of any marking surveys that may have taken place well after fishing season opened.

fdat=fndat%>% # now that I have only surveys I want, pooling across start and end dates within a year to create an anual total number marked for comparison to creel data
  group_by(WBIC, Waterbody.Name, Survey.Year)%>%
  summarise(nFN.marked=sum(nFish))

ifish=cfish.i%>% # making a data frame to add my groupings to so I can leave cfish.i alone
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel & !is.na(mark.found))%>%
  mutate(daytype=ifelse(wday(sample.date)<=5,"weekday","weekend"),
         month=month(sample.date),
         season=ifelse(month%in%4:10,"openwater","winter"))


# now making a reduced dataframe for each scenario

###### NO WINTER ####

ifish.nw=ifish%>%
  filter(season=="openwater")%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

ifish.nw=ifish.nw%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month)%>%
  summarise(n.marks.recapped=sum(nfish))

ang.exp.nw=ifish.nw%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year"))%>%
  dplyr::select(wbic,Waterbody.Name, year,month,nFN.marked,n.marks.recapped)%>%
  rename(creel.month=month)%>%
  mutate(exp.rate=n.marks.recapped/nFN.marked)


###### SUMMER ONLY ####

ifish.ma=ifish%>%
  filter(month%in%c(5:8))%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

ifish.ma=ifish.ma%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month)%>%
  summarise(n.marks.recapped=sum(nfish))

ang.exp.ma=ifish.ma%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year"))%>%
  dplyr::select(wbic,Waterbody.Name, year,month,nFN.marked,n.marks.recapped)%>%
  rename(creel.month=month)%>%
  mutate(exp.rate=n.marks.recapped/nFN.marked)


# for each of these % reductions I'm revoving data based on visit fish seq no to simulate what would happen if that creel clear visit to the lake were removed.
###### 25% WEEKDAYS ####

ifish.25wd=as.data.frame(matrix(NA,ncol=ncol(ifish)))
colnames(ifish.25wd)=colnames(ifish)

ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  for(m in 1:12){ # month loop
    full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$month[m] & ifish$daytype=="weekday",]
    visits=unique(full$visit.fish.seq.no)
    visit.rm=visits[c(round(runif(round(0.25*length(visits)),min=1,max = length(visits))))]
    reduced=full[!(full$visit.fish.seq.no%in%visit.rm),] # randomly remove 25% of the weekday observations
    
    ifish.25wd=rbind(ifish.25wd,reduced)
  }
}

wd25=ifish.25wd%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

wd25=wd25%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month)%>%
  summarise(n.marks.recapped=sum(nfish))

ang.exp.wd25=wd25%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year"))%>%
  dplyr::select(wbic,Waterbody.Name, year,month,nFN.marked,n.marks.recapped)%>%
  rename(creel.month=month)%>%
  mutate(exp.rate=n.marks.recapped/nFN.marked)


###### 50% WEEKDAYS ####

ifish.50wd=as.data.frame(matrix(NA,ncol=ncol(ifish)))
colnames(ifish.50wd)=colnames(ifish)

ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  for(m in 1:12){ # month loop
    full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$month[m] & ifish$daytype=="weekday",]
    visits=unique(full$visit.fish.seq.no)
    visit.rm=visits[c(round(runif(round(0.50*length(visits)),min=1,max = length(visits))))]
    reduced=full[!(full$visit.fish.seq.no%in%visit.rm),] # randomly remove 50% of the weekday observations

    ifish.50wd=rbind(ifish.50wd,reduced)
  }
}

wd50=ifish.50wd%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

wd50=wd50%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month)%>%
  summarise(n.marks.recapped=sum(nfish))

ang.exp.wd50=wd50%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year"))%>%
  dplyr::select(wbic,Waterbody.Name, year,month,nFN.marked,n.marks.recapped)%>%
  rename(creel.month=month)%>%
  mutate(exp.rate=n.marks.recapped/nFN.marked)

###### 50% WEEKENDS ####

ifish.50we=as.data.frame(matrix(NA,ncol=ncol(ifish)))
colnames(ifish.50we)=colnames(ifish)

ifish.surv=unique(ifish$survey.seq.no)

set.seed(3)
for(i in 1:length(ifish.surv)){ # survey loop
  for(m in 1:12){ # month loop
    full=ifish[ifish$survey.seq.no==ifish.surv[i] & ifish$month[m] & ifish$daytype=="weekend",]
    visits=unique(full$visit.fish.seq.no)
    visit.rm=visits[c(round(runif(round(0.50*length(visits)),min=1,max = length(visits))))]
    reduced=full[!(full$visit.fish.seq.no%in%visit.rm),] # randomly remove 50% of the weekend observations  
    
    ifish.50we=rbind(ifish.50we,reduced)
  }
}

we50=ifish.50we%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel

we50=we50%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month)%>%
  summarise(n.marks.recapped=sum(nfish))

ang.exp.we50=we50%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year"))%>%
  dplyr::select(wbic,Waterbody.Name, year,month,nFN.marked,n.marks.recapped)%>%
  rename(creel.month=month)%>%
  mutate(exp.rate=n.marks.recapped/nFN.marked)



```

```{r, monthly-rates, warning=F, message=F,fig.cap="Distribution of exploitation rates by month (on a log scale) for all years of CTWI creel data. Horizontal red line is overall mean exploitation rate. Black points are outliers."}


# table of marks found during creel surveys
crRecap=cfish.i%>%
  filter(species.code=="X22" & wbic%in%ctwiWBIC.creel & !is.na(mark.found))%>%
  mutate(month=month(sample.date))%>%
  group_by(wbic, year,survey.seq.no,survey.begin.date,survey.end.date, mark.found,month)%>%
  summarise(nfish=sum(fish.count)) # number of fish of each mark typed returned in that creel
crRecap=crRecap%>% # now throwing out mark type and just taking overall number of marks recapped that month for that survey.
  group_by(wbic, year, survey.seq.no, survey.begin.date, survey.end.date,month)%>%
  summarise(n.marks.recapped=sum(nfish))

ang.exp=crRecap%>%
  left_join(fdat, by=c("wbic"="WBIC","year"="Survey.Year"))%>%
  dplyr::select(wbic,Waterbody.Name, year,month,nFN.marked,n.marks.recapped)%>%
  rename(creel.month=month)%>%
  mutate(exp.rate=n.marks.recapped/nFN.marked)

# there is one month observation where the number of marks returned was higher than what was marked, Amber Lake 2016 in May 6 marked and 12 returned. I'm throwing this one out.
ang.exp=ang.exp[ang.exp$exp.rate<1.0,]

# plot of exploitation rate by month, basically standardized # of marks returned each month
ang.exp$plot.month=month(ang.exp$creel.month,label = T)
ggplot(ang.exp)+theme_classic()+
  geom_boxplot(aes(y=log(exp.rate), x=plot.month), fill="grey")+
  geom_hline(yintercept = mean(log(ang.exp$exp.rate), na.rm = T), color="red")+
  labs(y="Log(Exploitation Rate)", x="Month")


```

```{r u-rate-plot, warning=F, message=F, fig.cap="Distribution, on a log scale, of exploitation rates for the full data set plus 5 scenarios with reduced data. Actual = full data set, mayAug = May to August creel data only, noWinter = winter creel data removed, wd25 = 25% of weekday creel visits per month removed, wd50 = 50% of weekday creel visits per month removed, we50 = 50% of weekend creel visits per month removed."}



# combining actual and reduced dataframes into one big one for plotting

ttrExp=rbind(cbind(ang.exp[,c(1,4:10)],treat=rep("actual",nrow(ang.exp))),
             cbind(ang.exp.nw[,c(1,4:10)], treat=rep("noWinter",nrow(ang.exp.nw))),
             cbind(ang.exp.ma[,c(1,4:10)], treat=rep("mayAug",nrow(ang.exp.ma))),
             cbind(ang.exp.wd25[,c(1,4:10)], treat=rep("wd25",nrow(ang.exp.wd25))),
             cbind(ang.exp.wd50[,c(1,4:10)], treat=rep("wd50",nrow(ang.exp.wd50))),
             cbind(ang.exp.we50[,c(1,4:10)], treat=rep("we50",nrow(ang.exp.we50))))

ttrExp=ttrExp[!is.na(ttrExp$wbic),]

ggplot(ttrExp)+theme_classic()+
  geom_density(aes(x=log(exp.rate), fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+
  labs(x="Log(Exploitation Rate)", y="Density",fill="Scenario")
```

##### Bayesian Model Fitting

In order to estimate parameters for a model of the exploitation rate distribution that produced what is represented in the creel data a Bayesian modeling approach was used.
This is because the exploitation rate data are log-normally distributed and thus don't meet the assumptions of parametric analyses based on normality, unless they are first log-transformed.
A Bayesian approach also allows for the inclusion of prior information about the system and does not rely on p-values (though some Bayesian p-values are presented later on) which are arbitrary in nature and can be manipulated via high sample sizes.

Individual likelihoods were constructed for each reduced dataset and fit using the function in the `BayesianTools` package in `R`.
These model fits were checked for convergence using visual inspection of trace plots, posterior distributions, and Gelmin-Rubin Diagnostic tests to ensure that models had converged.
All models were fit using Differential Evolution Markov-Chain-Monte-Carlo algorithms run for 10,000 iterations with the first 5,000 discarded as burn-in.

```{r u-bt-model-fits, message=F, warning=F, echo=T}
set.seed(4)
modDat=ttrExp[!is.na(ttrExp$exp.rate),]

#### ACTUAL ####
uLL.a=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="actual",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="actual"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="actual"])),sd(log(modDat$exp.rate[modDat$treat=="actual"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.actual=createBayesianSetup(uLL.a, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)

expR.actual=runMCMC(bayesianSetup = setup.actual, sampler = "DEzs", settings = settings)

#### NW ####
uLL.nw=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="noWinter",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="noWinter"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="noWinter"])),sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.nw=createBayesianSetup(uLL.nw, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)

expR.nw=runMCMC(bayesianSetup = setup.nw, sampler = "DEzs", settings = settings)


#### MA ####

uLL.ma=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="mayAug",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="mayAug"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="mayAug"])),sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.ma=createBayesianSetup(uLL.ma, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)

expR.ma=runMCMC(bayesianSetup = setup.ma, sampler = "DEzs", settings = settings)


#### WD25 ####

uLL.wd25=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="wd25",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="wd25"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd25"])),sd(log(modDat$exp.rate[modDat$treat=="wd25"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.wd25=createBayesianSetup(uLL.wd25, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)

expR.25wd=runMCMC(bayesianSetup = setup.wd25, sampler = "DEzs", settings = settings)


#### WD50 ####
uLL.wd50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="wd50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="wd50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="wd50"])),sd(log(modDat$exp.rate[modDat$treat=="wd50"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.wd50=createBayesianSetup(uLL.wd50, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)

expR.50wd=runMCMC(bayesianSetup = setup.wd50, sampler = "DEzs", settings = settings)


#### WE50 ####
uLL.we50=function(param){
  alpha=param[1]
  beta=param[2]
  
  us=rlnorm(nrow(modDat[modDat$treat=="we50",]),meanlog = alpha, sdlog = beta)
  ll=dlnorm(modDat$exp.rate[modDat$treat=="we50"], meanlog = mean(log(us)), sdlog = sd(log(us)), log = T)
  return(sum(ll))
}

prior=createTruncatedNormalPrior(mean=c(mean(log(modDat$exp.rate[modDat$treat=="we50"])),sd(log(modDat$exp.rate[modDat$treat=="we50"]))), 
                                 sd=c(1,1),
                                 lower = c(-15,0),
                                 upper = c(15,5))

setup.we50=createBayesianSetup(uLL.we50, prior = prior)

settings=list(iterations=10000, nrChains=3, message=F, burnin=5000)
expR.50we=runMCMC(bayesianSetup = setup.we50, sampler = "DEzs", settings = settings)
```

###### Model Checking

Here model fit is being checked in several ways.
Initially a visual inspection comparing data simulated using median parameter values form the posterior compared to the observed data for that scenario.
A good model fit here is evident when the distribution of the observed and predicted data overlap nearly entirely.

Gelman-Rubin diagnostics were also used to assess convergence, while this doesn't assess model fit it does describe whether or not the MCMC algorithm converged on a solution or whether the parameter space has not been fully explored yet.
For this test values below 1.1 are considered 'converged' with a value of 1 being the lowest possible score.

Lastly, a Bayesian p-value was calculated for each model to further describe model fit to the data.
Bayesian p-values are based on the same idea as a frequentist p-value : *What is the probability of observing a more extreme test statistic than the one calculated from the observed data*.
To calculate a Bayesian p-value each set of parameter values in the MCMC chain is used to parametrized a log-normal distribution from which a set of 'new' data are drawn.
A test statistic is calculated for this new data and determined to either be more or less extreme than the test statistic is for observed data.
The proportion of these test statistics that are more extreme than the observed is then calculated.
If the model has done a good job of fitting the data then the parameter values should generally generate data that looks like the observed data and the test statistic should be equally likely to be more or less extreme than the observed value.
Thus, with Bayesian p-values a value of 0.5 is ideal as it means the model can generate data that matches the distribution of the observed data really well.
If this p-values was very low (\<.10) or very high (\>0.9) that would indicate that the model is not fitting the data well because it is unable to reproduce the data.

In this analysis two test statistics have been chosen to provide alternate, but not unrelated, measure of model fit.
These are the coefficient of variation and standard deviation.
Any test statistic of choice could be chosen as long as it can be justified for the dataset at hand.

```{r u-data-reproduction, message=F, warning=F, fig.cap="Distributions of the observed and model predicted data for each scenario. Model predicted data comes from lognormal distribution paramterized using the median parmater estimate of the model posterior."}

# looking to see if parm estimates produce data that visually at least looks like the observed data for that scenario
pars.a=getSample(expR.actual)
pars.nw=getSample(expR.nw)
pars.ma=getSample(expR.ma)
pars.wd25=getSample(expR.25wd)
pars.wd50=getSample(expR.50wd)
pars.we50=getSample(expR.50we)

aComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                         meanlog = median(pars.a[,1]),
                                                                         sdlog = median(pars.a[,2]))),
                    treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="actual"])),rep("pred",length(modDat$exp.rate[modDat$treat=="actual"]))))

nwComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="noWinter"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
                                                                    meanlog = median(pars.nw[,1]),
                                                                    sdlog = median(pars.nw[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="noWinter"])),rep("pred",length(modDat$exp.rate[modDat$treat=="noWinter"]))))

maComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="mayAug"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
                                                                    meanlog = median(pars.ma[,1]),
                                                                    sdlog = median(pars.ma[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="mayAug"])),rep("pred",length(modDat$exp.rate[modDat$treat=="mayAug"]))))

wd25Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="wd25"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
                                                                    meanlog = median(pars.wd25[,1]),
                                                                    sdlog = median(pars.wd25[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="wd25"])),rep("pred",length(modDat$exp.rate[modDat$treat=="wd25"]))))

wd50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="wd50"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
                                                                    meanlog = median(pars.wd50[,1]),
                                                                    sdlog = median(pars.wd50[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="wd50"])),rep("pred",length(modDat$exp.rate[modDat$treat=="wd50"]))))

we50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="we50"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
                                                                    meanlog = median(pars.we50[,1]),
                                                                    sdlog = median(pars.we50[,2]))),
                 treat=c(rep("observed",length(modDat$exp.rate[modDat$treat=="we50"])),rep("pred",length(modDat$exp.rate[modDat$treat=="we50"]))))

a.p=ggplot(aComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "Actual", fill=element_blank())
nw.p=ggplot(nwComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "No Winter Data (April - October)", fill=element_blank())
ma.p=ggplot(maComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "May - August Data Only", fill=element_blank())
wd25.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "25% of Weekday Creel Visits/Month Removed", fill=element_blank())
wd50.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekday Creel Visits/Month Removed", fill=element_blank())
we50.p=ggplot(we50Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekdend Creel Visits/Month Removed", fill=element_blank())

ggarrange(a.p,nw.p,ma.p,wd25.p,wd50.p,we50.p, common.legend = T)
```

```{r u-gelman-rubin-diagnostics, message=F, warning=F, echo=T}

gelmanDiagnostics(expR.actual) # converged
gelmanDiagnostics(expR.nw) # converged
gelmanDiagnostics(expR.ma) # converged
gelmanDiagnostics(expR.25wd) # converged
gelmanDiagnostics(expR.50wd) # converged
gelmanDiagnostics(expR.50we) # converged

```

```{r u-b-p-value, message=F, warning=F}

# dataframe to hold output
bpValues=data.frame(scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                    coef.var.pval=NA,
                    sd.pval=NA)
#### Pb ACTUAL ####
pval.actual=data.frame(alpha=rep(NA,nrow(pars.a)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.a)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                 meanlog = pars.a[i,1],
                 sdlog = pars.a[i,2])
  pval.actual$alpha[i]=pars.a[i,1]
  pval.actual$beta[i]=pars.a[i,2]
  pval.actual$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.actual$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.actual$cvExceed=0
pval.actual$sdExceed=0

actual.cv=sd(log(modDat$exp.rate[modDat$treat=="actual"]))/mean(log(modDat$exp.rate[modDat$treat=="actual"]))
actual.sd=sd(log(modDat$exp.rate[modDat$treat=="actual"]))

pval.actual$cvExceed[pval.actual$cv>actual.cv]=1
pval.actual$sdExceed[pval.actual$sd>actual.sd]=1

bpValues$coef.var.pval[1]=sum(pval.actual$cvExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[1]=sum(pval.actual$sdExceed==1)/nrow(pval.actual) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb NO WINTER ####
pval.noWinter=data.frame(alpha=rep(NA,nrow(pars.nw)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.nw)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="noWinter"]),
                 meanlog = pars.nw[i,1],
                 sdlog = pars.nw[i,2])
  pval.noWinter$alpha[i]=pars.nw[i,1]
  pval.noWinter$beta[i]=pars.nw[i,2]
  pval.noWinter$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.noWinter$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.noWinter$cvExceed=0
pval.noWinter$sdExceed=0

noWinter.cv=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))/mean(log(modDat$exp.rate[modDat$treat=="noWinter"]))
noWinter.sd=sd(log(modDat$exp.rate[modDat$treat=="noWinter"]))

pval.noWinter$cvExceed[pval.noWinter$cv>noWinter.cv]=1
pval.noWinter$sdExceed[pval.noWinter$sd>noWinter.sd]=1

bpValues$coef.var.pval[2]=sum(pval.noWinter$cvExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[2]=sum(pval.noWinter$sdExceed==1)/nrow(pval.noWinter) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb MAYAUGUST ####
pval.mayAug=data.frame(alpha=rep(NA,nrow(pars.ma)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.ma)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="mayAug"]),
                 meanlog = pars.ma[i,1],
                 sdlog = pars.ma[i,2])
  pval.mayAug$alpha[i]=pars.ma[i,1]
  pval.mayAug$beta[i]=pars.ma[i,2]
  pval.mayAug$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.mayAug$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.mayAug$cvExceed=0
pval.mayAug$sdExceed=0

mayAug.cv=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))/mean(log(modDat$exp.rate[modDat$treat=="mayAug"]))
mayAug.sd=sd(log(modDat$exp.rate[modDat$treat=="mayAug"]))

pval.mayAug$cvExceed[pval.mayAug$cv>mayAug.cv]=1
pval.mayAug$sdExceed[pval.mayAug$sd>mayAug.sd]=1

bpValues$coef.var.pval[3]=sum(pval.mayAug$cvExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[3]=sum(pval.mayAug$sdExceed==1)/nrow(pval.mayAug) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WD25 ####
pval.wd25=data.frame(alpha=rep(NA,nrow(pars.wd25)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.wd25)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd25"]),
                 meanlog = pars.wd25[i,1],
                 sdlog = pars.wd25[i,2])
  pval.wd25$alpha[i]=pars.wd25[i,1]
  pval.wd25$beta[i]=pars.wd25[i,2]
  pval.wd25$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.wd25$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.wd25$cvExceed=0
pval.wd25$sdExceed=0

wd25.cv=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))/mean(log(modDat$exp.rate[modDat$treat=="wd25"]))
wd25.sd=sd(log(modDat$exp.rate[modDat$treat=="wd25"]))

pval.wd25$cvExceed[pval.wd25$cv>wd25.cv]=1
pval.wd25$sdExceed[pval.wd25$sd>wd25.sd]=1

bpValues$coef.var.pval[4]=sum(pval.wd25$cvExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[4]=sum(pval.wd25$sdExceed==1)/nrow(pval.wd25) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WD50 ####
pval.wd50=data.frame(alpha=rep(NA,nrow(pars.wd50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.wd50)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="wd50"]),
                 meanlog = pars.wd50[i,1],
                 sdlog = pars.wd50[i,2])
  pval.wd50$alpha[i]=pars.wd50[i,1]
  pval.wd50$beta[i]=pars.wd50[i,2]
  pval.wd50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.wd50$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.wd50$cvExceed=0
pval.wd50$sdExceed=0

wd50.cv=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))/mean(log(modDat$exp.rate[modDat$treat=="wd50"]))
wd50.sd=sd(log(modDat$exp.rate[modDat$treat=="wd50"]))

pval.wd50$cvExceed[pval.wd50$cv>wd50.cv]=1
pval.wd50$sdExceed[pval.wd50$sd>wd50.sd]=1

bpValues$coef.var.pval[5]=sum(pval.wd50$cvExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[5]=sum(pval.wd50$sdExceed==1)/nrow(pval.wd50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

#### Pb WE50 ####
pval.we50=data.frame(alpha=rep(NA,nrow(pars.we50)),
                       beta=NA,
                       cv=NA,
                       sd=NA)
for(i in 1:nrow(pars.we50)){
  tempdat=rlnorm(n=length(modDat$exp.rate[modDat$treat=="we50"]),
                 meanlog = pars.we50[i,1],
                 sdlog = pars.we50[i,2])
  pval.we50$alpha[i]=pars.we50[i,1]
  pval.we50$beta[i]=pars.we50[i,2]
  pval.we50$cv[i]=sd(log(tempdat))/mean(log(tempdat))
  pval.we50$sd[i]=sd(log(tempdat))
}

# now calculate the number of times the cv or sd exceeds that of the real data

pval.we50$cvExceed=0
pval.we50$sdExceed=0

we50.cv=sd(log(modDat$exp.rate[modDat$treat=="we50"]))/mean(log(modDat$exp.rate[modDat$treat=="we50"]))
we50.sd=sd(log(modDat$exp.rate[modDat$treat=="we50"]))

pval.we50$cvExceed[pval.we50$cv>we50.cv]=1
pval.we50$sdExceed[pval.we50$sd>we50.sd]=1

bpValues$coef.var.pval[6]=sum(pval.we50$cvExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal
bpValues$sd.pval[6]=sum(pval.we50$sdExceed==1)/nrow(pval.we50) # if this value is < 0.1 for >0.9 that would suggest model is not fitting well. 0.5 is ideal

kable(bpValues, digits=3, caption="Bayesian p-values for two variance metrics and each modeled data set.")
```

###### Inference

Having established that the models are fitting the data well, some inference can be gained as to whether or not the reductions in creel effort proposed here would result in a meaningful change in the exploitation rates estimated from the data.

```{r u-data-comparison-to-actual, warning=F, message=F, fig.cap="Comparison of the distribution of actual exploitation rates calculated from creel data and exploitation rates calculated from simulated creel data for each data reduction scenario and the resulting median parameter values for their respective model fits."}

aComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                    meanlog = median(pars.a[,1]),
                                                                    sdlog = median(pars.a[,2]))),
                 treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

nwComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                       meanlog = median(pars.nw[,1]),
                                                                       sdlog = median(pars.nw[,2]))),
                  treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

maComp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.ma[,1]),
                                                                     sdlog = median(pars.ma[,2]))),
                  treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

wd25Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.wd25[,1]),
                                                                     sdlog = median(pars.wd25[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

wd50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.wd50[,1]),
                                                                     sdlog = median(pars.wd50[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

we50Comp=data.frame(u=c(modDat$exp.rate[modDat$treat=="actual"],rlnorm(n=length(modDat$exp.rate[modDat$treat=="actual"]),
                                                                     meanlog = median(pars.we50[,1]),
                                                                     sdlog = median(pars.we50[,2]))),
                    treat=c(rep("actual",length(modDat$exp.rate[modDat$treat=="actual"])),rep("model",length(modDat$exp.rate[modDat$treat=="actual"]))))

a.p=ggplot(aComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "Actual", fill=element_blank())
nw.p=ggplot(nwComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "No Winter Data (April - October)", fill=element_blank())
ma.p=ggplot(maComp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "May - August Data Only", fill=element_blank())
wd25.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "25% of Weekday Creel Visits/Month Removed", fill=element_blank())
wd50.p=ggplot(wd25Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekday Creel Visits/Month Removed", fill=element_blank())
we50.p=ggplot(we50Comp)+theme_classic()+
  geom_density(aes(x=log(u),fill=treat),alpha=0.2)+
  scale_fill_viridis_d()+labs(x="Log(Exploitation Rate)", y="Density", title = "50% of Weekdend Creel Visits/Month Removed", fill=element_blank())

ggarrange(a.p,nw.p,ma.p,wd25.p,wd50.p,we50.p, common.legend = T)
```

A Bayesian p-value can be employed here too.
A test between the cv and sd of the actual data and the cv and sd of the simulated reduction data can describe whether the model of the reduced data is able to approximate the actual data or not.
Successful approximations are p-values close to 0.5 with reduced fit as values increase or decrease beyond 0.5.
Generally, the rule of thumb is that p-values $<0.10$ or $>0.90$ signal unacceptable fits.

The results of these test suggest that when using coefficient of variation as the variance metric to assess model fit and the $<0.10$ or $>0.90$ rule that the no winter and may-august data reduction models both approximate the actual data just as well as the model fit to the actual data.
When using standard deviation as the metric for comparison then all models except the 50% reduction in weekend effort scenario approximate the actual data just as well as the model fit to the actual data.
The reason for this discrepancy between metrics is likely that the coefficient of variation is standardized based on the mean of the distribution while the standard deviation is not and is a raw variance metric.
In this case using the coefficient of variation results seems like the most pragmatic choice, it is also supported the by multi-panel plot comparing the actual exploitation rates to the distributions of simulated data from each scenario.
That's two lines of evidence pointing in the same direction.

```{r b-p-value-comparison, warning=F, message=F}
## p-value tests comparing the scenarios to actual to show that some scenarios approximate the actual quite well.

bpval.comp=data.frame(scenario=c("Actual", "No Winter", "May-August","25% weeday removal","50% weekday removal","50% weekend removal"),
                      coef.var.pval=NA,
                      sd.pval=NA)
pval.actual$cvComp=0
pval.actual$sdComp=0
pval.actual$cvComp[pval.actual$cv>actual.cv]=1
pval.actual$sdComp[pval.actual$sd>actual.sd]=1

pval.noWinter$cvComp=0
pval.noWinter$sdComp=0
pval.noWinter$cvComp[pval.noWinter$cv>actual.cv]=1
pval.noWinter$sdComp[pval.noWinter$sd>actual.sd]=1

pval.mayAug$cvComp=0
pval.mayAug$sdComp=0
pval.mayAug$cvComp[pval.mayAug$cv>actual.cv]=1
pval.mayAug$sdComp[pval.mayAug$sd>actual.sd]=1

pval.wd25$cvComp=0
pval.wd25$sdComp=0
pval.wd25$cvComp[pval.wd25$cv>actual.cv]=1
pval.wd25$sdComp[pval.wd25$sd>actual.sd]=1

pval.wd50$cvComp=0
pval.wd50$sdComp=0
pval.wd50$cvComp[pval.wd50$cv>actual.cv]=1
pval.wd50$sdComp[pval.wd50$sd>actual.sd]=1

pval.we50$cvComp=0
pval.we50$sdComp=0
pval.we50$cvComp[pval.we50$cv>actual.cv]=1
pval.we50$sdComp[pval.we50$sd>actual.sd]=1

bpval.comp$coef.var.pval=c(sum(pval.actual$cvComp)/nrow(pval.actual),
                           sum(pval.noWinter$cvComp)/nrow(pval.noWinter),
                           sum(pval.mayAug$cvComp)/nrow(pval.mayAug),
                           sum(pval.wd25$cvComp)/nrow(pval.wd25),
                           sum(pval.wd50$cvComp)/nrow(pval.wd50),
                           sum(pval.we50$cvComp)/nrow(pval.we50))

bpval.comp$sd.pval=c(sum(pval.actual$sdComp)/nrow(pval.actual),
                           sum(pval.noWinter$sdComp)/nrow(pval.noWinter),
                           sum(pval.mayAug$sdComp)/nrow(pval.mayAug),
                           sum(pval.wd25$sdComp)/nrow(pval.wd25),
                           sum(pval.wd50$sdComp)/nrow(pval.wd50),
                           sum(pval.we50$sdComp)/nrow(pval.we50))
kable(bpval.comp, digits=3, caption="Bayesian p-values for the comparison between the actual data and the scenario-specific model. This test is asking whether the scenario-specific model can approximate the actual data. When true this signals no effect of the data reduction for that scenario on the resulting exploitation rates.")
```

# Appendix A

## Summary of random data removal analysis for catch, harvest, and harvest rate

Detailed code chunks for the catch, harvest, and harvest rate loops are provided below as well as detailed plots of their output.

```{r catch-threshold-loop, warning=F, message=F, echo=T}

# threshold loop for catch
ttrHarv=ttrHarv[!is.na(ttrHarv$wbic),]
survs=unique(ttrHarv$survey.seq.no[!is.na(ttrHarv$survey.seq.no)])
thresh.c=data.frame(wbic=NA,
                  year=NA,
                  month=NA,
                  daytype=NA,
                  survey.seq.no=NA,
                  species=NA,
                  actual.catch.rate=NA,
                  reduced.catch.rate=NA,
                  reduced.catch.rate.sd=NA,
                  pReduc=NA)

for(i in 1:length(survs)){
  tdat=ttrHarv[ttrHarv$survey.seq.no==survs[i],]
  adat=charv[charv$survey.seq.no==survs[i],]
  for(y in 1:length(unique(adat$year))){
    for(m in 1:length(unique(adat$month))){
      for(d in 1:length(unique(adat$daytype))){
        for(s in 1:length(unique(adat$species))){
          z=tdat[tdat$year==unique(adat$year)[y] &
                   tdat$month==unique(adat$month)[m] & 
                   tdat$daytype==unique(adat$daytype)[d] & 
                   tdat$species==unique(adat$species)[s],]
          
          b=adat[adat$year==unique(adat$year)[y] & 
                   adat$month==unique(adat$month)[m] & 
                   adat$daytype==unique(adat$daytype)[d] &
                   adat$species==unique(adat$species)[s],]
          # square rooting varience here to get a SD to keep things in the same terms as the effort threshold above
          if(any(unique(z$total.spp.catch)!=0)){
            pR=max(unique(z$pReduc)[which(!(b$catch.rate<(z$catch.rate+sqrt(z$catch.rate.var)) & b$catch.rate>(z$catch.rate-sqrt(z$catch.rate.var))))])
            if(is.infinite(pR)){
              addDat=c(wbic=unique(z$wbic),
                       year=unique(z$year),
                       month=unique(z$month),
                       daytype=unique(z$daytype),
                       survey.seq.no=survs[i],
                       species=unique(adat$species)[s],
                       actual.catch.rate=b$catch.rate,
                       reduced.catch.rate=NA,
                       reduced.catch.rate.sd=NA,
                       pReduc=NA)
              
              thresh.c=rbind(thresh.c,addDat)
            }else{
              addDat=c(wbic=unique(z$wbic),
                       year=unique(z$year),
                       month=unique(z$month),
                       daytype=unique(z$daytype),
                       survey.seq.no=survs[i],
                       species=unique(adat$species)[s],
                       actual.catch.rate=b$catch.rate,
                       reduced.catch.rate=z$catch.rate[z$pReduc==pR],
                       reduced.catch.rate.sd=sqrt(z$catch.rate.var[z$pReduc==pR]),
                       pReduc=pR)
              
              thresh.c=rbind(thresh.c,addDat)
            }
          }
        }
      }
    } 
  }
}
thresh.c=thresh.c[!is.na(thresh.c$wbic),]

# warnings about -inf are fine and dealt with in the loop using the is.infinite() call

```

```{r catch-threshold-sppPlot1, message=F, warning=F, fig.cap="Distribution of the % reduction where accuracy threshold was exceeded for weekend days. X-axis values of 'NA' represent scenarios where accuracy was never lost. Fishery metric being tested here is catch rate calculated from creel information. Different colored bars represent different months.", fig.width=7, fig.height=5}

thresh.c$month=factor(thresh.c$month, levels = sort(as.numeric(unique(thresh.c$month))), labels = sort(as.numeric(unique(thresh.c$month))))
thresh.c.p=thresh.c[thresh.c$species%in%c("black_crappie","bluegill","largemouth_bass","muskellunge","northern_pike","pumpkinseed","rock_bass","smallmouth_bass","walleye","yellow_perch","brook_trout","brown_trout","lake_trout","rainbow_trout"),] # picking out important species to plot
wknd=ggplot(thresh.c.p[thresh.c.p$daytype=="weekend",])+theme_classic()+
  geom_bar(aes(x=pReduc,fill=month),position = "dodge")+
  scale_fill_viridis_d()+
  facet_wrap(~species, scales = "free")+
  labs(x="Proportion of Data Removed", fill= "Month")+
  theme(legend.position = "bottom")
wknd

```

```{r catch-threshold-sppPlot2, message=F, warning=F, fig.cap="Distribution of the % reduction where accuracy threshold was exceeded for week days. X-axis values of 'NA' represent scenarios where accuracy was never lost. Fishery metric being tested here is catch rate calculated from creel information. Different colored bars represent different months.", fig.width=7, fig.height=5}

wkdy=ggplot(thresh.c.p[thresh.c.p$daytype=="weekday",])+theme_classic()+
  geom_bar(aes(x=pReduc,fill=month),position = "dodge")+
  scale_fill_viridis_d()+
  facet_wrap(~species, scales = "free")+
  labs(x="Proportion of Data Removed", fill= "Month")+
  theme(legend.position = "bottom")
wkdy

```

```{r catch-threshold-annualPlot, message=F, warning=F, fig.cap="Distribution of the % reduction where accurace threshold was exceeded for both weekend and week days (different colors), for all species, and pooled across months. X-axis values represent the mean % reduction (averaged across months)."}

# grouping across month. Ranbow Trout did not have enough data to calculate a mean % reduction for lost accuracy."

annualThresh.c=thresh.c.p%>%
  group_by(wbic,year,daytype, survey.seq.no, species)%>%
  summarise(actualcatch.rate=sum(as.numeric(actual.catch.rate)),
            reduced.catch.rate=sum(as.numeric(reduced.catch.rate),na.rm = T),
            reduced.catch.rate.sd=sd(as.numeric(reduced.catch.rate.sd),na.rm = T),
            meanPR=mean(as.numeric(pReduc),na.rm=T))

ggplot(annualThresh.c)+theme_classic()+
  geom_density(aes(x=meanPR,fill=daytype),alpha=0.2)+
  facet_wrap(~species, scales = "free")+
  scale_fill_viridis_d()+
  labs(x="Mean % of Data Removed", y="Density",fill="Day Type")

```

```{r harvest-threshold-loop, warning=F, message=F}

# threshold loop for harvest

survs=unique(ttrHarv$survey.seq.no[!is.na(ttrHarv$survey.seq.no)])
thresh.h=data.frame(wbic=NA,
                    year=NA,
                    month=NA,
                    daytype=NA,
                    survey.seq.no=NA,
                    species=NA,
                    actual.total.harvest=NA,
                    reduced.total.harvest=NA,
                    reduced.harvest.sd=NA,
                    pReduc=NA)

for(i in 1:length(survs)){
  tdat=ttrHarv[ttrHarv$survey.seq.no==survs[i],]
  adat=charv[charv$survey.seq.no==survs[i],]
  for(y in 1:length(unique(adat$year))){
    for(m in 1:length(unique(adat$month))){
      for(d in 1:length(unique(adat$daytype))){
        for(s in 1:length(unique(adat$species))){
          z=tdat[tdat$year==unique(adat$year)[y] &
                   tdat$month==unique(adat$month)[m] & 
                   tdat$daytype==unique(adat$daytype)[d] & 
                   tdat$species==unique(adat$species)[s],]
          
          b=adat[adat$year==unique(adat$year)[y] & 
                   adat$month==unique(adat$month)[m] & 
                   adat$daytype==unique(adat$daytype)[d] &
                   adat$species==unique(adat$species)[s],]
          # square rooting varience here to get a SD to keep things in the same terms as the effort threshold above
          if(any(unique(z$total.harvest)!=0)){
            pR=max(unique(z$pReduc)[which(!(b$total.harvest<(z$total.harvest+sqrt(z$harvest.var)) & b$total.harvest>(z$total.harvest-sqrt(z$harvest.var))))])
            if(is.infinite(pR)){
              addDat=c(wbic=unique(z$wbic),
                       year=unique(z$year),
                       month=unique(z$month),
                       daytype=unique(z$daytype),
                       survey.seq.no=survs[i],
                       species=unique(adat$species)[s],
                       actual.total.harvest=b$total.harvest,
                       reduced.total.harvest=NA,
                       reduced.harvest.sd=NA,
                       pReduc=NA)
              
              thresh.h=rbind(thresh.h,addDat)
            }else{
              addDat=c(wbic=unique(z$wbic),
                       year=unique(z$year),
                       month=unique(z$month),
                       daytype=unique(z$daytype),
                       survey.seq.no=survs[i],
                       species=unique(adat$species)[s],
                       actual.total.harvest=b$total.harvest,
                       reduced.total.harvest=z$total.harvest[z$pReduc==pR],
                       reduced.harvest.sd=sqrt(z$harvest.var[z$pReduc==pR]),
                       pReduc=pR)
              
              thresh.h=rbind(thresh.h,addDat)
            }
          }
        }
      }
    } 
  }
}
thresh.h=thresh.h[!is.na(thresh.h$wbic),]

# warnings about -inf are fine and dealt with in the loop using the is.infinite() call

```

```{r harvest-threshold-sppPlot1, warning=F, message=F, ,fig.cap="Distribution of the % reduction where accuracy threshold was exceeded for weekend days. X-axis values of 'NA' represent scenarios where accuracy was never lost. Fishery metric being tested here is total species harvest calculated from creel information. Different colored bars represent different months.", fig.width=7, fig.height=5}

thresh.h$month=factor(thresh.h$month, levels = sort(as.numeric(unique(thresh.h$month))), labels = sort(as.numeric(unique(thresh.h$month))))
thresh.h.p=thresh.h[thresh.h$species%in%c("black_crappie","bluegill","largemouth_bass","muskellunge","northern_pike","pumpkinseed","rock_bass","smallmouth_bass","walleye","yellow_perch","brook_trout","brown_trout","lake_trout","rainbow_trout"),] # picking out important species to plot
wknd=ggplot(thresh.h.p[thresh.h.p$daytype=="weekend",])+theme_classic()+
  geom_bar(aes(x=pReduc,fill=month),position = "dodge")+
  scale_fill_viridis_d()+
  facet_wrap(~species, scales = "free")+
  labs(x="Proportion of Data Removed", fill= "Month")+
  theme(legend.position = "bottom")
wknd
```

```{r harvest-threshold-sppPlot2, warning=F, message=F, fig.cap="Distribution of the % reduction where accuracy threshold was exceeded for week days. X-axis values of 'NA' represent scenarios where accuracy was never lost. Fishery metric being tested here is total species harvest calculated from creel information. Different colored bars represent different months.", fig.width=7, fig.height=5}

wkdy=ggplot(thresh.h.p[thresh.h.p$daytype=="weekday",])+theme_classic()+
  geom_bar(aes(x=pReduc,fill=month),position = "dodge")+
  scale_fill_viridis_d()+
  facet_wrap(~species, scales = "free")+
  labs(x="Proportion of Data Removed", fill= "Month")+
  theme(legend.position = "bottom")
wkdy
```

```{r harvest-threshold-annualPlot, warning=F, message=F, , fig.cap="Distribution of the % reduction where accurace threshold was exceeded for both weekend and week days (different colors), for all species, and pooled across months. X-axis values represent the mean % reduction (averaged across months)."}

# grouping across month

annualThresh.h=thresh.h.p%>%
  group_by(wbic,year,daytype, survey.seq.no, species)%>%
  summarise(actual.total.harvest=sum(as.numeric(actual.total.harvest)),
            reduced.total.harvest=sum(as.numeric(reduced.total.harvest),na.rm = T),
            reduced.harvest.sd=sd(as.numeric(reduced.harvest.sd),na.rm = T),
            meanPR=mean(as.numeric(pReduc),na.rm=T))

ggplot(annualThresh.h)+theme_classic()+
  geom_density(aes(x=meanPR,fill=daytype),alpha=0.2)+
  facet_wrap(~species, scales = "free")+
  scale_fill_viridis_d()+
  labs(x="Mean % of Data Removed", y="Density",fill="Day Type")

```

```{r harvestRate-threshold-loop, warning=F, message=F}

# threshold loop for harvest rate

survs=unique(ttrHarv$survey.seq.no[!is.na(ttrHarv$survey.seq.no)])
thresh.hr=data.frame(wbic=NA,
                    year=NA,
                    month=NA,
                    daytype=NA,
                    survey.seq.no=NA,
                    species=NA,
                    actual.harvest.rate=NA,
                    reduced.harvest.rate=NA,
                    reduced.harvest.rate.sd=NA,
                    pReduc=NA)

for(i in 1:length(survs)){
  tdat=ttrHarv[ttrHarv$survey.seq.no==survs[i],]
  adat=charv[charv$survey.seq.no==survs[i],]
  for(y in 1:length(unique(adat$year))){
    for(m in 1:length(unique(adat$month))){
      for(d in 1:length(unique(adat$daytype))){
        for(s in 1:length(unique(adat$species))){
          z=tdat[tdat$year==unique(adat$year)[y] &
                   tdat$month==unique(adat$month)[m] & 
                   tdat$daytype==unique(adat$daytype)[d] & 
                   tdat$species==unique(adat$species)[s],]
          
          b=adat[adat$year==unique(adat$year)[y] & 
                   adat$month==unique(adat$month)[m] & 
                   adat$daytype==unique(adat$daytype)[d] &
                   adat$species==unique(adat$species)[s],]
          # square rooting varience here to get a SD to keep things in the same terms as the effort threshold above
          if(any(unique(z$harvest.rate)!=0)){
            pR=max(unique(z$pReduc)[which(!(b$harvest.rate<(z$harvest.rate+sqrt(z$harvest.rate.var)) & b$harvest.rate>(z$harvest.rate-sqrt(z$harvest.rate.var))))])
            if(is.infinite(pR)){
              addDat=c(wbic=unique(z$wbic),
                       year=unique(z$year),
                       month=unique(z$month),
                       daytype=unique(z$daytype),
                       survey.seq.no=survs[i],
                       species=unique(adat$species)[s],
                       actual.harvest.rate=b$harvest.rate,
                       reduced.harvest.rate=NA,
                       reduced.harvest.sd=NA,
                       pReduc=NA)
              
              thresh.hr=rbind(thresh.hr,addDat)
            }else{
              addDat=c(wbic=unique(z$wbic),
                       year=unique(z$year),
                       month=unique(z$month),
                       daytype=unique(z$daytype),
                       survey.seq.no=survs[i],
                       species=unique(adat$species)[s],
                       actual.harvest.rate=b$harvest.rate,
                       reduced.harvest.rate=z$harvest.rate[z$pReduc==pR],
                       reduced.harvest.sd=sqrt(z$harvest.rate.var[z$pReduc==pR]),
                       pReduc=pR)
              
              thresh.hr=rbind(thresh.hr,addDat)
            }
          }
        }
      }
    } 
  }
}
thresh.hr=thresh.hr[!is.na(thresh.hr$wbic),]

```

```{r harvestRate-threshold-sppPlot1, warning=F, message=F, ,fig.cap="Distribution of the % reduction where accuracy threshold was exceeded for weekend days. X-axis values of 'NA' represent scenarios where accuracy was never lost. Fishery metric being tested here is species harvest rate calculated from creel information. Different colored bars represent different months.", fig.width=7, fig.height=5}

thresh.hr$month=factor(thresh.hr$month, levels = sort(as.numeric(unique(thresh.hr$month))), labels = sort(as.numeric(unique(thresh.hr$month))))
thresh.hr.p=thresh.hr[thresh.hr$species%in%c("black_crappie","bluegill","largemouth_bass","muskellunge","northern_pike","pumpkinseed","rock_bass","smallmouth_bass","walleye","yellow_perch","brook_trout","brown_trout","lake_trout","rainbow_trout"),] # picking out important species to plot
wknd=ggplot(thresh.hr.p[thresh.hr.p$daytype=="weekend",])+theme_classic()+
  geom_bar(aes(x=pReduc,fill=month),position = "dodge")+
  scale_fill_viridis_d()+
  facet_wrap(~species, scales = "free")+
  labs(x="Proportion of Data Removed", fill= "Month")+
  theme(legend.position = "bottom")
wknd

```

```{r harvestRate-threshold-sppPlot2, warning=F, message=F, fig.cap="Distribution of the % reduction where accuracy threshold was exceeded for week days. X-axis values of 'NA' represent scenarios where accuracy was never lost. Fishery metric being tested here is species harvest rate calculated from creel information. Different colored bars represent different months.", fig.width=7, fig.height=5}
wkdy=ggplot(thresh.hr.p[thresh.hr.p$daytype=="weekday",])+theme_classic()+
  geom_bar(aes(x=pReduc,fill=month),position = "dodge")+
  scale_fill_viridis_d()+
  facet_wrap(~species, scales = "free")+
  labs(x="Proportion of Data Removed", fill= "Month")+
  theme(legend.position = "bottom")
wkdy

```

```{r harvestRate-threshold-annualPlot, warning=F, message=F, , fig.cap="Distribution of the % reduction where accurace threshold was exceeded for both weekend and week days (different colors), for all species, and pooled across months. X-axis values represent the mean % reduction (averaged across months)."}

# grouping across month

annualthresh.hr=thresh.hr.p%>%
  group_by(wbic,year,daytype, survey.seq.no, species)%>%
  summarise(actual.harvest.rate=sum(as.numeric(actual.harvest.rate)),
            reduced.harvest.rate=sum(as.numeric(reduced.harvest.rate),na.rm = T),
            reduced.harvest.rate.sd=sd(as.numeric(reduced.harvest.rate.sd),na.rm = T),
            meanPR=mean(as.numeric(pReduc),na.rm=T))

ggplot(annualthresh.hr)+theme_classic()+
  geom_density(aes(x=meanPR,fill=daytype),alpha=0.2)+
  facet_wrap(~species, scales = "free")+
  scale_fill_viridis_d()+
  labs(x="Mean % of Data Removed", y="Density",fill="Day Type")
```

<fill out text and code chunks here to mirror those for effort winter creel removal analysis in the main text>
